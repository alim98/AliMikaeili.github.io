{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "849f580106ae4d619099269c67c24983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab05fddbae8f4a89930ca375fd2da70e",
              "IPY_MODEL_47e0dce8189241dea6fd4cf755117eee",
              "IPY_MODEL_863c6c615d804d8db76a232c6d248583"
            ],
            "layout": "IPY_MODEL_6f7812943c0047c3873fb4bd2041fc53"
          }
        },
        "ab05fddbae8f4a89930ca375fd2da70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24e17ca43ff847afb2a73c9f15f43f3f",
            "placeholder": "​",
            "style": "IPY_MODEL_d0d4d285b28143a485f1e8b5203dd291",
            "value": "config.json: 100%"
          }
        },
        "47e0dce8189241dea6fd4cf755117eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24adf96ff6a44d91b24dfe5219d4e67e",
            "max": 725,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba66cece06d549b982930fffe2b68a41",
            "value": 725
          }
        },
        "863c6c615d804d8db76a232c6d248583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84af5d595bf94000924ae77836fc7ebc",
            "placeholder": "​",
            "style": "IPY_MODEL_956d6516843042c0a5a7de95128d4e66",
            "value": " 725/725 [00:00&lt;00:00, 59.2kB/s]"
          }
        },
        "6f7812943c0047c3873fb4bd2041fc53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24e17ca43ff847afb2a73c9f15f43f3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0d4d285b28143a485f1e8b5203dd291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24adf96ff6a44d91b24dfe5219d4e67e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba66cece06d549b982930fffe2b68a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84af5d595bf94000924ae77836fc7ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "956d6516843042c0a5a7de95128d4e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfefd5f567cc4489a809b1925c9ad99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_468e3d1baaee4d04a3d6c4baa70dfc4c",
              "IPY_MODEL_4b6a6bedd15649ae800f9ffa37c16405",
              "IPY_MODEL_ab948544bfb44d5898d2aacdcfb86fd9"
            ],
            "layout": "IPY_MODEL_0f5d378c89fa4270853738d432eef7d0"
          }
        },
        "468e3d1baaee4d04a3d6c4baa70dfc4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36e47a460ca64c2c9cd25fd5efe6a268",
            "placeholder": "​",
            "style": "IPY_MODEL_ed17c2dec864496199a8e1ae2327d274",
            "value": "model.safetensors: 100%"
          }
        },
        "4b6a6bedd15649ae800f9ffa37c16405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d56bf6cbead446508b6512317429cf25",
            "max": 376873760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf988f5aa564469b90d81a7f0dcdbe41",
            "value": 376873760
          }
        },
        "ab948544bfb44d5898d2aacdcfb86fd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a96fcd9f2e648268bb43aaf02ca2ca7",
            "placeholder": "​",
            "style": "IPY_MODEL_df8c9de064724bb0b3336c653a105572",
            "value": " 377M/377M [00:01&lt;00:00, 241MB/s]"
          }
        },
        "0f5d378c89fa4270853738d432eef7d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36e47a460ca64c2c9cd25fd5efe6a268": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed17c2dec864496199a8e1ae2327d274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d56bf6cbead446508b6512317429cf25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf988f5aa564469b90d81a7f0dcdbe41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a96fcd9f2e648268bb43aaf02ca2ca7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df8c9de064724bb0b3336c653a105572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f46bc40dc404ebfb758360caa27081b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c396f22ebe74d8585fea42f72858b77",
              "IPY_MODEL_e222e95a1ec64a54be1ed7bfe37f62e3",
              "IPY_MODEL_77d4640d285c4793a31d4b3773a42f8a"
            ],
            "layout": "IPY_MODEL_65007a23829a431f80d9dc83cdd126e0"
          }
        },
        "7c396f22ebe74d8585fea42f72858b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aa1336e1747473bad52dbf59da2613c",
            "placeholder": "​",
            "style": "IPY_MODEL_b602cf9fa6994257bde1c2a3dc16a1df",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "e222e95a1ec64a54be1ed7bfe37f62e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cafbdfd41d74e3285c234fcda5bde28",
            "max": 271,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e320c6f2df14b26bf8baeca908fa910",
            "value": 271
          }
        },
        "77d4640d285c4793a31d4b3773a42f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecc77165738b4de1b33c813b6cceb079",
            "placeholder": "​",
            "style": "IPY_MODEL_d618b6c567644d169f84bb9438fdc9e9",
            "value": " 271/271 [00:00&lt;00:00, 23.0kB/s]"
          }
        },
        "65007a23829a431f80d9dc83cdd126e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aa1336e1747473bad52dbf59da2613c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b602cf9fa6994257bde1c2a3dc16a1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cafbdfd41d74e3285c234fcda5bde28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e320c6f2df14b26bf8baeca908fa910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ecc77165738b4de1b33c813b6cceb079": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d618b6c567644d169f84bb9438fdc9e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/AliMikaeili.github.io/blob/master/MPI_video_MAE_f.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Essential downloads"
      ],
      "metadata": {
        "id": "daa58lj6b7qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O downloaded_file.zip \"https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\"\n",
        "\n",
        "!unzip -q downloaded_file.zip"
      ],
      "metadata": {
        "id": "AIIIxarwTVT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5003818-f9e9-49c2-8463-997674a6f2ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-13 18:16:37--  https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 64.233.170.132, 2404:6800:4003:c1a::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|64.233.170.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1264688649 (1.2G) [application/octet-stream]\n",
            "Saving to: ‘downloaded_file.zip’\n",
            "\n",
            "downloaded_file.zip 100%[===================>]   1.18G  73.0MB/s    in 18s     \n",
            "\n",
            "2025-01-13 18:16:57 (67.8 MB/s) - ‘downloaded_file.zip’ saved [1264688649/1264688649]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install -q"
      ],
      "metadata": {
        "id": "Lo_tyyqLb0sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip -q install transformers scikit-learn matplotlib seaborn torch torchvision umap-learn\n",
        "!pip -q install openpyxl imageio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZwyJMBQTXV3",
        "outputId": "d99b0514-8374-4483-d73b-95fdb227f721"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#run 3 channedl seg"
      ],
      "metadata": {
        "id": "AyUcdZycdiIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# - Channel 0: Segmented part with mask_1 in grayscale\n",
        "# - Channel 1: Original image in grayscale\n",
        "# - Channel 2: Segmented part with mask_2 in grayscale\n",
        "import os\n",
        "import glob\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import imageio.v2 as iio\n",
        "from transformers import (\n",
        "    VideoMAEForPreTraining,\n",
        "    VideoMAEImageProcessor,\n",
        "    get_cosine_schedule_with_warmup,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import umap.umap_ as umap\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from collections import deque\n",
        "import time\n",
        "import multiprocessing\n",
        "import imageio\n",
        "import wandb\n",
        "import io  # For in-memory file handling\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.data.dataloader\")\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    \"\"\"\n",
        "    Parse command-line arguments for configurable paths and training parameters.\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"VideoMAE Pre-training Script with Segmented Videos\")\n",
        "    parser.add_argument('--raw_base_dir', type=str, default='./raw', help='Path to raw data directory')\n",
        "    parser.add_argument('--seg_base_dir', type=str, default='./seg', help='Path to segmentation data directory')\n",
        "    parser.add_argument('--csv_output_dir', type=str, default='csv_outputs', help='Directory to save CSV outputs')\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints', help='Directory to save model checkpoints')\n",
        "    parser.add_argument('--log_dir', type=str, default='logs', help='Directory for TensorBoard logs')\n",
        "    parser.add_argument('--batch_size', type=int, default=2, help='Batch size for training')\n",
        "    parser.add_argument('--num_epochs', type=int, default=100, help='Number of training epochs')\n",
        "    parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate for optimizer')\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-2, help='Weight decay for optimizer')\n",
        "    parser.add_argument('--subvol_size', type=int, default=80, help='Size of the sub-volume to extract')\n",
        "    parser.add_argument('--num_frames', type=int, default=16, help='Number of frames per video clip')\n",
        "    parser.add_argument('--mask_ratio', type=float, default=0.50, help='Mask ratio for VideoMAE')\n",
        "    parser.add_argument('--patience', type=int, default=3, help='Patience for early stopping')\n",
        "    parser.add_argument('--resume_checkpoint', type=str, default=None, help='Path to resume checkpoint')\n",
        "\n",
        "    # Optionally add WandB specific arguments\n",
        "    parser.add_argument('--wandb_project', type=str, default='VideoMAE_PreTraining', help='WandB project name')\n",
        "    parser.add_argument('--wandb_entity', type=str, default=None, help='WandB entity/team name')\n",
        "    parser.add_argument('--wandb_run_name', type=str, default=None, help='WandB run name')\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Early stopping utility to halt training when validation loss stops improving.\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=10, verbose=False, delta=0.0, path='checkpoint.pth'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, current_loss, model, optimizer, scheduler, epoch):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = current_loss\n",
        "            self.save_checkpoint(model, optimizer, scheduler, epoch, current_loss)\n",
        "        elif current_loss > self.best_loss - self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = current_loss\n",
        "            self.save_checkpoint(model, optimizer, scheduler, epoch, current_loss)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model, optimizer, scheduler, epoch, loss):\n",
        "        \"\"\"\n",
        "        Saves model when validation loss decreases.\n",
        "        \"\"\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss': loss,\n",
        "        }, self.path)\n",
        "        if self.verbose:\n",
        "            print(f\"Validation loss decreased. Saving model to {self.path}\")\n",
        "\n",
        "\n",
        "def load_volumes(bbox_name, raw_base_dir, seg_base_dir):\n",
        "    \"\"\"\n",
        "    Load raw volume and segmentation volume for a bounding box.\n",
        "\n",
        "    Args:\n",
        "        bbox_name (str): Name of the bounding box directory.\n",
        "        raw_base_dir (str): Base directory for raw data.\n",
        "        seg_base_dir (str): Base directory for segmentation data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (raw_vol, seg_vol) each as np.ndarray\n",
        "    \"\"\"\n",
        "    raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "    seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "\n",
        "    raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "    seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "\n",
        "    if len(raw_tif_files) == 0:\n",
        "        print(f\"No raw files found for {bbox_name} in {raw_dir}\")\n",
        "        return None, None\n",
        "\n",
        "    if len(seg_tif_files) == 0:\n",
        "        print(f\"No segmentation files found for {bbox_name} in {seg_dir}\")\n",
        "        return None, None\n",
        "\n",
        "    if len(raw_tif_files) != len(seg_tif_files):\n",
        "        print(f\"Mismatch in number of raw vs seg slices for {bbox_name}. Skipping.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        raw_vol = np.stack([iio.imread(f) for f in raw_tif_files], axis=0)  # shape: (Z, Y, X)\n",
        "        seg_vol = np.stack([iio.imread(f).astype(np.uint32) for f in seg_tif_files], axis=0)\n",
        "        return raw_vol, seg_vol\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading volumes for {bbox_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def create_segmented_cube(\n",
        "    raw_vol,\n",
        "    seg_vol,\n",
        "    central_coord,\n",
        "    side1_coord,\n",
        "    side2_coord,\n",
        "    subvolume_size=80,\n",
        "    alpha=0.3\n",
        "):\n",
        "    \"\"\"\n",
        "    Constructs an 80x80x80 segmented 3D cube around the specified synapse coordinates\n",
        "    and overlays both segmentation masks (side1_coord, side2_coord) on the raw data\n",
        "    with specified transparency for each slice.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Overlaid cube of shape (height, width, 3, depth),\n",
        "                    i.e., (80, 80, 3, 80) if subvolume_size=80.\n",
        "    \"\"\"\n",
        "\n",
        "    def create_segment_masks(segmentation_volume, s1_coord, s2_coord):\n",
        "        x1, y1, z1 = s1_coord\n",
        "        x2, y2, z2 = s2_coord\n",
        "        # Validate within volume\n",
        "        if not (0 <= z1 < segmentation_volume.shape[0] and\n",
        "                0 <= y1 < segmentation_volume.shape[1] and\n",
        "                0 <= x1 < segmentation_volume.shape[2]):\n",
        "            raise ValueError(\"Side1 coordinates are out of bounds.\")\n",
        "\n",
        "        if not (0 <= z2 < segmentation_volume.shape[0] and\n",
        "                0 <= y2 < segmentation_volume.shape[1] and\n",
        "                0 <= x2 < segmentation_volume.shape[2]):\n",
        "            raise ValueError(\"Side2 coordinates are out of bounds.\")\n",
        "\n",
        "        seg_id_1 = segmentation_volume[z1, y1, x1]\n",
        "        seg_id_2 = segmentation_volume[z2, y2, x2]\n",
        "\n",
        "        # If seg_id == 0, it means no segment at that voxel\n",
        "        if seg_id_1 == 0:\n",
        "            mask_1 = np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        else:\n",
        "            mask_1 = (segmentation_volume == seg_id_1)\n",
        "\n",
        "        if seg_id_2 == 0:\n",
        "            mask_2 = np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        else:\n",
        "            mask_2 = (segmentation_volume == seg_id_2)\n",
        "\n",
        "        return mask_1, mask_2\n",
        "\n",
        "    # Build masks\n",
        "    mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "\n",
        "    # Define subvolume bounds\n",
        "    half_size = subvolume_size // 2\n",
        "    cx, cy, cz = central_coord\n",
        "\n",
        "    x_start, x_end = max(cx - half_size, 0), min(cx + half_size, raw_vol.shape[2])\n",
        "    y_start, y_end = max(cy - half_size, 0), min(cy + half_size, raw_vol.shape[1])\n",
        "    z_start, z_end = max(cz - half_size, 0), min(cz + half_size, raw_vol.shape[0])\n",
        "\n",
        "    # Extract subvolumes\n",
        "    sub_raw = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_1 = mask_1_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_2 = mask_2_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "    # Pad if smaller than subvolume_size\n",
        "    pad_z = subvolume_size - sub_raw.shape[0]\n",
        "    pad_y = subvolume_size - sub_raw.shape[1]\n",
        "    pad_x = subvolume_size - sub_raw.shape[2]\n",
        "\n",
        "    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n",
        "        sub_raw = np.pad(sub_raw, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                         mode='constant', constant_values=0)\n",
        "        sub_mask_1 = np.pad(sub_mask_1, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                            mode='constant', constant_values=False)\n",
        "        sub_mask_2 = np.pad(sub_mask_2, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                            mode='constant', constant_values=False)\n",
        "\n",
        "    # Slice to exact shape\n",
        "    sub_raw = sub_raw[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_1 = sub_mask_1[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_2 = sub_mask_2[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "\n",
        "    # ایجاد آرایه برای تصاویر سه‌کاناله\n",
        "    # ابعاد: (ارتفاع، عرض، کانال‌ها، عمق)\n",
        "    overlaid_cube = np.zeros((subvolume_size, subvolume_size, 3, subvolume_size), dtype=np.uint8)\n",
        "\n",
        "    for z in range(subvolume_size):\n",
        "        # نرمال‌سازی برش اصلی به محدوده [0, 1]\n",
        "        raw_slice = sub_raw[z].astype(np.float32)\n",
        "        mn, mx = raw_slice.min(), raw_slice.max()\n",
        "        if mx > mn:\n",
        "            raw_normalized = (raw_slice - mn) / (mx - mn)\n",
        "        else:\n",
        "            raw_normalized = raw_slice - mn  # در صورتی که همه مقادیر برابر باشند\n",
        "\n",
        "        # تبدیل به مقیاس 0-255\n",
        "        raw_scaled = (raw_normalized * 255).astype(np.uint8)\n",
        "\n",
        "        # کانال میانی (کانال 1) عکس اصلی\n",
        "        overlaid_cube[:, :, 1, z] = raw_scaled\n",
        "\n",
        "        # استخراج ماسک‌ها\n",
        "        mask1 = sub_mask_1[z].astype(np.uint8)\n",
        "        mask2 = sub_mask_2[z].astype(np.uint8)\n",
        "\n",
        "        # کانال 0: فقط قسمت سگمنت شده با mask_1\n",
        "        overlaid_cube[:, :, 0, z] = raw_scaled * mask1\n",
        "\n",
        "        # کانال 2: فقط قسمت سگمنت شده با mask_2\n",
        "        overlaid_cube[:, :, 2, z] = raw_scaled * mask2\n",
        "    return overlaid_cube\n",
        "\n",
        "\n",
        "class VideoMAEDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class that uses segmented volumes (side1 & side2) for VideoMAE pre-training.\n",
        "    \"\"\"\n",
        "    def __init__(self, vol_data_list, synapse_df, processor, subvol_size=80, num_frames=16, alpha=0.3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vol_data_list (List[Tuple[np.ndarray, np.ndarray]]): List of (raw_vol, seg_vol).\n",
        "            synapse_df (pd.DataFrame): DataFrame with synapse coordinates (central, side1, side2).\n",
        "            processor (VideoMAEImageProcessor): Processor for VideoMAE.\n",
        "            subvol_size (int): Size of the sub-volume to extract.\n",
        "            num_frames (int): Number of frames for the model.\n",
        "            alpha (float): Blending alpha for segmentation.\n",
        "        \"\"\"\n",
        "        self.vol_data_list = vol_data_list\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.subvol_size = subvol_size\n",
        "        self.num_frames = num_frames\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        syn_info = self.synapse_df.iloc[idx]\n",
        "        bbox_index = syn_info['bbox_index']\n",
        "\n",
        "        raw_vol, seg_vol = self.vol_data_list[bbox_index]\n",
        "        if raw_vol is None or seg_vol is None:\n",
        "            # Return dummy data if volumes not found\n",
        "            pixel_values = torch.zeros((self.num_frames, 3, self.subvol_size, self.subvol_size), dtype=torch.float32)\n",
        "            return pixel_values, pixel_values\n",
        "\n",
        "        # Coordinates\n",
        "        central_coord = (\n",
        "            int(syn_info['central_coord_1']),\n",
        "            int(syn_info['central_coord_2']),\n",
        "            int(syn_info['central_coord_3'])\n",
        "        )\n",
        "        side1_coord = (\n",
        "            int(syn_info['side_1_coord_1']),\n",
        "            int(syn_info['side_1_coord_2']),\n",
        "            int(syn_info['side_1_coord_3'])\n",
        "        )\n",
        "        side2_coord = (\n",
        "            int(syn_info['side_2_coord_1']),\n",
        "            int(syn_info['side_2_coord_2']),\n",
        "            int(syn_info['side_2_coord_3'])\n",
        "        )\n",
        "\n",
        "        # Create the overlaid segmented cube\n",
        "        overlaid_cube = create_segmented_cube(\n",
        "            raw_vol=raw_vol,\n",
        "            seg_vol=seg_vol,\n",
        "            central_coord=central_coord,\n",
        "            side1_coord=side1_coord,\n",
        "            side2_coord=side2_coord,\n",
        "            subvolume_size=self.subvol_size,\n",
        "            alpha=self.alpha\n",
        "        )  # shape: (80, 80, 3, 80)\n",
        "\n",
        "        # We interpret the last dimension (depth) as frames\n",
        "        frames = []\n",
        "        for z in range(overlaid_cube.shape[3]):  # 80 slices\n",
        "            frame_rgb = overlaid_cube[..., z]  # (80, 80, 3)\n",
        "            frames.append(frame_rgb)\n",
        "\n",
        "        # Now reduce or expand to self.num_frames\n",
        "        total_slices = len(frames)  # 80\n",
        "        if total_slices < self.num_frames:\n",
        "            while len(frames) < self.num_frames:\n",
        "                frames.append(frames[-1])\n",
        "        elif total_slices > self.num_frames:\n",
        "            indices = np.linspace(0, total_slices - 1, self.num_frames, dtype=int)\n",
        "            frames = [frames[i] for i in indices]\n",
        "\n",
        "        # Process using the VideoMAEImageProcessor\n",
        "        inputs = self.processor(frames, return_tensors=\"pt\")\n",
        "        pixel_values = inputs[\"pixel_values\"].squeeze(0)  # (num_frames, 3, H, W)\n",
        "        pixel_values = pixel_values.float()\n",
        "\n",
        "        # For MAE, target is the same\n",
        "        return pixel_values, pixel_values\n",
        "\n",
        "\n",
        "def generate_masked_positions(batch_size, sequence_length, mask_ratio=0.75, device='cuda'):\n",
        "    \"\"\"\n",
        "    Generate a boolean mask indicating which positions are masked.\n",
        "    \"\"\"\n",
        "    masks = torch.zeros(batch_size, sequence_length, dtype=torch.bool, device=device)\n",
        "    num_mask = int(mask_ratio * sequence_length)\n",
        "    for i in range(batch_size):\n",
        "        mask_indices = torch.randperm(sequence_length, device=device)[:num_mask]\n",
        "        masks[i, mask_indices] = True\n",
        "    return masks\n",
        "\n",
        "\n",
        "def log_input_gifs(pixel_values, epoch, prefix=\"Training\"):\n",
        "    \"\"\"\n",
        "    Convert input pixel values to GIFs and log them to WandB.\n",
        "    \"\"\"\n",
        "    # Ensure pixel_values is on CPU and convert to numpy\n",
        "    pixel_values = pixel_values.cpu().numpy()\n",
        "\n",
        "    gifs = []\n",
        "    for i in range(min(2, pixel_values.shape[0])):  # just take up to 2 samples for logging\n",
        "        frames = pixel_values[i]  # shape: (num_frames, 3, height, width)\n",
        "\n",
        "        # Normalize frames to [0, 255]\n",
        "        frames = frames - frames.min()\n",
        "        if frames.max() != 0:\n",
        "            frames = frames / frames.max()\n",
        "        frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "        # Rearrange to (num_frames, height, width, 3)\n",
        "        frames = frames.transpose(0, 2, 3, 1)\n",
        "\n",
        "        image_list = [f for f in frames]\n",
        "        gif_buffer = io.BytesIO()\n",
        "        imageio.mimsave(gif_buffer, image_list, format='GIF', fps=5)\n",
        "        gif_buffer.seek(0)\n",
        "        gifs.append(wandb.Video(gif_buffer, format=\"gif\"))\n",
        "\n",
        "    for idx, gif in enumerate(gifs):\n",
        "        wandb.log({f\"{prefix}_Input_Cube_Sample_{idx+1}_Epoch_{epoch}\": gif})\n",
        "\n",
        "\n",
        "def visualize_before_training(dataloader, epoch, prefix=\"Pre-Training Visualization\"):\n",
        "    \"\"\"\n",
        "    Visualize a few samples from the dataloader before training starts.\n",
        "    \"\"\"\n",
        "    print(f\"Starting {prefix}...\")\n",
        "    for batch_idx, (pixel_values, targets) in enumerate(dataloader):\n",
        "        log_input_gifs(pixel_values, epoch=epoch, prefix=prefix)\n",
        "        print(f\"Logged {prefix} for batch {batch_idx + 1}\")\n",
        "        # Only log the first batch\n",
        "        break\n",
        "    print(f\"{prefix} completed.\")\n",
        "\n",
        "\n",
        "def save_sample_gifs(dataloader, save_dir, num_gifs=2, prefix=\"Segmented\"):\n",
        "    \"\"\"\n",
        "    Save a specified number of sample GIFs from the dataloader to a directory.\n",
        "    This will save the segmented cubes as GIFs before training.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    print(f\"Saving {num_gifs} sample GIFs to {save_dir}...\")\n",
        "\n",
        "    saved_gifs = 0\n",
        "    for batch_idx, (pixel_values, targets) in enumerate(dataloader):\n",
        "        # pixel_values shape: (batch_size, num_frames, 3, H, W)\n",
        "        pixel_values = pixel_values.cpu().numpy()\n",
        "        for i in range(pixel_values.shape[0]):\n",
        "            if saved_gifs >= num_gifs:\n",
        "                break\n",
        "            frames = pixel_values[i]  # (num_frames, 3, H, W)\n",
        "\n",
        "            # Normalize to [0, 255]\n",
        "            frames = frames - frames.min()\n",
        "            if frames.max() > 0:\n",
        "                frames = frames / frames.max()\n",
        "            frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "            # Rearrange to (num_frames, H, W, 3)\n",
        "            frames = frames.transpose(0, 2, 3, 1)\n",
        "\n",
        "            image_list = [frame for frame in frames]\n",
        "\n",
        "            gif_filename = f\"{prefix}_Sample_{saved_gifs + 1}.gif\"\n",
        "            gif_path = os.path.join(save_dir, gif_filename)\n",
        "            imageio.mimsave(gif_path, image_list, format='GIF', fps=5)\n",
        "            print(f\"Saved GIF: {gif_path}\")\n",
        "\n",
        "            saved_gifs += 1\n",
        "\n",
        "        if saved_gifs >= num_gifs:\n",
        "            break\n",
        "    print(f\"Successfully saved {saved_gifs} GIF(s) to {save_dir}.\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    wandb.init(\n",
        "        project=args.wandb_project,\n",
        "        entity=args.wandb_entity,\n",
        "        name=args.wandb_run_name,\n",
        "        config={\n",
        "            \"raw_base_dir\": args.raw_base_dir,\n",
        "            \"seg_base_dir\": args.seg_base_dir,\n",
        "            \"csv_output_dir\": args.csv_output_dir,\n",
        "            \"checkpoint_dir\": args.checkpoint_dir,\n",
        "            \"log_dir\": args.log_dir,\n",
        "            \"batch_size\": args.batch_size,\n",
        "            \"num_epochs\": args.num_epochs,\n",
        "            \"learning_rate\": args.learning_rate,\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "            \"subvol_size\": args.subvol_size,\n",
        "            \"num_frames\": args.num_frames,\n",
        "            \"mask_ratio\": args.mask_ratio,\n",
        "            \"patience\": args.patience,\n",
        "            \"resume_checkpoint\": args.resume_checkpoint,\n",
        "        },\n",
        "        save_code=True,\n",
        "    )\n",
        "\n",
        "    os.makedirs(args.csv_output_dir, exist_ok=True)\n",
        "    os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
        "    os.makedirs(args.log_dir, exist_ok=True)\n",
        "\n",
        "    # Directory to save GIFs\n",
        "    saved_gifs_dir = os.path.join(args.log_dir, 'saved_gifs')\n",
        "    os.makedirs(saved_gifs_dir, exist_ok=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model_name = \"MCG-NJU/videomae-base\"\n",
        "    print(\"Initializing VideoMAE model and processor...\")\n",
        "\n",
        "    model_videomae = VideoMAEForPreTraining.from_pretrained(\n",
        "        model_name,\n",
        "        attn_implementation=\"sdpa\",\n",
        "        torch_dtype=torch.float32\n",
        "    ).to(device)\n",
        "\n",
        "    processor_videomae = VideoMAEImageProcessor.from_pretrained(model_name)\n",
        "    model_videomae.train()\n",
        "    print(\"VideoMAE model and processor initialized.\")\n",
        "\n",
        "    # For demonstration, let's assume we have bounding box names and their corresponding Excel files\n",
        "    bbox_names = [f'bbox{i}' for i in range(1, 8)]  # fewer bboxes for a short demo\n",
        "    all_vol_data = []\n",
        "    all_syn_df = []\n",
        "\n",
        "    for bbox_index, bbox_name in enumerate(bbox_names):\n",
        "        print(f\"Loading data for {bbox_name}...\")\n",
        "        raw_vol, seg_vol = load_volumes(bbox_name, args.raw_base_dir, args.seg_base_dir)\n",
        "        if raw_vol is None or seg_vol is None:\n",
        "            print(f\"Skipping {bbox_name} due to loading errors.\")\n",
        "            continue\n",
        "\n",
        "        # Suppose we have an Excel file: bbox1.xlsx, bbox2.xlsx, etc.\n",
        "        excel_file = f\"{bbox_name}.xlsx\"\n",
        "        if not os.path.exists(excel_file):\n",
        "            print(f\"Excel file {excel_file} not found. Skipping {bbox_name}.\")\n",
        "            continue\n",
        "\n",
        "        syn_df = pd.read_excel(excel_file)\n",
        "\n",
        "        # We assume syn_df has columns: central_coord_1/2/3, side_1_coord_1/2/3, side_2_coord_1/2/3, etc.\n",
        "        # We'll just add the bbox_index:\n",
        "        syn_df['bbox_index'] = bbox_index\n",
        "\n",
        "        all_vol_data.append((raw_vol, seg_vol))\n",
        "        all_syn_df.append(syn_df)\n",
        "\n",
        "    if not all_syn_df:\n",
        "        print(\"No synapse data loaded. Exiting.\")\n",
        "        wandb.finish()\n",
        "        return\n",
        "\n",
        "    combined_syn_df = pd.concat(all_syn_df, ignore_index=True)\n",
        "    print(f\"Total synapses loaded: {len(combined_syn_df)}\")\n",
        "\n",
        "    # Split into train/val\n",
        "    train_syn_df, val_syn_df = train_test_split(combined_syn_df, test_size=0.2, random_state=42)\n",
        "    print(f\"Training synapses: {len(train_syn_df)}, Validation synapses: {len(val_syn_df)}\")\n",
        "\n",
        "    # Build Datasets\n",
        "    dataset_videomae_train = VideoMAEDataset(\n",
        "        vol_data_list=all_vol_data,\n",
        "        synapse_df=train_syn_df,\n",
        "        processor=processor_videomae,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=0.3\n",
        "    )\n",
        "    dataset_videomae_val = VideoMAEDataset(\n",
        "        vol_data_list=all_vol_data,\n",
        "        synapse_df=val_syn_df,\n",
        "        processor=processor_videomae,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=0.3\n",
        "    )\n",
        "\n",
        "    num_workers = min(4, multiprocessing.cpu_count())\n",
        "    print(f\"Using {num_workers} workers for DataLoader.\")\n",
        "\n",
        "    dataloader_videomae_train = DataLoader(\n",
        "        dataset_videomae_train,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    dataloader_videomae_val = DataLoader(\n",
        "        dataset_videomae_val,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(\"Saving 2 sample segmented GIFs before training starts...\")\n",
        "    save_sample_gifs(\n",
        "        dataloader=dataloader_videomae_train,\n",
        "        save_dir=saved_gifs_dir,\n",
        "        num_gifs=2,\n",
        "        prefix=\"Segmented\"\n",
        "    )\n",
        "\n",
        "    print(\"Visualizing sample inputs before training starts...\")\n",
        "    visualize_before_training(dataloader_videomae_train, epoch=0, prefix=\"Pre-Training\")\n",
        "    print(\"Visualization completed.\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model_videomae.parameters(),\n",
        "        lr=args.learning_rate,\n",
        "        weight_decay=args.weight_decay\n",
        "    )\n",
        "\n",
        "    total_steps = len(dataloader_videomae_train) * args.num_epochs\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1 * total_steps),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        patience=args.patience,\n",
        "        verbose=True,\n",
        "        path=os.path.join(args.checkpoint_dir, 'best_model.pth')\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    start_epoch = 1\n",
        "    if args.resume_checkpoint:\n",
        "        if os.path.exists(args.resume_checkpoint):\n",
        "            checkpoint = torch.load(args.resume_checkpoint, map_location=device)\n",
        "            model_videomae.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "            start_epoch = checkpoint['epoch'] + 1\n",
        "            print(f\"Resumed training from checkpoint {args.resume_checkpoint} at epoch {start_epoch}\")\n",
        "            wandb.run.summary[\"resumed_from_epoch\"] = start_epoch\n",
        "        else:\n",
        "            print(f\"Checkpoint {args.resume_checkpoint} not found. Starting from scratch.\")\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(start_epoch, args.num_epochs + 1):\n",
        "        model_videomae.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        train_pbar = tqdm(dataloader_videomae_train, desc=f\"Epoch {epoch}/{args.num_epochs} - Train\")\n",
        "        for batch_idx, (pixel_values, targets) in enumerate(train_pbar):\n",
        "            pixel_values = pixel_values.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            tubelet_size = model_videomae.config.tubelet_size\n",
        "            image_size = model_videomae.config.image_size\n",
        "            patch_size = model_videomae.config.patch_size\n",
        "\n",
        "            num_patches_per_frame = (image_size // patch_size) ** 2\n",
        "            num_tubelets = pixel_values.shape[1] // tubelet_size\n",
        "            sequence_length = num_tubelets * num_patches_per_frame\n",
        "\n",
        "            bool_masked_pos = generate_masked_positions(\n",
        "                pixel_values.shape[0],\n",
        "                sequence_length,\n",
        "                mask_ratio=args.mask_ratio,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model_videomae(\n",
        "                    pixel_values=pixel_values,\n",
        "                    bool_masked_pos=bool_masked_pos\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model_videomae.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            train_pbar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(dataloader_videomae_train)\n",
        "        print(f\"Epoch {epoch} - Training Loss: {avg_epoch_loss:.4f}\")\n",
        "        wandb.log({\n",
        "            'epoch': epoch,\n",
        "            'train_loss': avg_epoch_loss,\n",
        "            'learning_rate': scheduler.get_last_lr()[0]\n",
        "        })\n",
        "\n",
        "        # Validation\n",
        "        model_videomae.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            val_pbar = tqdm(dataloader_videomae_val, desc=f\"Epoch {epoch}/{args.num_epochs} - Val\")\n",
        "            for batch_idx, (pixel_values, targets) in enumerate(val_pbar):\n",
        "                pixel_values = pixel_values.to(device)\n",
        "\n",
        "                tubelet_size = model_videomae.config.tubelet_size\n",
        "                image_size = model_videomae.config.image_size\n",
        "                patch_size = model_videomae.config.patch_size\n",
        "\n",
        "                num_patches_per_frame = (image_size // patch_size) ** 2\n",
        "                num_tubelets = pixel_values.shape[1] // tubelet_size\n",
        "                sequence_length = num_tubelets * num_patches_per_frame\n",
        "\n",
        "                bool_masked_pos = generate_masked_positions(\n",
        "                    pixel_values.shape[0],\n",
        "                    sequence_length,\n",
        "                    mask_ratio=args.mask_ratio,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                outputs = model_videomae(\n",
        "                    pixel_values=pixel_values,\n",
        "                    bool_masked_pos=bool_masked_pos\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(dataloader_videomae_val)\n",
        "            print(f\"Epoch {epoch} - Validation Loss: {avg_val_loss:.4f}\")\n",
        "            wandb.log({'val_loss': avg_val_loss, 'epoch': epoch})\n",
        "\n",
        "        # Early stopping\n",
        "        early_stopping(avg_val_loss, model_videomae, optimizer, scheduler, epoch)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            wandb.run.summary[\"early_stopped_at_epoch\"] = epoch\n",
        "            break\n",
        "\n",
        "        # Save checkpoint every epoch (you can adjust frequency)\n",
        "        checkpoint_path = os.path.join(args.checkpoint_dir, f'epoch_{epoch}.pth')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model_videomae.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss': avg_epoch_loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "        wandb.save(checkpoint_path)\n",
        "\n",
        "    # Final save\n",
        "    final_model_path = os.path.join(args.checkpoint_dir, 'final_checkpoint.pth')\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model_videomae.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': avg_epoch_loss,\n",
        "    }, final_model_path)\n",
        "    print(f\"Training completed. Final checkpoint saved at {final_model_path}\")\n",
        "    wandb.save(final_model_path)\n",
        "\n",
        "    artifact = wandb.Artifact('final_model', type='model')\n",
        "    artifact.add_file(final_model_path)\n",
        "    wandb.log_artifact(artifact)\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "849f580106ae4d619099269c67c24983",
            "ab05fddbae8f4a89930ca375fd2da70e",
            "47e0dce8189241dea6fd4cf755117eee",
            "863c6c615d804d8db76a232c6d248583",
            "6f7812943c0047c3873fb4bd2041fc53",
            "24e17ca43ff847afb2a73c9f15f43f3f",
            "d0d4d285b28143a485f1e8b5203dd291",
            "24adf96ff6a44d91b24dfe5219d4e67e",
            "ba66cece06d549b982930fffe2b68a41",
            "84af5d595bf94000924ae77836fc7ebc",
            "956d6516843042c0a5a7de95128d4e66",
            "dfefd5f567cc4489a809b1925c9ad99f",
            "468e3d1baaee4d04a3d6c4baa70dfc4c",
            "4b6a6bedd15649ae800f9ffa37c16405",
            "ab948544bfb44d5898d2aacdcfb86fd9",
            "0f5d378c89fa4270853738d432eef7d0",
            "36e47a460ca64c2c9cd25fd5efe6a268",
            "ed17c2dec864496199a8e1ae2327d274",
            "d56bf6cbead446508b6512317429cf25",
            "cf988f5aa564469b90d81a7f0dcdbe41",
            "7a96fcd9f2e648268bb43aaf02ca2ca7",
            "df8c9de064724bb0b3336c653a105572",
            "4f46bc40dc404ebfb758360caa27081b",
            "7c396f22ebe74d8585fea42f72858b77",
            "e222e95a1ec64a54be1ed7bfe37f62e3",
            "77d4640d285c4793a31d4b3773a42f8a",
            "65007a23829a431f80d9dc83cdd126e0",
            "8aa1336e1747473bad52dbf59da2613c",
            "b602cf9fa6994257bde1c2a3dc16a1df",
            "1cafbdfd41d74e3285c234fcda5bde28",
            "9e320c6f2df14b26bf8baeca908fa910",
            "ecc77165738b4de1b33c813b6cceb079",
            "d618b6c567644d169f84bb9438fdc9e9"
          ]
        },
        "id": "q5oXbuXDdkGN",
        "outputId": "8f392f7f-a9ac-45cb-d76c-f4c2fbd7b423"
      },
      "execution_count": 3,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250113_181901-0l31xk1m</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alim98barnet-university-of-tehran/VideoMAE_PreTraining/runs/0l31xk1m' target=\"_blank\">honest-forest-13</a></strong> to <a href='https://wandb.ai/alim98barnet-university-of-tehran/VideoMAE_PreTraining' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/alim98barnet-university-of-tehran/VideoMAE_PreTraining' target=\"_blank\">https://wandb.ai/alim98barnet-university-of-tehran/VideoMAE_PreTraining</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/alim98barnet-university-of-tehran/VideoMAE_PreTraining/runs/0l31xk1m' target=\"_blank\">https://wandb.ai/alim98barnet-university-of-tehran/VideoMAE_PreTraining/runs/0l31xk1m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Initializing VideoMAE model and processor...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "849f580106ae4d619099269c67c24983",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfefd5f567cc4489a809b1925c9ad99f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/377M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f46bc40dc404ebfb758360caa27081b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VideoMAE model and processor initialized.\n",
            "Loading data for bbox1...\n",
            "Loading data for bbox2...\n",
            "Loading data for bbox3...\n",
            "Loading data for bbox4...\n",
            "Loading data for bbox5...\n",
            "Loading data for bbox6...\n",
            "Loading data for bbox7...\n",
            "Total synapses loaded: 509\n",
            "Training synapses: 407, Validation synapses: 102\n",
            "Using 4 workers for DataLoader.\n",
            "Saving 2 sample segmented GIFs before training starts...\n",
            "Saving 2 sample GIFs to logs/saved_gifs...\n",
            "Saved GIF: logs/saved_gifs/Segmented_Sample_1.gif\n",
            "Saved GIF: logs/saved_gifs/Segmented_Sample_2.gif\n",
            "Successfully saved 2 GIF(s) to logs/saved_gifs.\n",
            "Visualizing sample inputs before training starts...\n",
            "Starting Pre-Training...\n",
            "Logged Pre-Training for batch 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-565b342f443c>:605: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-Training completed.\n",
            "Visualization completed.\n",
            "Starting training...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 - Train:   0%|          | 0/204 [00:00<?, ?it/s]<ipython-input-3-565b342f443c>:645: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 1/100 - Train: 100%|██████████| 204/204 [00:37<00:00,  5.39it/s, loss=0.295, lr=1e-5]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Training Loss: 0.2774\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 - Val: 100%|██████████| 51/51 [00:08<00:00,  5.80it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Validation Loss: 0.2716\n",
            "Validation loss decreased. Saving model to checkpoints/best_model.pth\n",
            "Checkpoint saved at checkpoints/epoch_1.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/100 - Train: 100%|██████████| 204/204 [00:36<00:00,  5.53it/s, loss=0.263, lr=2e-5]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Training Loss: 0.2672\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/100 - Val: 100%|██████████| 51/51 [00:09<00:00,  5.34it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Validation Loss: 0.2651\n",
            "Validation loss decreased. Saving model to checkpoints/best_model.pth\n",
            "Checkpoint saved at checkpoints/epoch_2.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/100 - Train: 100%|██████████| 204/204 [00:36<00:00,  5.62it/s, loss=0.297, lr=3e-5]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Training Loss: 0.2629\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/100 - Val: 100%|██████████| 51/51 [00:08<00:00,  5.80it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Validation Loss: 0.2639\n",
            "Validation loss decreased. Saving model to checkpoints/best_model.pth\n",
            "Checkpoint saved at checkpoints/epoch_3.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/100 - Train: 100%|██████████| 204/204 [00:36<00:00,  5.57it/s, loss=0.248, lr=4e-5]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Training Loss: 0.2600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/100 - Val: 100%|██████████| 51/51 [00:09<00:00,  5.41it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Validation Loss: 0.2594\n",
            "Validation loss decreased. Saving model to checkpoints/best_model.pth\n",
            "Checkpoint saved at checkpoints/epoch_4.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/100 - Train: 100%|██████████| 204/204 [00:36<00:00,  5.61it/s, loss=0.251, lr=5e-5]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Training Loss: 0.2581\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/100 - Val: 100%|██████████| 51/51 [00:08<00:00,  5.81it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Validation Loss: 0.2593\n",
            "Validation loss decreased. Saving model to checkpoints/best_model.pth\n",
            "Checkpoint saved at checkpoints/epoch_5.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/100 - Train: 100%|██████████| 204/204 [00:36<00:00,  5.57it/s, loss=0.245, lr=6e-5]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 - Training Loss: 0.2564\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/100 - Val: 100%|██████████| 51/51 [00:09<00:00,  5.49it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 - Validation Loss: 0.2575\n",
            "Validation loss decreased. Saving model to checkpoints/best_model.pth\n",
            "Checkpoint saved at checkpoints/epoch_6.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/100 - Train: 100%|██████████| 204/204 [00:36<00:00,  5.55it/s, loss=0.294, lr=7e-5]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 - Training Loss: 0.2551\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/100 - Val: 100%|██████████| 51/51 [00:08<00:00,  5.94it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 - Validation Loss: 0.2565\n",
            "Validation loss decreased. Saving model to checkpoints/best_model.pth\n",
            "Checkpoint saved at checkpoints/epoch_7.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/100 - Train: 100%|██████████| 204/204 [00:37<00:00,  5.49it/s, loss=0.276, lr=8e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Training Loss: 0.2534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/100 - Val: 100%|██████████| 51/51 [00:09<00:00,  5.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Validation Loss: 0.2561\n",
            "Validation loss decreased. Saving model to checkpoints/best_model.pth\n",
            "Checkpoint saved at checkpoints/epoch_8.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/100 - Train: 100%|██████████| 204/204 [00:37<00:00,  5.47it/s, loss=0.204, lr=9e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Training Loss: 0.2523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/100 - Val: 100%|██████████| 51/51 [00:08<00:00,  5.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Validation Loss: 0.2548\n",
            "Validation loss decreased. Saving model to checkpoints/best_model.pth\n",
            "Checkpoint saved at checkpoints/epoch_9.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/100 - Train: 100%|██████████| 204/204 [00:37<00:00,  5.48it/s, loss=0.262, lr=0.0001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Training Loss: 0.2513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/100 - Val: 100%|██████████| 51/51 [00:08<00:00,  5.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Validation Loss: 0.2555\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Checkpoint saved at checkpoints/epoch_10.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/100 - Train: 100%|██████████| 204/204 [00:36<00:00,  5.55it/s, loss=0.228, lr=0.0001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 - Training Loss: 0.2499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/100 - Val: 100%|██████████| 51/51 [00:08<00:00,  5.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 - Validation Loss: 0.2545\n",
            "Validation loss decreased. Saving model to checkpoints/best_model.pth\n",
            "Checkpoint saved at checkpoints/epoch_11.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/100 - Train: 100%|██████████| 204/204 [00:36<00:00,  5.55it/s, loss=0.204, lr=9.99e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 - Training Loss: 0.2483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/100 - Val: 100%|██████████| 51/51 [00:09<00:00,  5.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 - Validation Loss: 0.2550\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Checkpoint saved at checkpoints/epoch_12.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/100 - Train: 100%|██████████| 204/204 [00:37<00:00,  5.42it/s, loss=0.246, lr=9.97e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 - Training Loss: 0.2468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/100 - Val: 100%|██████████| 51/51 [00:08<00:00,  5.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 - Validation Loss: 0.2519\n",
            "Validation loss decreased. Saving model to checkpoints/best_model.pth\n",
            "Checkpoint saved at checkpoints/epoch_13.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/100 - Train: 100%|██████████| 204/204 [00:36<00:00,  5.51it/s, loss=0.206, lr=9.95e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 - Training Loss: 0.2451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/100 - Val: 100%|██████████| 51/51 [00:09<00:00,  5.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 - Validation Loss: 0.2517\n",
            "Validation loss decreased. Saving model to checkpoints/best_model.pth\n",
            "Checkpoint saved at checkpoints/epoch_14.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/100 - Train: 100%|██████████| 204/204 [00:36<00:00,  5.55it/s, loss=0.219, lr=9.92e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 - Training Loss: 0.2438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/100 - Val: 100%|██████████| 51/51 [00:09<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 - Validation Loss: 0.2490\n",
            "Validation loss decreased. Saving model to checkpoints/best_model.pth\n",
            "Checkpoint saved at checkpoints/epoch_15.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/100 - Train: 100%|██████████| 204/204 [00:36<00:00,  5.56it/s, loss=0.262, lr=9.89e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 - Training Loss: 0.2429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/100 - Val: 100%|██████████| 51/51 [00:08<00:00,  5.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 - Validation Loss: 0.2497\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Checkpoint saved at checkpoints/epoch_16.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/100 - Train: 100%|██████████| 204/204 [00:36<00:00,  5.53it/s, loss=0.221, lr=9.85e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 - Training Loss: 0.2417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/100 - Val: 100%|██████████| 51/51 [00:09<00:00,  5.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 - Validation Loss: 0.2494\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Checkpoint saved at checkpoints/epoch_17.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/100 - Train: 100%|██████████| 204/204 [00:37<00:00,  5.50it/s, loss=0.214, lr=9.81e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 - Training Loss: 0.2401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/100 - Val: 100%|██████████| 51/51 [00:09<00:00,  5.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 - Validation Loss: 0.2492\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping triggered.\n",
            "Training completed. Final checkpoint saved at checkpoints/final_checkpoint.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>▁▂▃▃▄▅▆▆▇█████████</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>val_loss</td><td>█▆▆▄▄▄▃▃▃▃▃▃▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>early_stopped_at_epoch</td><td>18</td></tr><tr><td>epoch</td><td>18</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train_loss</td><td>0.24009</td></tr><tr><td>val_loss</td><td>0.24924</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">honest-forest-13</strong> at: <a href='https://wandb.ai/alim98barnet-university-of-tehran/VideoMAE_PreTraining/runs/0l31xk1m' target=\"_blank\">https://wandb.ai/alim98barnet-university-of-tehran/VideoMAE_PreTraining/runs/0l31xk1m</a><br> View project at: <a href='https://wandb.ai/alim98barnet-university-of-tehran/VideoMAE_PreTraining' target=\"_blank\">https://wandb.ai/alim98barnet-university-of-tehran/VideoMAE_PreTraining</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 20 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250113_181901-0l31xk1m/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run colored Seg"
      ],
      "metadata": {
        "id": "UbZFxIJIttF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> to do\n",
        ">\n",
        ">\n",
        ">1.   add Lr-scheduler\n",
        ">2.   use large model\n",
        "\n"
      ],
      "metadata": {
        "id": "n628GE0TEqSH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXKAaxjHZz_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   vis samples\n",
        "*  added wandb\n",
        "\n"
      ],
      "metadata": {
        "id": "O9WpiGp6ZyXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import imageio.v2 as iio\n",
        "from transformers import (\n",
        "    VideoMAEForPreTraining,\n",
        "    VideoMAEImageProcessor,\n",
        "    get_cosine_schedule_with_warmup,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "import umap.umap_ as umap  # Not necessarily used, but left for completeness\n",
        "import plotly.express as px  # Not necessarily used, but left for completeness\n",
        "import plotly.graph_objects as go  # Not necessarily used, but left for completeness\n",
        "from plotly.subplots import make_subplots  # Not necessarily used, but left for completeness\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from collections import deque\n",
        "import time\n",
        "import multiprocessing\n",
        "import imageio\n",
        "import wandb\n",
        "import io  # For in-memory file handling\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.data.dataloader\")\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    \"\"\"\n",
        "    Parse command-line arguments for configurable paths and training parameters.\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"VideoMAE Pre-training Script with Segmented Videos\")\n",
        "    parser.add_argument('--raw_base_dir', type=str, default='./raw', help='Path to raw data directory')\n",
        "    parser.add_argument('--seg_base_dir', type=str, default='./seg', help='Path to segmentation data directory')\n",
        "    parser.add_argument('--csv_output_dir', type=str, default='csv_outputs', help='Directory to save CSV outputs')\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints', help='Directory to save model checkpoints')\n",
        "    parser.add_argument('--log_dir', type=str, default='logs', help='Directory for TensorBoard logs')\n",
        "    parser.add_argument('--batch_size', type=int, default=2, help='Batch size for training')\n",
        "    parser.add_argument('--num_epochs', type=int, default=5, help='Number of training epochs')\n",
        "    parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate for optimizer')\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-2, help='Weight decay for optimizer')\n",
        "    parser.add_argument('--subvol_size', type=int, default=80, help='Size of the sub-volume to extract')\n",
        "    parser.add_argument('--num_frames', type=int, default=16, help='Number of frames per video clip')\n",
        "    parser.add_argument('--mask_ratio', type=float, default=0.75, help='Mask ratio for VideoMAE')\n",
        "    parser.add_argument('--patience', type=int, default=3, help='Patience for early stopping')\n",
        "    parser.add_argument('--resume_checkpoint', type=str, default=None, help='Path to resume checkpoint')\n",
        "\n",
        "    # Optionally add WandB specific arguments\n",
        "    parser.add_argument('--wandb_project', type=str, default='VideoMAE_PreTraining', help='WandB project name')\n",
        "    parser.add_argument('--wandb_entity', type=str, default=None, help='WandB entity/team name')\n",
        "    parser.add_argument('--wandb_run_name', type=str, default=None, help='WandB run name')\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Early stopping utility to halt training when validation loss stops improving.\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=10, verbose=False, delta=0.0, path='checkpoint.pth'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, current_loss, model, optimizer, scheduler, epoch):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = current_loss\n",
        "            self.save_checkpoint(model, optimizer, scheduler, epoch, current_loss)\n",
        "        elif current_loss > self.best_loss - self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = current_loss\n",
        "            self.save_checkpoint(model, optimizer, scheduler, epoch, current_loss)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model, optimizer, scheduler, epoch, loss):\n",
        "        \"\"\"\n",
        "        Saves model when validation loss decreases.\n",
        "        \"\"\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss': loss,\n",
        "        }, self.path)\n",
        "        if self.verbose:\n",
        "            print(f\"Validation loss decreased. Saving model to {self.path}\")\n",
        "\n",
        "\n",
        "def load_volumes(bbox_name, raw_base_dir, seg_base_dir):\n",
        "    \"\"\"\n",
        "    Load raw volume and segmentation volume for a bounding box.\n",
        "\n",
        "    Args:\n",
        "        bbox_name (str): Name of the bounding box directory.\n",
        "        raw_base_dir (str): Base directory for raw data.\n",
        "        seg_base_dir (str): Base directory for segmentation data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (raw_vol, seg_vol) each as np.ndarray\n",
        "    \"\"\"\n",
        "    raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "    seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "\n",
        "    raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "    seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "\n",
        "    if len(raw_tif_files) == 0:\n",
        "        print(f\"No raw files found for {bbox_name} in {raw_dir}\")\n",
        "        return None, None\n",
        "\n",
        "    if len(seg_tif_files) == 0:\n",
        "        print(f\"No segmentation files found for {bbox_name} in {seg_dir}\")\n",
        "        return None, None\n",
        "\n",
        "    if len(raw_tif_files) != len(seg_tif_files):\n",
        "        print(f\"Mismatch in number of raw vs seg slices for {bbox_name}. Skipping.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        raw_vol = np.stack([iio.imread(f) for f in raw_tif_files], axis=0)  # shape: (Z, Y, X)\n",
        "        seg_vol = np.stack([iio.imread(f).astype(np.uint32) for f in seg_tif_files], axis=0)\n",
        "        return raw_vol, seg_vol\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading volumes for {bbox_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def create_segmented_cube(\n",
        "    raw_vol,\n",
        "    seg_vol,\n",
        "    central_coord,\n",
        "    side1_coord,\n",
        "    side2_coord,\n",
        "    subvolume_size=80,\n",
        "    alpha=0.3\n",
        "):\n",
        "    \"\"\"\n",
        "    Constructs an 80x80x80 segmented 3D cube around the specified synapse coordinates\n",
        "    and overlays both segmentation masks (side1_coord, side2_coord) on the raw data\n",
        "    with specified transparency for each slice.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Overlaid cube of shape (height, width, 3, depth),\n",
        "                    i.e., (80, 80, 3, 80) if subvolume_size=80.\n",
        "    \"\"\"\n",
        "\n",
        "    def create_segment_masks(segmentation_volume, s1_coord, s2_coord):\n",
        "        x1, y1, z1 = s1_coord\n",
        "        x2, y2, z2 = s2_coord\n",
        "        # Validate within volume\n",
        "        if not (0 <= z1 < segmentation_volume.shape[0] and\n",
        "                0 <= y1 < segmentation_volume.shape[1] and\n",
        "                0 <= x1 < segmentation_volume.shape[2]):\n",
        "            raise ValueError(\"Side1 coordinates are out of bounds.\")\n",
        "\n",
        "        if not (0 <= z2 < segmentation_volume.shape[0] and\n",
        "                0 <= y2 < segmentation_volume.shape[1] and\n",
        "                0 <= x2 < segmentation_volume.shape[2]):\n",
        "            raise ValueError(\"Side2 coordinates are out of bounds.\")\n",
        "\n",
        "        seg_id_1 = segmentation_volume[z1, y1, x1]\n",
        "        seg_id_2 = segmentation_volume[z2, y2, x2]\n",
        "\n",
        "        # If seg_id == 0, it means no segment at that voxel\n",
        "        if seg_id_1 == 0:\n",
        "            mask_1 = np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        else:\n",
        "            mask_1 = (segmentation_volume == seg_id_1)\n",
        "\n",
        "        if seg_id_2 == 0:\n",
        "            mask_2 = np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        else:\n",
        "            mask_2 = (segmentation_volume == seg_id_2)\n",
        "\n",
        "        return mask_1, mask_2\n",
        "\n",
        "    # Build masks\n",
        "    mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "\n",
        "    # Define subvolume bounds\n",
        "    half_size = subvolume_size // 2\n",
        "    cx, cy, cz = central_coord\n",
        "\n",
        "    x_start, x_end = max(cx - half_size, 0), min(cx + half_size, raw_vol.shape[2])\n",
        "    y_start, y_end = max(cy - half_size, 0), min(cy + half_size, raw_vol.shape[1])\n",
        "    z_start, z_end = max(cz - half_size, 0), min(cz + half_size, raw_vol.shape[0])\n",
        "\n",
        "    # Extract subvolumes\n",
        "    sub_raw = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_1 = mask_1_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_2 = mask_2_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "    # Pad if smaller than subvolume_size\n",
        "    pad_z = subvolume_size - sub_raw.shape[0]\n",
        "    pad_y = subvolume_size - sub_raw.shape[1]\n",
        "    pad_x = subvolume_size - sub_raw.shape[2]\n",
        "\n",
        "    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n",
        "        sub_raw = np.pad(sub_raw, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                         mode='constant', constant_values=0)\n",
        "        sub_mask_1 = np.pad(sub_mask_1, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                            mode='constant', constant_values=False)\n",
        "        sub_mask_2 = np.pad(sub_mask_2, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                            mode='constant', constant_values=False)\n",
        "\n",
        "    # Slice to exact shape\n",
        "    sub_raw = sub_raw[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_1 = sub_mask_1[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_2 = sub_mask_2[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "\n",
        "    # We'll build an overlaid cube: shape => (H, W, 3, D)\n",
        "    overlaid_cube = np.zeros((subvolume_size, subvolume_size, 3, subvolume_size), dtype=np.uint8)\n",
        "\n",
        "    # Colors\n",
        "    side1_color = np.array([0, 0, 0], dtype=np.float32)  # Red\n",
        "    side2_color = np.array([0, 0, 1], dtype=np.float32)  # Blue\n",
        "\n",
        "    for z in range(subvolume_size):\n",
        "        # Normalize raw slice to [0, 1]\n",
        "        raw_slice = sub_raw[z].astype(np.float32)\n",
        "        mn, mx = raw_slice.min(), raw_slice.max()\n",
        "        if mx > mn:\n",
        "            raw_slice = (raw_slice - mn) / (mx - mn)\n",
        "        else:\n",
        "            raw_slice = raw_slice - mn  # all zeros if mn=mx\n",
        "\n",
        "        raw_rgb = np.stack([raw_slice]*3, axis=-1)  # shape (H, W, 3)\n",
        "\n",
        "        # Build colored masks\n",
        "        mask1_rgb = np.zeros_like(raw_rgb)\n",
        "        mask1_rgb[sub_mask_1[z]] = side1_color\n",
        "\n",
        "        mask2_rgb = np.zeros_like(raw_rgb)\n",
        "        mask2_rgb[sub_mask_2[z]] = side2_color\n",
        "\n",
        "        # Blend\n",
        "        overlaid_image = (1 - alpha) * raw_rgb + alpha * (mask1_rgb + mask2_rgb)\n",
        "        overlaid_image = np.clip(overlaid_image, 0, 1)\n",
        "\n",
        "        overlaid_image = (overlaid_image * 255).astype(np.uint8)\n",
        "        overlaid_cube[:, :, :, z] = overlaid_image\n",
        "\n",
        "    return overlaid_cube\n",
        "\n",
        "\n",
        "class VideoMAEDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class that uses segmented volumes (side1 & side2) for VideoMAE pre-training.\n",
        "    \"\"\"\n",
        "    def __init__(self, vol_data_list, synapse_df, processor, subvol_size=80, num_frames=16, alpha=0.3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vol_data_list (List[Tuple[np.ndarray, np.ndarray]]): List of (raw_vol, seg_vol).\n",
        "            synapse_df (pd.DataFrame): DataFrame with synapse coordinates (central, side1, side2).\n",
        "            processor (VideoMAEImageProcessor): Processor for VideoMAE.\n",
        "            subvol_size (int): Size of the sub-volume to extract.\n",
        "            num_frames (int): Number of frames for the model.\n",
        "            alpha (float): Blending alpha for segmentation.\n",
        "        \"\"\"\n",
        "        self.vol_data_list = vol_data_list\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.subvol_size = subvol_size\n",
        "        self.num_frames = num_frames\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        syn_info = self.synapse_df.iloc[idx]\n",
        "        bbox_index = syn_info['bbox_index']\n",
        "\n",
        "        raw_vol, seg_vol = self.vol_data_list[bbox_index]\n",
        "        if raw_vol is None or seg_vol is None:\n",
        "            # Return dummy data if volumes not found\n",
        "            pixel_values = torch.zeros((self.num_frames, 3, self.subvol_size, self.subvol_size), dtype=torch.float32)\n",
        "            return pixel_values, pixel_values\n",
        "\n",
        "        # Coordinates\n",
        "        central_coord = (\n",
        "            int(syn_info['central_coord_1']),\n",
        "            int(syn_info['central_coord_2']),\n",
        "            int(syn_info['central_coord_3'])\n",
        "        )\n",
        "        side1_coord = (\n",
        "            int(syn_info['side_1_coord_1']),\n",
        "            int(syn_info['side_1_coord_2']),\n",
        "            int(syn_info['side_1_coord_3'])\n",
        "        )\n",
        "        side2_coord = (\n",
        "            int(syn_info['side_2_coord_1']),\n",
        "            int(syn_info['side_2_coord_2']),\n",
        "            int(syn_info['side_2_coord_3'])\n",
        "        )\n",
        "\n",
        "        # Create the overlaid segmented cube\n",
        "        overlaid_cube = create_segmented_cube(\n",
        "            raw_vol=raw_vol,\n",
        "            seg_vol=seg_vol,\n",
        "            central_coord=central_coord,\n",
        "            side1_coord=side1_coord,\n",
        "            side2_coord=side2_coord,\n",
        "            subvolume_size=self.subvol_size,\n",
        "            alpha=self.alpha\n",
        "        )  # shape: (80, 80, 3, 80)\n",
        "\n",
        "        # We interpret the last dimension (depth) as frames\n",
        "        frames = []\n",
        "        for z in range(overlaid_cube.shape[3]):  # 80 slices\n",
        "            frame_rgb = overlaid_cube[..., z]  # (80, 80, 3)\n",
        "            frames.append(frame_rgb)\n",
        "\n",
        "        # Now reduce or expand to self.num_frames\n",
        "        total_slices = len(frames)  # 80\n",
        "        if total_slices < self.num_frames:\n",
        "            while len(frames) < self.num_frames:\n",
        "                frames.append(frames[-1])\n",
        "        elif total_slices > self.num_frames:\n",
        "            indices = np.linspace(0, total_slices - 1, self.num_frames, dtype=int)\n",
        "            frames = [frames[i] for i in indices]\n",
        "\n",
        "        # Process using the VideoMAEImageProcessor\n",
        "        inputs = self.processor(frames, return_tensors=\"pt\")\n",
        "        pixel_values = inputs[\"pixel_values\"].squeeze(0)  # (num_frames, 3, H, W)\n",
        "        pixel_values = pixel_values.float()\n",
        "\n",
        "        # For MAE, target is the same\n",
        "        return pixel_values, pixel_values\n",
        "\n",
        "\n",
        "def generate_masked_positions(batch_size, sequence_length, mask_ratio=0.75, device='cuda'):\n",
        "    \"\"\"\n",
        "    Generate a boolean mask indicating which positions are masked.\n",
        "    \"\"\"\n",
        "    masks = torch.zeros(batch_size, sequence_length, dtype=torch.bool, device=device)\n",
        "    num_mask = int(mask_ratio * sequence_length)\n",
        "    for i in range(batch_size):\n",
        "        mask_indices = torch.randperm(sequence_length, device=device)[:num_mask]\n",
        "        masks[i, mask_indices] = True\n",
        "    return masks\n",
        "\n",
        "\n",
        "def log_input_gifs(pixel_values, epoch, prefix=\"Training\"):\n",
        "    \"\"\"\n",
        "    Convert input pixel values to GIFs and log them to WandB.\n",
        "    \"\"\"\n",
        "    # Ensure pixel_values is on CPU and convert to numpy\n",
        "    pixel_values = pixel_values.cpu().numpy()\n",
        "\n",
        "    gifs = []\n",
        "    for i in range(min(2, pixel_values.shape[0])):  # just take up to 2 samples for logging\n",
        "        frames = pixel_values[i]  # shape: (num_frames, 3, height, width)\n",
        "\n",
        "        # Normalize frames to [0, 255]\n",
        "        frames = frames - frames.min()\n",
        "        if frames.max() != 0:\n",
        "            frames = frames / frames.max()\n",
        "        frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "        # Rearrange to (num_frames, height, width, 3)\n",
        "        frames = frames.transpose(0, 2, 3, 1)\n",
        "\n",
        "        image_list = [f for f in frames]\n",
        "        gif_buffer = io.BytesIO()\n",
        "        imageio.mimsave(gif_buffer, image_list, format='GIF', fps=5)\n",
        "        gif_buffer.seek(0)\n",
        "        gifs.append(wandb.Video(gif_buffer, format=\"gif\"))\n",
        "\n",
        "    for idx, gif in enumerate(gifs):\n",
        "        wandb.log({f\"{prefix}_Input_Cube_Sample_{idx+1}_Epoch_{epoch}\": gif})\n",
        "\n",
        "\n",
        "def visualize_before_training(dataloader, epoch, prefix=\"Pre-Training Visualization\"):\n",
        "    \"\"\"\n",
        "    Visualize a few samples from the dataloader before training starts.\n",
        "    \"\"\"\n",
        "    print(f\"Starting {prefix}...\")\n",
        "    for batch_idx, (pixel_values, targets) in enumerate(dataloader):\n",
        "        log_input_gifs(pixel_values, epoch=epoch, prefix=prefix)\n",
        "        print(f\"Logged {prefix} for batch {batch_idx + 1}\")\n",
        "        # Only log the first batch\n",
        "        break\n",
        "    print(f\"{prefix} completed.\")\n",
        "\n",
        "\n",
        "def save_sample_gifs(dataloader, save_dir, num_gifs=2, prefix=\"Segmented\"):\n",
        "    \"\"\"\n",
        "    Save a specified number of sample GIFs from the dataloader to a directory.\n",
        "    This will save the segmented cubes as GIFs before training.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    print(f\"Saving {num_gifs} sample GIFs to {save_dir}...\")\n",
        "\n",
        "    saved_gifs = 0\n",
        "    for batch_idx, (pixel_values, targets) in enumerate(dataloader):\n",
        "        # pixel_values shape: (batch_size, num_frames, 3, H, W)\n",
        "        pixel_values = pixel_values.cpu().numpy()\n",
        "        for i in range(pixel_values.shape[0]):\n",
        "            if saved_gifs >= num_gifs:\n",
        "                break\n",
        "            frames = pixel_values[i]  # (num_frames, 3, H, W)\n",
        "\n",
        "            # Normalize to [0, 255]\n",
        "            frames = frames - frames.min()\n",
        "            if frames.max() > 0:\n",
        "                frames = frames / frames.max()\n",
        "            frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "            # Rearrange to (num_frames, H, W, 3)\n",
        "            frames = frames.transpose(0, 2, 3, 1)\n",
        "\n",
        "            image_list = [frame for frame in frames]\n",
        "\n",
        "            gif_filename = f\"{prefix}_Sample_{saved_gifs + 1}.gif\"\n",
        "            gif_path = os.path.join(save_dir, gif_filename)\n",
        "            imageio.mimsave(gif_path, image_list, format='GIF', fps=5)\n",
        "            print(f\"Saved GIF: {gif_path}\")\n",
        "\n",
        "            saved_gifs += 1\n",
        "\n",
        "        if saved_gifs >= num_gifs:\n",
        "            break\n",
        "    print(f\"Successfully saved {saved_gifs} GIF(s) to {save_dir}.\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    wandb.init(\n",
        "        project=args.wandb_project,\n",
        "        entity=args.wandb_entity,\n",
        "        name=args.wandb_run_name,\n",
        "        config={\n",
        "            \"raw_base_dir\": args.raw_base_dir,\n",
        "            \"seg_base_dir\": args.seg_base_dir,\n",
        "            \"csv_output_dir\": args.csv_output_dir,\n",
        "            \"checkpoint_dir\": args.checkpoint_dir,\n",
        "            \"log_dir\": args.log_dir,\n",
        "            \"batch_size\": args.batch_size,\n",
        "            \"num_epochs\": args.num_epochs,\n",
        "            \"learning_rate\": args.learning_rate,\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "            \"subvol_size\": args.subvol_size,\n",
        "            \"num_frames\": args.num_frames,\n",
        "            \"mask_ratio\": args.mask_ratio,\n",
        "            \"patience\": args.patience,\n",
        "            \"resume_checkpoint\": args.resume_checkpoint,\n",
        "        },\n",
        "        save_code=True,\n",
        "    )\n",
        "\n",
        "    os.makedirs(args.csv_output_dir, exist_ok=True)\n",
        "    os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
        "    os.makedirs(args.log_dir, exist_ok=True)\n",
        "\n",
        "    # Directory to save GIFs\n",
        "    saved_gifs_dir = os.path.join(args.log_dir, 'saved_gifs')\n",
        "    os.makedirs(saved_gifs_dir, exist_ok=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model_name = \"MCG-NJU/videomae-base\"\n",
        "    print(\"Initializing VideoMAE model and processor...\")\n",
        "\n",
        "    model_videomae = VideoMAEForPreTraining.from_pretrained(\n",
        "        model_name,\n",
        "        attn_implementation=\"sdpa\",\n",
        "        torch_dtype=torch.float32\n",
        "    ).to(device)\n",
        "\n",
        "    processor_videomae = VideoMAEImageProcessor.from_pretrained(model_name)\n",
        "    model_videomae.train()\n",
        "    print(\"VideoMAE model and processor initialized.\")\n",
        "\n",
        "    # For demonstration, let's assume we have bounding box names and their corresponding Excel files\n",
        "    bbox_names = [f'bbox{i}' for i in range(1, 3)]  # fewer bboxes for a short demo\n",
        "    all_vol_data = []\n",
        "    all_syn_df = []\n",
        "\n",
        "    for bbox_index, bbox_name in enumerate(bbox_names):\n",
        "        print(f\"Loading data for {bbox_name}...\")\n",
        "        raw_vol, seg_vol = load_volumes(bbox_name, args.raw_base_dir, args.seg_base_dir)\n",
        "        if raw_vol is None or seg_vol is None:\n",
        "            print(f\"Skipping {bbox_name} due to loading errors.\")\n",
        "            continue\n",
        "\n",
        "        # Suppose we have an Excel file: bbox1.xlsx, bbox2.xlsx, etc.\n",
        "        excel_file = f\"{bbox_name}.xlsx\"\n",
        "        if not os.path.exists(excel_file):\n",
        "            print(f\"Excel file {excel_file} not found. Skipping {bbox_name}.\")\n",
        "            continue\n",
        "\n",
        "        syn_df = pd.read_excel(excel_file)\n",
        "\n",
        "        # We assume syn_df has columns: central_coord_1/2/3, side_1_coord_1/2/3, side_2_coord_1/2/3, etc.\n",
        "        # We'll just add the bbox_index:\n",
        "        syn_df['bbox_index'] = bbox_index\n",
        "\n",
        "        all_vol_data.append((raw_vol, seg_vol))\n",
        "        all_syn_df.append(syn_df)\n",
        "\n",
        "    if not all_syn_df:\n",
        "        print(\"No synapse data loaded. Exiting.\")\n",
        "        wandb.finish()\n",
        "        return\n",
        "\n",
        "    combined_syn_df = pd.concat(all_syn_df, ignore_index=True)\n",
        "    print(f\"Total synapses loaded: {len(combined_syn_df)}\")\n",
        "\n",
        "    # Split into train/val\n",
        "    train_syn_df, val_syn_df = train_test_split(combined_syn_df, test_size=0.2, random_state=42)\n",
        "    print(f\"Training synapses: {len(train_syn_df)}, Validation synapses: {len(val_syn_df)}\")\n",
        "\n",
        "    # Build Datasets\n",
        "    dataset_videomae_train = VideoMAEDataset(\n",
        "        vol_data_list=all_vol_data,\n",
        "        synapse_df=train_syn_df,\n",
        "        processor=processor_videomae,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=0.3\n",
        "    )\n",
        "    dataset_videomae_val = VideoMAEDataset(\n",
        "        vol_data_list=all_vol_data,\n",
        "        synapse_df=val_syn_df,\n",
        "        processor=processor_videomae,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=0.3\n",
        "    )\n",
        "\n",
        "    num_workers = min(4, multiprocessing.cpu_count())\n",
        "    print(f\"Using {num_workers} workers for DataLoader.\")\n",
        "\n",
        "    dataloader_videomae_train = DataLoader(\n",
        "        dataset_videomae_train,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    dataloader_videomae_val = DataLoader(\n",
        "        dataset_videomae_val,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(\"Saving 2 sample segmented GIFs before training starts...\")\n",
        "    save_sample_gifs(\n",
        "        dataloader=dataloader_videomae_train,\n",
        "        save_dir=saved_gifs_dir,\n",
        "        num_gifs=2,\n",
        "        prefix=\"Segmented\"\n",
        "    )\n",
        "\n",
        "    print(\"Visualizing sample inputs before training starts...\")\n",
        "    visualize_before_training(dataloader_videomae_train, epoch=0, prefix=\"Pre-Training\")\n",
        "    print(\"Visualization completed.\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model_videomae.parameters(),\n",
        "        lr=args.learning_rate,\n",
        "        weight_decay=args.weight_decay\n",
        "    )\n",
        "\n",
        "    total_steps = len(dataloader_videomae_train) * args.num_epochs\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1 * total_steps),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        patience=args.patience,\n",
        "        verbose=True,\n",
        "        path=os.path.join(args.checkpoint_dir, 'best_model.pth')\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    start_epoch = 1\n",
        "    if args.resume_checkpoint:\n",
        "        if os.path.exists(args.resume_checkpoint):\n",
        "            checkpoint = torch.load(args.resume_checkpoint, map_location=device)\n",
        "            model_videomae.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "            start_epoch = checkpoint['epoch'] + 1\n",
        "            print(f\"Resumed training from checkpoint {args.resume_checkpoint} at epoch {start_epoch}\")\n",
        "            wandb.run.summary[\"resumed_from_epoch\"] = start_epoch\n",
        "        else:\n",
        "            print(f\"Checkpoint {args.resume_checkpoint} not found. Starting from scratch.\")\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(start_epoch, args.num_epochs + 1):\n",
        "        model_videomae.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        train_pbar = tqdm(dataloader_videomae_train, desc=f\"Epoch {epoch}/{args.num_epochs} - Train\")\n",
        "        for batch_idx, (pixel_values, targets) in enumerate(train_pbar):\n",
        "            pixel_values = pixel_values.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            tubelet_size = model_videomae.config.tubelet_size\n",
        "            image_size = model_videomae.config.image_size\n",
        "            patch_size = model_videomae.config.patch_size\n",
        "\n",
        "            num_patches_per_frame = (image_size // patch_size) ** 2\n",
        "            num_tubelets = pixel_values.shape[1] // tubelet_size\n",
        "            sequence_length = num_tubelets * num_patches_per_frame\n",
        "\n",
        "            bool_masked_pos = generate_masked_positions(\n",
        "                pixel_values.shape[0],\n",
        "                sequence_length,\n",
        "                mask_ratio=args.mask_ratio,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model_videomae(\n",
        "                    pixel_values=pixel_values,\n",
        "                    bool_masked_pos=bool_masked_pos\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model_videomae.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            train_pbar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(dataloader_videomae_train)\n",
        "        print(f\"Epoch {epoch} - Training Loss: {avg_epoch_loss:.4f}\")\n",
        "        wandb.log({\n",
        "            'epoch': epoch,\n",
        "            'train_loss': avg_epoch_loss,\n",
        "            'learning_rate': scheduler.get_last_lr()[0]\n",
        "        })\n",
        "\n",
        "        # Validation\n",
        "        model_videomae.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            val_pbar = tqdm(dataloader_videomae_val, desc=f\"Epoch {epoch}/{args.num_epochs} - Val\")\n",
        "            for batch_idx, (pixel_values, targets) in enumerate(val_pbar):\n",
        "                pixel_values = pixel_values.to(device)\n",
        "\n",
        "                tubelet_size = model_videomae.config.tubelet_size\n",
        "                image_size = model_videomae.config.image_size\n",
        "                patch_size = model_videomae.config.patch_size\n",
        "\n",
        "                num_patches_per_frame = (image_size // patch_size) ** 2\n",
        "                num_tubelets = pixel_values.shape[1] // tubelet_size\n",
        "                sequence_length = num_tubelets * num_patches_per_frame\n",
        "\n",
        "                bool_masked_pos = generate_masked_positions(\n",
        "                    pixel_values.shape[0],\n",
        "                    sequence_length,\n",
        "                    mask_ratio=args.mask_ratio,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                outputs = model_videomae(\n",
        "                    pixel_values=pixel_values,\n",
        "                    bool_masked_pos=bool_masked_pos\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(dataloader_videomae_val)\n",
        "            print(f\"Epoch {epoch} - Validation Loss: {avg_val_loss:.4f}\")\n",
        "            wandb.log({'val_loss': avg_val_loss, 'epoch': epoch})\n",
        "\n",
        "        # Early stopping\n",
        "        early_stopping(avg_val_loss, model_videomae, optimizer, scheduler, epoch)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            wandb.run.summary[\"early_stopped_at_epoch\"] = epoch\n",
        "            break\n",
        "\n",
        "        # Save checkpoint every epoch (you can adjust frequency)\n",
        "        checkpoint_path = os.path.join(args.checkpoint_dir, f'epoch_{epoch}.pth')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model_videomae.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss': avg_epoch_loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "        wandb.save(checkpoint_path)\n",
        "\n",
        "    # Final save\n",
        "    final_model_path = os.path.join(args.checkpoint_dir, 'final_checkpoint.pth')\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model_videomae.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': avg_epoch_loss,\n",
        "    }, final_model_path)\n",
        "    print(f\"Training completed. Final checkpoint saved at {final_model_path}\")\n",
        "    wandb.save(final_model_path)\n",
        "\n",
        "    artifact = wandb.Artifact('final_model', type='model')\n",
        "    artifact.add_file(final_model_path)\n",
        "    wandb.log_artifact(artifact)\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "i-aYgUHSwg2G",
        "outputId": "ae24f2f0-dbf3-47cb-d179-7a4c31546f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Initializing VideoMAE model and processor...\n",
            "VideoMAE model and processor initialized.\n",
            "Loading data for bbox1...\n",
            "Loading data for bbox2...\n",
            "Total synapses loaded: 158\n",
            "Training synapses: 126, Validation synapses: 32\n",
            "Using 2 workers for DataLoader.\n",
            "Saving 2 sample segmented GIFs before training starts...\n",
            "Saving 2 sample GIFs to logs/saved_gifs...\n",
            "Saved GIF: logs/saved_gifs/Segmented_Sample_1.gif\n",
            "Saved GIF: logs/saved_gifs/Segmented_Sample_2.gif\n",
            "Successfully saved 2 GIF(s) to logs/saved_gifs.\n",
            "Visualizing sample inputs before training starts...\n",
            "Starting Pre-Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-abe8d5cbb8cc>:606: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged Pre-Training for batch 1\n",
            "Pre-Training completed.\n",
            "Visualization completed.\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/5 - Train:   0%|          | 0/63 [00:00<?, ?it/s]<ipython-input-13-abe8d5cbb8cc>:646: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "Epoch 1/5 - Train:   5%|▍         | 3/63 [01:15<25:10, 25.18s/it, loss=0.615, lr=9.68e-6]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-abe8d5cbb8cc>\u001b[0m in \u001b[0;36m<cell line: 743>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-abe8d5cbb8cc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_videomae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More masks on the segmented areas"
      ],
      "metadata": {
        "id": "BVPNvOcbbe2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/checkpoints/final_model.pth /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "VLN5ZjJOoF30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import imageio.v2 as iio\n",
        "from transformers import (\n",
        "    VideoMAEForPreTraining,\n",
        "    VideoMAEImageProcessor,\n",
        "    VideoMAEModel,\n",
        ")\n",
        "from sklearn.decomposition import PCA\n",
        "import umap.umap_ as umap\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "import multiprocessing\n",
        "from collections import deque\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.data.dataloader\")\n",
        "\n",
        "# Directories and configurations\n",
        "raw_base_dir = '/content/raw'\n",
        "seg_base_dir = '/content/seg'\n",
        "bbox_names = [f'bbox{i}' for i in range(1, 8)]\n",
        "\n",
        "os.makedirs('csv_outputs', exist_ok=True)\n",
        "os.makedirs('checkpoints', exist_ok=True)  # Directory for saving checkpoints\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def load_bbox_data(bbox_name, max_slices=None):\n",
        "    \"\"\"\n",
        "    Load raw and segmentation volumes for a bounding box.\n",
        "    Returns (raw_vol, seg_vol) each shape (Z, Y, X).\n",
        "    \"\"\"\n",
        "    raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "    seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "\n",
        "    raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "    seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "\n",
        "    if max_slices is not None:\n",
        "        raw_tif_files = raw_tif_files[:max_slices]\n",
        "        seg_tif_files = seg_tif_files[:max_slices]\n",
        "\n",
        "    assert len(raw_tif_files) == len(seg_tif_files), f\"Raw/Seg mismatch in {bbox_name}\"\n",
        "\n",
        "    raw_slices = [iio.imread(f) for f in raw_tif_files]\n",
        "    seg_slices = [iio.imread(f).astype(np.uint32) for f in seg_tif_files]\n",
        "\n",
        "    raw_vol = np.stack(raw_slices, axis=0)  # shape: (Z, Y, X)\n",
        "    seg_vol = np.stack(seg_slices, axis=0)  # shape: (Z, Y, X)\n",
        "    return raw_vol, seg_vol\n",
        "\n",
        "def create_segment_masks(seg_vol, side1_coord, side2_coord):\n",
        "    \"\"\"\n",
        "    Creates boolean masks for side_1 and side_2 coords in the segmentation volume.\n",
        "    \"\"\"\n",
        "    x1, y1, z1 = [int(c) for c in side1_coord]\n",
        "    x2, y2, z2 = [int(c) for c in side2_coord]\n",
        "\n",
        "    seg_id_1 = seg_vol[z1, y1, x1]\n",
        "    seg_id_2 = seg_vol[z2, y2, x2]\n",
        "\n",
        "    mask_1 = (seg_vol == seg_id_1) if seg_id_1 != 0 else np.zeros_like(seg_vol, dtype=bool)\n",
        "    mask_2 = (seg_vol == seg_id_2) if seg_id_2 != 0 else np.zeros_like(seg_vol, dtype=bool)\n",
        "    return mask_1, mask_2\n",
        "\n",
        "class FeatureExtractionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for feature extraction using the trained VideoMAE encoder.\n",
        "    Each item consists of a video clip extracted from the sub-volume around a central coordinate.\n",
        "    \"\"\"\n",
        "    def __init__(self, vol_data_list, synapse_df, processor, subvol_size=80, num_frames=16):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vol_data_list (List[Tuple[np.ndarray, np.ndarray]]): List containing tuples of (raw_vol, seg_vol).\n",
        "            synapse_df (pd.DataFrame): DataFrame containing synapse information.\n",
        "            processor (VideoMAEImageProcessor): Processor for VideoMAE.\n",
        "            subvol_size (int, optional): Size of the sub-volume to extract around the central coordinate. Defaults to 80.\n",
        "            num_frames (int, optional): Number of frames per video clip for VideoMAE. Defaults to 16.\n",
        "        \"\"\"\n",
        "        self.vol_data_list = vol_data_list\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.subvol_size = subvol_size\n",
        "        self.half_size = subvol_size // 2\n",
        "        self.num_frames = num_frames\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        syn_info = self.synapse_df.iloc[idx]\n",
        "        bbox_index = syn_info['bbox_index']\n",
        "        raw_vol, seg_vol = self.vol_data_list[bbox_index]\n",
        "\n",
        "        # Coordinates\n",
        "        central_coord = (\n",
        "            int(syn_info['central_coord_1']),\n",
        "            int(syn_info['central_coord_2']),\n",
        "            int(syn_info['central_coord_3'])\n",
        "        )\n",
        "        side1_coord = (\n",
        "            int(syn_info['side_1_coord_1']),\n",
        "            int(syn_info['side_1_coord_2']),\n",
        "            int(syn_info['side_1_coord_3'])\n",
        "        )\n",
        "        side2_coord = (\n",
        "            int(syn_info['side_2_coord_1']),\n",
        "            int(syn_info['side_2_coord_2']),\n",
        "            int(syn_info['side_2_coord_3'])\n",
        "        )\n",
        "\n",
        "        # Create side1 and side2 masks\n",
        "        mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "\n",
        "        # Determine sub-volume bounds\n",
        "        cx, cy, cz = central_coord\n",
        "        x_start = max(cx - self.half_size, 0)\n",
        "        x_end   = min(cx + self.half_size, raw_vol.shape[2])\n",
        "        y_start = max(cy - self.half_size, 0)\n",
        "        y_end   = min(cy + self.half_size, raw_vol.shape[1])\n",
        "        z_start = max(cz - self.half_size, 0)\n",
        "        z_end   = min(cz + self.half_size, raw_vol.shape[0])\n",
        "\n",
        "        sub_raw    = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "        sub_mask_1 = mask_1_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "        sub_mask_2 = mask_2_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "        # Pad sub-volumes to (subvol_size, subvol_size, subvol_size) if near edges\n",
        "        desired_shape = (self.subvol_size, self.subvol_size, self.subvol_size)\n",
        "        dz, dy, dx = sub_raw.shape\n",
        "\n",
        "        padded_sub_raw    = np.zeros(desired_shape, dtype=sub_raw.dtype)\n",
        "        padded_sub_mask1  = np.zeros(desired_shape, dtype=np.uint8)\n",
        "        padded_sub_mask2  = np.zeros(desired_shape, dtype=np.uint8)\n",
        "\n",
        "        padded_sub_raw[:dz, :dy, :dx] = sub_raw\n",
        "        padded_sub_mask1[:dz, :dy, :dx] = sub_mask_1\n",
        "        padded_sub_mask2[:dz, :dy, :dx] = sub_mask_2\n",
        "\n",
        "        # Create RGB-like frames: R = side1 mask, G = raw intensity, B = side2 mask\n",
        "        frames = []\n",
        "        for z in range(self.subvol_size):\n",
        "            frame_raw = padded_sub_raw[z]\n",
        "            frame_mask1 = padded_sub_mask1[z]\n",
        "            frame_mask2 = padded_sub_mask2[z]\n",
        "\n",
        "            # Normalize raw intensity to [0, 1]\n",
        "            if frame_raw.max() > frame_raw.min():\n",
        "                frame_raw_norm = (frame_raw - frame_raw.min()) / (frame_raw.max() - frame_raw.min())\n",
        "            else:\n",
        "                frame_raw_norm = np.zeros_like(frame_raw)\n",
        "\n",
        "            # Stack into 3 channels\n",
        "            frame_rgb = np.stack([frame_mask1, frame_raw_norm, frame_mask2], axis=-1)  # Shape: (Y, X, 3)\n",
        "            frames.append(frame_rgb)\n",
        "\n",
        "        if len(frames) < self.num_frames:\n",
        "            while len(frames) < self.num_frames:\n",
        "                frames.append(frames[-1])\n",
        "        elif len(frames) > self.num_frames:\n",
        "            indices = np.linspace(0, len(frames)-1, self.num_frames, dtype=int)\n",
        "            frames = [frames[i] for i in indices]\n",
        "\n",
        "        frames = [ (frame * 255).astype(np.uint8) for frame in frames ]\n",
        "\n",
        "        inputs = self.processor(\n",
        "            frames,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        pixel_values = inputs[\"pixel_values\"].squeeze(0)  # Shape: (num_frames, num_channels, height, width)\n",
        "\n",
        "        # Convert to float32 to match the model's dtype\n",
        "        pixel_values = pixel_values.float()\n",
        "\n",
        "        syn_info_dict = syn_info.to_dict()\n",
        "\n",
        "        return pixel_values, syn_info_dict\n",
        "\n",
        "# Initialize TensorBoard writer for feature extraction\n",
        "feature_log_dir = os.path.join('logs', 'feature_extraction', time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "feature_writer = SummaryWriter(log_dir=feature_log_dir)\n",
        "print(f\"TensorBoard logging initialized at {feature_log_dir}\")\n",
        "\n",
        "# Load the pre-trained VideoMAE model for feature extraction\n",
        "model_name = \"MCG-NJU/videomae-base\"\n",
        "model_save_path = 'checkpoints/best_model.pth'  # Path to your saved checkpoint\n",
        "\n",
        "print(\"Initializing VideoMAEModel for feature extraction...\")\n",
        "model_videomae_feature = VideoMAEModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Load pre-trained weights\n",
        "pretrained_dict = torch.load(model_save_path, map_location=device)\n",
        "\n",
        "filtered_dict = {}\n",
        "for key, value in pretrained_dict['model_state_dict'].items():\n",
        "    if key.startswith('encoder.'):\n",
        "        new_key = key.replace('encoder.', '')\n",
        "        filtered_dict[new_key] = value\n",
        "\n",
        "model_videomae_feature.load_state_dict(filtered_dict, strict=False)\n",
        "\n",
        "model_videomae_feature.eval()\n",
        "\n",
        "print(\"Pre-trained weights loaded into VideoMAEModel for feature extraction\")\n",
        "\n",
        "# Initialize the processor\n",
        "processor_videomae = VideoMAEImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Load data\n",
        "all_vol_data = []\n",
        "all_syn_df = []\n",
        "\n",
        "for bbox_index, bbox_name in enumerate(bbox_names):\n",
        "    print(f\"Loading data for {bbox_name}...\")\n",
        "    raw_vol, seg_vol = load_bbox_data(bbox_name)\n",
        "    excel_file = f'/content/{bbox_name}.xlsx'\n",
        "    syn_df = pd.read_excel(excel_file)\n",
        "\n",
        "    syn_df['bbox_index'] = bbox_index\n",
        "    syn_df['bbox_name']  = bbox_name\n",
        "\n",
        "    # Append to the lists\n",
        "    all_vol_data.append( (raw_vol, seg_vol) )\n",
        "    all_syn_df.append(syn_df)\n",
        "\n",
        "combined_syn_df = pd.concat(all_syn_df, ignore_index=True)\n",
        "print(f\"Total synapses loaded: {len(combined_syn_df)}\")\n",
        "\n",
        "subvol_size = 80\n",
        "num_frames = 16   # Number of frames VideoMAE expects\n",
        "\n",
        "# Initialize Feature Extraction Dataset\n",
        "feature_dataset = FeatureExtractionDataset(\n",
        "    vol_data_list=all_vol_data,\n",
        "    synapse_df=combined_syn_df,\n",
        "    processor=processor_videomae,\n",
        "    subvol_size=subvol_size,\n",
        "    num_frames=num_frames\n",
        ")\n",
        "\n",
        "# Determine optimal number of workers\n",
        "num_workers = min(8, multiprocessing.cpu_count())  # Adjust based on your system\n",
        "print(f\"Using {num_workers} workers for FeatureExtraction DataLoader.\")\n",
        "\n",
        "feature_dataloader = DataLoader(\n",
        "    feature_dataset,\n",
        "    batch_size=2,        # Adjust based on your GPU memory\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True  # Keep workers alive between epochs\n",
        ")\n",
        "\n",
        "print(\"Feature Extraction DataLoader created.\")\n",
        "\n",
        "# Function to extract features with VideoMAE\n",
        "def extract_features_with_videomae(video_batch):\n",
        "    \"\"\"\n",
        "    Extract features using the trained VideoMAE model.\n",
        "\n",
        "    Args:\n",
        "        video_batch (torch.Tensor): Tensor of shape [B, num_frames, 3, H, W].\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Array of extracted features with shape [B, hidden_size].\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        outputs = model_videomae_feature(\n",
        "            pixel_values=video_batch.to(device),\n",
        "            return_dict=True\n",
        "        )\n",
        "        last_hidden_states = outputs.last_hidden_state  # Shape: [B, sequence_length, hidden_size]\n",
        "        # Aggregate features (e.g., mean pooling)\n",
        "        pooled_features = last_hidden_states.mean(dim=1)  # Shape: [B, hidden_size]\n",
        "\n",
        "    return pooled_features.cpu().numpy()\n",
        "\n",
        "# Initialize variables for feature extraction\n",
        "all_csv_paths = []\n",
        "start_time_total = time.time()\n",
        "\n",
        "print(\"Starting feature extraction with VideoMAE encoder...\")\n",
        "for bbox_idx, bbox_name in enumerate(bbox_names):\n",
        "    print(f\"Processing {bbox_name}...\")\n",
        "    raw_vol, seg_vol = load_bbox_data(bbox_name)\n",
        "    excel_file = f'/content/{bbox_name}.xlsx'\n",
        "    syn_df = pd.read_excel(excel_file)\n",
        "\n",
        "    syn_df['bbox_index'] = bbox_idx\n",
        "    syn_df['bbox_name']  = bbox_name\n",
        "\n",
        "    dataset_bbox = FeatureExtractionDataset(\n",
        "        vol_data_list=all_vol_data,\n",
        "        synapse_df=syn_df,\n",
        "        processor=processor_videomae,\n",
        "        subvol_size=subvol_size,\n",
        "        num_frames=num_frames\n",
        "    )\n",
        "    dataloader_bbox = DataLoader(\n",
        "        dataset_bbox,\n",
        "        batch_size=2,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    bbox_features = []\n",
        "    bbox_syn_info = []\n",
        "\n",
        "    for batch_idx, (video_batch, syn_infos) in enumerate(tqdm(dataloader_bbox, desc=f\"Extracting features for {bbox_name}\")):\n",
        "        feats = extract_features_with_videomae(video_batch)\n",
        "        bbox_features.append(feats)\n",
        "\n",
        "        syn_infos_df = pd.DataFrame(syn_infos)\n",
        "        bbox_syn_info.append(syn_infos_df)\n",
        "\n",
        "        # Optional: Log progress to TensorBoard\n",
        "        feature_writer.add_scalar('Features/Processed Batches', batch_idx+1, bbox_idx * len(dataloader_bbox) + batch_idx + 1)\n",
        "\n",
        "    bbox_features = np.concatenate(bbox_features, axis=0)\n",
        "    bbox_syn_info = pd.concat(bbox_syn_info, axis=0).reset_index(drop=True)\n",
        "\n",
        "    feature_cols = [f'feat_{j}' for j in range(bbox_features.shape[1])]\n",
        "    features_df = pd.DataFrame(bbox_features, columns=feature_cols)\n",
        "\n",
        "    output_df = pd.concat([bbox_syn_info, features_df], axis=1)\n",
        "\n",
        "    output_csv_name = f'csv_outputs/{bbox_name}_videomae_features.csv'\n",
        "    output_df.to_csv(output_csv_name, index=False)\n",
        "    all_csv_paths.append(output_csv_name)\n",
        "    print(f\"Saved VideoMAE features for {bbox_name} -> {output_csv_name}\")\n",
        "\n",
        "    # Checkpoint: Save after each bbox\n",
        "    checkpoint_path = f'checkpoints/{bbox_name}_features.pth'\n",
        "    torch.save({\n",
        "        'bbox_name': bbox_name,\n",
        "        'features': bbox_features,\n",
        "        'syn_info': syn_infos_df\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "\n",
        "    # Log to TensorBoard\n",
        "    feature_writer.add_scalar('Features/BBoxes Processed', bbox_idx + 1, bbox_idx + 1)\n",
        "\n",
        "print(\"Feature extraction completed.\")\n",
        "\n",
        "print(\"Merging all CSV files...\")\n",
        "merged_df = pd.concat([pd.read_csv(p) for p in all_csv_paths], ignore_index=True)\n",
        "print(f\"Merged {len(all_csv_paths)} CSVs into one DataFrame with {len(merged_df)} rows.\")\n",
        "\n",
        "merged_csv = 'csv_outputs/all_features_merged_videomae.csv'\n",
        "merged_df.to_csv(merged_csv, index=False)\n",
        "print(f\"Final merged CSV: {merged_csv}\")\n",
        "\n",
        "# Close the TensorBoard writer for feature extraction\n",
        "feature_writer.close()\n",
        "\n",
        "print(\"Starting PCA and UMAP dimensionality reduction...\")\n",
        "df = pd.read_csv(merged_csv)\n",
        "\n",
        "feat_cols = [c for c in df.columns if c.startswith('feat_')]\n",
        "X = df[feat_cols].values\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "\n",
        "# Apply PCA\n",
        "print(\"Applying PCA...\")\n",
        "pca = PCA(n_components=50, random_state=42)\n",
        "X_pca = pca.fit_transform(X)\n",
        "print(f\"PCA transformed shape: {X_pca.shape}\")\n",
        "\n",
        "# Apply UMAP for 3D visualization\n",
        "print(\"Applying UMAP for 3D dimensionality reduction...\")\n",
        "umap_3d = umap.UMAP(\n",
        "    n_components=3,\n",
        "    n_neighbors=15,\n",
        "    min_dist=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "X_umap3 = umap_3d.fit_transform(X_pca)\n",
        "df['umap_x'] = X_umap3[:,0]\n",
        "df['umap_y'] = X_umap3[:,1]\n",
        "df['umap_z'] = X_umap3[:,2]\n",
        "\n",
        "print(\"Creating 3D UMAP visualization...\")\n",
        "fig = px.scatter_3d(\n",
        "    df,\n",
        "    x='umap_x',\n",
        "    y='umap_y',\n",
        "    z='umap_z',\n",
        "    color='bbox_name',\n",
        "    hover_data=['central_coord_1', 'central_coord_2', 'central_coord_3']\n",
        ")\n",
        "fig.update_traces(marker=dict(size=3))\n",
        "fig.update_layout(width=800, height=600)\n",
        "fig.write_html(\"videomae_umap3d.html\")\n",
        "fig.show()\n",
        "\n",
        "print(\"Creating 2D UMAP projections...\")\n",
        "fig_xy = px.scatter(\n",
        "    df,\n",
        "    x=\"umap_x\",\n",
        "    y=\"umap_y\",\n",
        "    color=\"bbox_name\",\n",
        "    title=\"UMAP (x vs y)\",\n",
        "    hover_data=[\"umap_x\", \"umap_y\", \"bbox_name\"]\n",
        ")\n",
        "fig_xy.write_html(\"videomae_umap_x_vs_y.html\")\n",
        "fig_xy.show()\n",
        "\n",
        "fig_xz = px.scatter(\n",
        "    df,\n",
        "    x=\"umap_x\",\n",
        "    y=\"umap_z\",\n",
        "    color=\"bbox_name\",\n",
        "    title=\"UMAP (x vs z)\",\n",
        "    hover_data=[\"umap_x\", \"umap_z\", \"bbox_name\"]\n",
        ")\n",
        "fig_xz.write_html(\"videomae_umap_x_vs_z.html\")\n",
        "fig_xz.show()\n",
        "\n",
        "fig_yz = px.scatter(\n",
        "    df,\n",
        "    x=\"umap_y\",\n",
        "    y=\"umap_z\",\n",
        "    color=\"bbox_name\",\n",
        "    title=\"UMAP (y vs z)\",\n",
        "    hover_data=[\"umap_y\", \"umap_z\", \"bbox_name\"]\n",
        ")\n",
        "fig_yz.write_html(\"videomae_umap_y_vs_z.html\")\n",
        "fig_yz.show()\n",
        "\n",
        "print(\"Creating combined 2D UMAP projections...\")\n",
        "fig_combined = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    subplot_titles=[\n",
        "        \"UMAP (x vs y)\",\n",
        "        \"UMAP (x vs z)\",\n",
        "        \"UMAP (y vs z)\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "cat_codes = df[\"bbox_name\"].astype(\"category\").cat.codes\n",
        "\n",
        "trace_xy = go.Scatter(\n",
        "    x=df[\"umap_x\"],\n",
        "    y=df[\"umap_y\"],\n",
        "    mode=\"markers\",\n",
        "    name=\"(x vs y)\",\n",
        "    marker=dict(\n",
        "        color=cat_codes,\n",
        "        colorscale=\"Viridis\",\n",
        "        showscale=True,\n",
        "        size=5\n",
        "    ),\n",
        "    text=df[\"bbox_name\"],    # Hover text\n",
        "    hovertemplate=\"bbox_name:%{text}<br>umap_x=%{x}<br>umap_y=%{y}\"\n",
        ")\n",
        "fig_combined.add_trace(trace_xy, row=1, col=1)\n",
        "\n",
        "# Trace for (x vs z)\n",
        "trace_xz = go.Scatter(\n",
        "    x=df[\"umap_x\"],\n",
        "    y=df[\"umap_z\"],\n",
        "    mode=\"markers\",\n",
        "    name=\"(x vs z)\",\n",
        "    marker=dict(\n",
        "        color=cat_codes,\n",
        "        colorscale=\"Viridis\",\n",
        "        showscale=False,  # Colorbar already shown in first subplot\n",
        "        size=5\n",
        "    ),\n",
        "    text=df[\"bbox_name\"],\n",
        "    hovertemplate=\"bbox_name:%{text}<br>umap_x=%{x}<br>umap_z=%{y}\"\n",
        ")\n",
        "fig_combined.add_trace(trace_xz, row=1, col=2)\n",
        "\n",
        "trace_yz = go.Scatter(\n",
        "    x=df[\"umap_y\"],\n",
        "    y=df[\"umap_z\"],\n",
        "    mode=\"markers\",\n",
        "    name=\"(y vs z)\",\n",
        "    marker=dict(\n",
        "        color=cat_codes,\n",
        "        colorscale=\"Viridis\",\n",
        "        showscale=False,\n",
        "        size=5\n",
        "    ),\n",
        "    text=df[\"bbox_name\"],\n",
        "    hovertemplate=\"bbox_name:%{text}<br>umap_y=%{x}<br>umap_z=%{y}\"\n",
        ")\n",
        "fig_combined.add_trace(trace_yz, row=1, col=3)\n",
        "\n",
        "fig_combined.update_layout(\n",
        "    title=\"2D UMAP Projections (All Pairwise Components)\",\n",
        "    width=1800,   # Wide figure\n",
        "    height=600,\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig_combined.write_html(\"videomae_combined_umap_projections.html\")\n",
        "fig_combined.show()\n",
        "print(\"Dimensionality reduction and visualization completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gJkO-sRtosbg",
        "outputId": "bd1ce86e-2cde-41ae-db7a-b7654753ee88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "TensorBoard logging initialized at logs/feature_extraction/20250109-103645\n",
            "Initializing VideoMAEModel for feature extraction...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-7f32111e01dc>:202: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_dict = torch.load(model_save_path, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-trained weights loaded into VideoMAEModel for feature extraction\n",
            "Loading data for bbox1...\n",
            "Loading data for bbox2...\n",
            "Loading data for bbox3...\n",
            "Loading data for bbox4...\n",
            "Loading data for bbox5...\n",
            "Loading data for bbox6...\n",
            "Loading data for bbox7...\n",
            "Total synapses loaded: 509\n",
            "Using 8 workers for FeatureExtraction DataLoader.\n",
            "Feature Extraction DataLoader created.\n",
            "Starting feature extraction with VideoMAE encoder...\n",
            "Processing bbox1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features for bbox1: 100%|██████████| 29/29 [00:05<00:00,  5.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved VideoMAE features for bbox1 -> csv_outputs/bbox1_videomae_features.csv\n",
            "Checkpoint saved at checkpoints/bbox1_features.pth\n",
            "Processing bbox2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features for bbox2: 100%|██████████| 50/50 [00:07<00:00,  6.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved VideoMAE features for bbox2 -> csv_outputs/bbox2_videomae_features.csv\n",
            "Checkpoint saved at checkpoints/bbox2_features.pth\n",
            "Processing bbox3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features for bbox3: 100%|██████████| 31/31 [00:05<00:00,  5.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved VideoMAE features for bbox3 -> csv_outputs/bbox3_videomae_features.csv\n",
            "Checkpoint saved at checkpoints/bbox3_features.pth\n",
            "Processing bbox4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features for bbox4: 100%|██████████| 20/20 [00:03<00:00,  5.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved VideoMAE features for bbox4 -> csv_outputs/bbox4_videomae_features.csv\n",
            "Checkpoint saved at checkpoints/bbox4_features.pth\n",
            "Processing bbox5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features for bbox5: 100%|██████████| 43/43 [00:06<00:00,  6.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved VideoMAE features for bbox5 -> csv_outputs/bbox5_videomae_features.csv\n",
            "Checkpoint saved at checkpoints/bbox5_features.pth\n",
            "Processing bbox6...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features for bbox6: 100%|██████████| 49/49 [00:07<00:00,  6.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved VideoMAE features for bbox6 -> csv_outputs/bbox6_videomae_features.csv\n",
            "Checkpoint saved at checkpoints/bbox6_features.pth\n",
            "Processing bbox7...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features for bbox7: 100%|██████████| 33/33 [00:05<00:00,  5.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved VideoMAE features for bbox7 -> csv_outputs/bbox7_videomae_features.csv\n",
            "Checkpoint saved at checkpoints/bbox7_features.pth\n",
            "Feature extraction completed.\n",
            "Merging all CSV files...\n",
            "Merged 7 CSVs into one DataFrame with 509 rows.\n",
            "Final merged CSV: csv_outputs/all_features_merged_videomae.csv\n",
            "Starting PCA and UMAP dimensionality reduction...\n",
            "Feature matrix shape: (509, 768)\n",
            "Applying PCA...\n",
            "PCA transformed shape: (509, 50)\n",
            "Applying UMAP for 3D dimensionality reduction...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating 3D UMAP visualization...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"5803f9eb-fc06-4224-8c3f-3db7a5d05cdf\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5803f9eb-fc06-4224-8c3f-3db7a5d05cdf\")) {                    Plotly.newPlot(                        \"5803f9eb-fc06-4224-8c3f-3db7a5d05cdf\",                        [{\"customdata\":[[171,260,350],[223,113,425],[280,102,377],[455,131,162],[138,121,302],[122,113,325],[175,257,297],[300,383,404],[274,158,188],[467,348,242],[467,390,244],[441,362,255],[340,188,269],[344,376,359],[357,329,309],[467,425,318],[138,307,185],[235,303,300],[248,286,293],[113,253,403],[165,367,392],[156,287,360],[130,247,389],[252,360,356],[453,386,324],[396,105,261],[394,284,259],[397,318,361],[247,225,192],[412,338,290],[440,288,256],[169,336,146],[252,377,268],[468,424,292],[460,396,339],[459,414,350],[198,300,322],[155,353,165],[150,366,158],[156,330,163],[132,293,179],[181,373,180],[161,353,195],[195,409,195],[211,437,195],[189,241,235],[418,219,199],[352,337,341],[203,347,359],[264,444,469],[177,266,345],[269,466,454],[169,322,354],[367,354,369],[339,370,395],[278,385,395],[252,388,395],[253,343,337]],\"hovertemplate\":\"bbox_name=bbox1\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eumap_z=%{z}\\u003cbr\\u003ecentral_coord_1=%{customdata[0]}\\u003cbr\\u003ecentral_coord_2=%{customdata[1]}\\u003cbr\\u003ecentral_coord_3=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox1\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"bbox1\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[2.41263,2.8405435,3.2112813,3.8682745,3.1008127,3.0664816,0.4044861,0.79632866,3.0943813,3.5137486,3.5055268,3.4992557,4.5665107,2.4502635,4.014919,2.6115563,2.8444645,0.60130125,0.7286513,2.6430306,2.3854818,2.299192,2.6573563,2.2870653,2.6042004,1.7968578,1.6444384,2.5996172,3.592825,2.9970717,1.6173,2.9493577,4.2568645,2.6474597,2.584508,2.2620986,2.2212753,2.94586,2.8997843,2.9144015,2.29175,2.6893935,2.6837502,2.8879633,1.3657045,3.8539345,2.526171,4.213559,2.1441433,2.4517722,2.318443,3.324269,2.16239,2.3999054,2.4838493,2.37132,2.3817031,2.3021975],\"y\":[2.8237913,1.3639009,1.4746708,1.782302,2.780227,2.7588482,2.5451133,0.9149695,2.336175,1.474145,1.5371263,1.4629123,1.3355843,1.8233283,1.7744348,3.0366395,3.2409792,2.412801,2.4971313,2.9273474,2.524806,2.7466176,2.9953718,1.6864046,2.8750482,1.7544695,-0.5852801,2.2179105,2.1229663,1.8729151,-0.50735426,3.2971678,0.6319819,3.039042,2.8103507,1.6000838,2.6498215,3.3816595,3.3792968,3.3377938,2.8909895,2.8212097,2.8312333,3.3156557,1.4707342,1.2890917,2.7378936,2.206582,2.469945,1.570091,2.7683372,1.9187359,2.57266,1.7678288,1.6573162,1.6718076,1.6728762,1.6256465],\"z\":[5.631356,6.3855743,6.19007,6.6685767,6.9465404,6.958422,7.4157667,7.0224333,6.981039,5.4054184,5.9446616,5.6118107,6.246667,3.949304,6.507305,4.1582117,5.556969,6.873834,6.7992573,5.569264,5.408563,5.7114935,5.588109,3.7850096,4.013313,4.733913,5.3439198,4.10923,6.8709044,5.367771,5.335884,5.5705476,4.1391525,4.360586,3.9745467,4.4028463,5.7170324,5.482933,5.5886116,5.574605,4.8398647,6.148655,5.856585,5.088962,5.3788643,6.3805017,4.448108,5.5438743,5.6350513,4.2606025,5.67949,3.9987352,5.750781,3.7730644,3.6848764,3.6747694,3.6528862,3.8513348],\"type\":\"scatter3d\"},{\"customdata\":[[325,193,399],[376,381,205],[355,400,190],[392,381,190],[345,121,172],[345,140,164],[453,193,491],[201,193,424],[185,106,386],[167,187,299],[474,109,332],[215,331,375],[285,354,304],[196,359,243],[168,471,253],[171,445,292],[134,439,122],[101,221,196],[470,227,383],[256,266,339],[261,225,283],[208,460,163],[169,443,213],[473,303,277],[435,307,228],[248,294,225],[178,260,228],[123,124,201],[143,130,259],[188,160,327],[231,223,344],[211,210,353],[202,263,212],[198,295,203],[187,116,334],[230,201,329],[142,288,346],[205,205,292],[253,239,263],[281,255,313],[315,191,332],[286,109,142],[457,242,189],[418,206,196],[359,174,333],[343,141,327],[242,211,341],[421,224,215],[240,276,303],[127,241,191],[457,302,370],[459,243,386],[235,277,409],[111,275,149],[167,230,326],[140,192,105],[278,122,163],[107,212,160],[148,430,169],[183,433,172],[185,455,138],[188,447,190],[156,425,190],[205,112,306],[230,112,334],[230,106,308],[466,304,304],[438,248,121],[457,225,212],[426,308,253],[218,239,327],[380,188,315],[353,188,316],[297,257,340],[457,243,329],[169,277,297],[152,281,306],[152,258,317],[310,260,340],[408,235,343],[433,221,345],[442,202,345],[349,216,235],[437,290,213],[256,335,276],[160,379,384],[172,201,318],[316,170,334],[254,249,341],[246,253,352],[197,167,338],[228,248,279],[137,115,273],[468,290,295],[472,300,277],[135,127,250],[190,241,190],[360,238,253],[221,233,306],[234,207,245]],\"hovertemplate\":\"bbox_name=bbox2\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eumap_z=%{z}\\u003cbr\\u003ecentral_coord_1=%{customdata[0]}\\u003cbr\\u003ecentral_coord_2=%{customdata[1]}\\u003cbr\\u003ecentral_coord_3=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox2\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"bbox2\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[2.008221,1.336219,1.5839216,1.2926859,5.6679783,5.7204094,1.7806102,0.4975393,2.2742248,4.7825227,6.2977533,2.9096558,1.5448548,1.4589165,1.3250877,1.0433645,2.0632598,6.149167,-0.23948579,5.6607537,-0.87918586,1.0533942,0.9267259,-0.34627217,0.42111593,1.2174463,-0.41945922,-0.4029661,-0.15415777,-0.91647434,-0.7721833,6.0333905,4.774583,3.9363217,5.8064303,5.966628,5.9779277,5.6594825,5.5016956,6.2872405,6.0906134,6.703046,6.9306126,6.3427415,5.867302,6.1693807,6.309946,6.6205363,5.074852,5.990981,5.2685018,5.3566594,4.961988,2.414478,-0.9338273,5.7061257,-0.7640931,-0.6356493,0.87244606,1.0172342,1.1461884,0.9282669,0.9177271,-0.7845986,-0.7531676,-0.68588865,-0.46461955,1.0236741,-0.5253631,0.24161917,-0.8701067,-0.08247039,-0.5515687,-0.7188221,0.0005509338,-0.9210684,-0.94093895,-0.985244,-0.80075645,-0.8978583,-0.1476687,-0.29172242,-0.7355936,0.10865344,0.121108346,0.29770857,-0.8394292,-0.69923097,-0.6403705,-0.572723,-0.86840445,-0.90806043,0.0017317804,-0.19602802,-0.23419082,-0.22158,-0.17519522,-0.59326625,-0.93495476,-0.9305485],\"y\":[1.8505154,1.4626622,1.6144924,0.9546825,0.7600041,0.73134005,1.3810247,2.2993941,2.2108428,0.2746233,-0.37739682,3.223761,1.9617682,1.6937413,1.756045,1.6877652,1.6376649,-0.19830045,2.3780408,-0.20773582,3.0262082,2.4759626,2.6239746,2.7593174,2.560442,-0.094971694,2.496995,2.4141173,2.208344,2.819719,2.6770082,-0.3192255,0.4496833,0.8965682,-0.20094536,-0.44899496,-0.20639372,-0.12175842,-0.072279364,-0.113431275,-0.31900525,-0.09841616,-0.048884016,-0.15589328,-0.17622449,-0.08042106,-0.51107466,-0.1626262,0.08827878,-0.22535992,0.95604205,0.89804924,1.140321,1.8922007,2.6804514,0.7538889,2.775328,2.8684218,2.5673215,2.5777886,2.317145,2.64408,2.63956,2.5870767,2.5726695,2.6109257,2.6793115,0.26362512,2.8597262,2.5457113,2.6257532,2.5810955,2.7725341,2.92764,2.4546933,2.7200367,2.717126,2.56559,2.8730805,2.6555693,2.3736036,2.4186568,2.9044974,2.6553922,2.0573797,2.1072257,2.6206095,2.8920927,2.755993,2.6816428,2.740539,2.8260748,2.1172311,2.5680757,2.7676728,2.2157197,2.3354328,2.8899047,2.7791095,2.9622035],\"z\":[7.3282094,5.5198836,6.630788,5.8812685,8.259637,8.320213,6.480857,7.1207795,5.96315,3.9615548,3.9869735,5.280477,5.543113,7.3648715,5.464124,6.7784376,4.850018,3.24689,8.837676,3.4298618,8.586334,7.606988,7.634493,8.8736315,7.783554,6.047938,8.875458,9.096192,8.506387,8.735352,9.248197,3.3451247,4.186961,4.199299,3.14004,3.4051766,3.446027,3.2568247,3.263872,3.3992298,3.298712,3.4794793,3.6502054,3.2742789,3.2039557,3.2980266,3.7728434,3.4475179,3.800736,3.2131474,5.228587,5.118302,5.5170207,5.9442406,8.701098,8.306786,9.339785,9.373669,7.6328745,7.5944505,7.596474,7.6573057,7.6522694,8.711549,8.681561,8.776401,9.183939,6.3607183,9.519199,7.820033,8.562115,8.990291,9.543867,8.981859,8.919427,8.975507,8.977384,8.676827,8.9708805,9.188352,8.98262,8.958036,9.463988,8.321503,7.4269304,8.096784,8.67263,9.367487,8.878752,8.942838,8.65817,8.468514,8.292801,8.907651,8.872386,8.473357,8.817516,9.582705,8.504689,8.619688],\"type\":\"scatter3d\"},{\"customdata\":[[351,290,458],[291,226,455],[153,190,413],[235,141,244],[196,212,251],[453,467,203],[206,84,206],[404,219,195],[253,467,187],[368,422,436],[398,469,398],[416,344,328],[273,205,278],[403,126,213],[473,203,193],[464,200,192],[478,208,193],[183,401,374],[160,357,445],[227,409,415],[223,388,446],[135,350,449],[234,372,442],[403,401,365],[333,310,280],[347,281,264],[264,202,224],[232,104,366],[310,194,409],[339,160,350],[193,180,345],[224,163,273],[286,145,229],[300,188,271],[291,233,243],[290,476,309],[278,478,341],[431,440,417],[194,388,426],[214,425,389],[133,233,417],[132,268,417],[136,144,438],[253,237,468],[266,248,468],[255,211,277],[331,337,300],[366,248,256],[282,143,258],[269,134,252],[253,223,249],[185,189,259],[172,207,258],[110,349,293],[325,371,279],[320,322,283],[287,322,181],[426,221,182],[461,323,170],[384,179,101],[404,190,101],[360,109,91]],\"hovertemplate\":\"bbox_name=bbox3\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eumap_z=%{z}\\u003cbr\\u003ecentral_coord_1=%{customdata[0]}\\u003cbr\\u003ecentral_coord_2=%{customdata[1]}\\u003cbr\\u003ecentral_coord_3=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox3\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"bbox3\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[1.9052733,1.1630459,2.6986802,1.9035199,0.57788587,0.984772,0.944357,0.3740682,1.6131536,2.7531214,1.0577978,0.5773393,3.603735,2.999266,1.8518579,1.8554194,1.9224253,4.212235,1.5979263,2.9574292,2.0310466,1.6478426,2.0872319,2.2136881,1.3151621,1.3472785,3.336064,3.7339203,0.6536247,2.4019988,0.57846314,1.1108916,2.6350765,3.358898,1.8492073,0.37182957,0.58377016,3.1846998,3.2495668,3.3280342,0.4770127,0.7776553,0.9032831,0.91945386,1.0239565,1.5130506,2.8060424,0.62368923,1.1677976,1.2045504,1.7112211,0.9991664,0.80062443,3.321964,0.8438875,1.1857127,4.4543467,0.44081968,-1.0464584,2.7967875,2.876698,4.3568673],\"y\":[2.8673117,1.878824,1.2108725,2.8885508,1.9379443,1.4206247,1.093815,1.7522157,0.036317006,0.57623106,1.3492024,1.6604799,1.4568688,3.2340093,1.4421904,1.2057422,0.86688465,0.9826603,2.309547,3.393512,2.9592986,2.5191245,2.925057,0.73982376,1.8679345,2.1006682,1.8798236,1.2622542,1.7942501,1.6336757,1.4473742,1.7676128,1.1311151,2.0205836,2.0119507,2.1197307,2.3667762,0.8504343,2.0785446,2.48844,2.2153924,2.021058,1.7614807,1.6400557,1.6484245,1.9963715,0.78526956,2.3758576,1.5516102,1.557142,2.1463332,1.899385,1.6533997,0.6845402,1.6838056,1.8137416,1.207868,1.665228,2.783216,2.378056,2.5087621,1.170352],\"z\":[4.7178564,6.410591,6.379648,4.6965995,7.729536,7.8627677,7.0933857,7.190431,5.469369,5.796889,7.513017,7.511224,5.2571745,5.1577024,4.4345193,4.4543796,4.7435975,6.130148,5.0829887,5.6031084,4.6853275,4.82594,4.6426663,4.5230217,7.593933,7.540039,6.968928,6.3463764,8.13886,6.9063625,7.286349,7.147222,6.486513,6.895167,7.8606,6.9880185,6.907093,6.011961,6.718334,7.031431,7.0213614,7.0894513,7.727969,7.4214416,7.3184466,8.003321,6.1541877,6.865877,7.603844,7.670861,7.757605,7.2334065,7.369742,4.6073084,7.9004116,7.9646473,6.1782913,7.3396845,8.847228,6.998262,6.911678,6.270438],\"type\":\"scatter3d\"},{\"customdata\":[[342,215,386],[268,223,306],[349,190,186],[401,184,119],[327,248,200],[330,352,397],[358,189,381],[164,298,320],[223,226,403],[233,337,329],[234,306,390],[226,267,373],[239,246,373],[264,237,327],[294,218,314],[358,324,328],[368,337,328],[460,425,324],[212,388,316],[247,350,316],[166,298,336],[184,318,141],[142,175,101],[162,148,111],[147,192,152],[126,210,152],[132,194,155],[140,173,155],[82,217,131],[179,162,289],[111,208,124],[315,120,193],[301,450,294],[318,474,299],[303,355,324],[327,361,320],[350,342,354],[346,416,290],[241,282,234],[193,148,266]],\"hovertemplate\":\"bbox_name=bbox4\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eumap_z=%{z}\\u003cbr\\u003ecentral_coord_1=%{customdata[0]}\\u003cbr\\u003ecentral_coord_2=%{customdata[1]}\\u003cbr\\u003ecentral_coord_3=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox4\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"bbox4\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[1.8772495,4.1934566,1.558995,1.54429,3.813722,0.027272787,4.2300205,1.5296135,1.7250987,2.5098572,1.5857309,1.541672,1.4530039,2.2607589,4.2832117,3.435408,3.4404345,1.5393516,2.7874253,2.474262,1.7719675,4.263826,0.4480478,2.426688,2.6666708,2.5687523,2.7221773,3.347529,2.205035,1.1076539,1.8810905,1.6274514,2.9146316,2.6027238,2.8027103,3.0094962,2.8717475,3.4751725,2.9888306,1.3478801],\"y\":[2.06553,2.3769598,-0.45451063,-0.23604158,1.8965014,2.223792,2.3292158,1.9429204,1.5272801,2.59867,1.5142825,1.676482,1.7339458,1.8354064,2.2507718,2.4774559,2.470766,-0.16611609,1.3194376,2.687437,1.9834803,2.3112764,2.346658,1.8064451,1.9593651,1.7804286,1.644105,0.8381123,1.9657755,2.2183635,0.8639911,-0.6780791,1.4616764,1.1923044,2.5441966,2.5987163,2.2711046,1.4708607,2.5405788,1.5690958],\"z\":[7.4251256,5.604115,5.315252,5.251929,6.711703,8.417414,5.575988,7.490372,4.9291267,5.4680834,4.8154755,4.9259543,5.3737545,4.8785515,5.5587025,3.986453,3.9837615,5.5021677,6.4940267,5.5167184,7.5362463,5.6761894,7.6010995,7.068627,3.6680763,3.627251,3.6949556,4.248265,4.660692,7.2550797,4.7812057,5.3932843,5.0242395,4.897191,4.8185906,4.570175,4.334226,5.1769147,4.297608,5.7061343],\"type\":\"scatter3d\"},{\"customdata\":[[436,469,413],[170,441,334],[459,359,387],[401,423,411],[88,420,395],[297,396,199],[129,172,104],[109,205,389],[422,286,156],[405,404,266],[399,296,161],[383,232,173],[428,300,116],[457,288,116],[386,455,333],[155,214,455],[264,185,403],[222,317,252],[389,418,266],[288,302,221],[225,289,242],[130,352,224],[130,312,230],[173,253,220],[155,210,199],[362,179,149],[121,254,140],[157,234,136],[204,217,136],[190,211,156],[138,212,160],[219,215,158],[210,219,193],[275,272,204],[138,410,203],[176,401,226],[312,372,207],[254,243,187],[406,333,354],[129,337,256],[313,131,432],[388,123,391],[356,168,398],[397,203,452],[248,295,431],[163,433,371],[113,231,415],[127,421,304],[287,103,330],[269,107,340],[447,129,185],[334,127,210],[328,128,256],[309,158,256],[149,136,221],[348,112,221],[274,126,289],[274,153,285],[135,221,232],[272,321,119],[309,339,124],[433,234,218],[446,232,192],[445,199,146],[456,209,140],[408,171,101],[385,193,111],[361,223,105],[402,154,113],[350,178,116],[338,192,120],[395,201,117],[472,348,223],[400,293,331],[435,343,281],[399,396,309],[403,475,333],[385,474,339],[424,470,327],[442,464,274],[448,376,249],[413,400,286],[434,382,262],[423,394,257],[379,458,257],[158,430,397]],\"hovertemplate\":\"bbox_name=bbox5\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eumap_z=%{z}\\u003cbr\\u003ecentral_coord_1=%{customdata[0]}\\u003cbr\\u003ecentral_coord_2=%{customdata[1]}\\u003cbr\\u003ecentral_coord_3=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox5\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"bbox5\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[1.588025,0.077645175,1.5495262,1.6000838,3.1624737,3.321806,5.4331126,0.69958836,-0.73200244,1.3363643,-0.69135094,5.624961,5.4567394,5.8920603,1.2718222,4.6529107,3.3885114,2.8411257,1.2664549,1.0489063,2.5833333,4.8090477,6.086376,5.5969334,2.0291772,2.733407,2.1832144,2.3209338,2.742142,2.451839,2.3762646,2.3699603,1.7002002,1.0249623,4.199527,3.2500715,3.2553575,1.1022372,6.346275,4.5447416,1.7719276,3.612407,2.4890115,0.67095894,2.0016105,1.046322,3.5487578,4.248,3.4745238,3.367325,5.1334844,2.1857836,1.9421216,1.9208044,1.0988053,2.048632,3.2799015,3.3142698,1.2763399,4.242226,4.093433,1.5593165,1.539337,3.4519353,3.4634063,3.5755446,2.9299335,3.810611,3.7978735,2.8781905,2.9821901,3.0500486,3.6237648,5.63871,6.39578,1.3926187,1.2644144,1.2652044,1.286869,1.0896696,1.5102108,1.499594,1.5316225,1.2424681,1.5104152,1.009357],\"y\":[1.763543,2.3599463,1.9034739,1.8836085,2.175861,1.5916611,0.8650379,2.231238,2.7242582,-0.37169293,2.7506557,0.81019735,0.8554583,-0.1500628,-0.7819022,1.403428,1.338938,1.8200352,-0.46477395,-0.45858407,1.7272884,1.2752345,0.5706823,0.8071125,1.2290692,2.2661664,2.2932515,2.5714712,2.3162827,2.2011862,2.3303058,1.959129,0.38610074,-0.4565882,1.8182478,1.8995968,1.6919334,-0.36942023,-0.11044071,1.6397489,1.8763597,2.2066865,1.9917114,2.3638558,2.941332,-0.10592871,2.3767984,2.1037362,2.1792824,1.8280481,1.0966597,2.058573,1.8421589,1.6257248,1.3213332,2.0070662,1.7456342,1.5891545,-0.5042989,1.7283028,1.8178942,-0.5125551,-0.68830836,2.146175,2.194834,2.3275955,2.602955,1.4256604,2.2759402,2.6198912,2.3661797,2.5708811,1.4346502,0.8000823,-0.0921387,-0.6752754,-0.7868431,-0.7498604,-0.7915217,-0.53851134,-0.66053635,-0.6893344,-0.6695493,-0.510769,-0.30617675,-0.39673364],\"z\":[7.8903246,8.538729,8.05647,8.0954,4.2043805,5.3132625,7.9988866,7.0731406,9.615474,5.318421,9.557448,8.221569,8.032826,3.0855277,5.649372,6.3072386,4.223668,5.5153785,5.274838,5.3846607,5.8197885,6.5451555,4.368603,8.200048,4.410573,4.3485208,5.896638,5.854591,4.607909,4.868108,5.102589,4.282432,5.10367,5.3865333,6.3012905,5.975948,5.4694414,5.362009,3.6357346,6.1293154,4.9737496,6.7853413,5.11134,6.7492566,4.5730343,5.7521353,6.7187357,5.4897075,3.8752058,3.874288,5.29065,4.4971633,4.48515,4.82994,7.200268,4.3242784,3.8236763,3.8973143,5.221296,6.324513,6.3246784,5.6349154,5.476361,3.824498,3.8268232,3.9253776,4.3147326,5.215443,3.9980702,4.252205,4.3742027,4.430251,5.1775107,8.229855,3.7644355,5.7275405,5.6404076,5.701491,5.6941724,5.4756594,5.461679,5.489318,5.441674,5.268981,5.6709332,5.3885555],\"type\":\"scatter3d\"},{\"customdata\":[[353,102,250],[116,308,172],[140,406,197],[357,330,268],[148,170,139],[151,198,139],[157,188,176],[175,216,172],[172,248,167],[131,256,179],[110,225,185],[157,192,191],[166,264,195],[211,262,199],[200,153,206],[205,143,206],[212,135,206],[207,192,219],[259,267,233],[286,266,226],[357,231,261],[209,305,197],[228,328,195],[207,281,213],[223,340,210],[275,336,222],[155,298,148],[292,353,212],[327,375,212],[347,348,212],[237,383,193],[232,388,170],[249,376,168],[182,366,168],[295,396,133],[314,464,151],[163,267,152],[157,235,152],[225,129,142],[148,134,141],[163,203,157],[141,177,157],[168,147,125],[164,264,147],[306,403,119],[269,411,119],[163,174,110],[286,294,230],[261,316,235],[313,351,235],[281,377,235],[232,351,233],[346,313,235],[344,302,244],[309,357,246],[275,336,248],[262,321,248],[254,265,248],[282,196,248],[351,165,264],[307,265,332],[302,362,332],[475,398,271],[285,324,253],[271,303,253],[330,297,253],[364,324,253],[218,314,257],[275,382,255],[282,184,253],[291,243,273],[247,342,283],[323,124,272],[317,270,272],[388,148,276],[354,155,285],[329,194,285],[342,205,285],[314,231,289],[280,236,290],[245,361,296],[392,241,322],[332,279,296],[462,168,303],[418,251,332],[456,137,329],[153,94,337],[356,220,347],[341,424,347],[468,125,358],[422,121,354],[398,476,410],[378,442,377],[428,162,416],[420,193,407],[392,460,427],[418,136,392],[202,266,389]],\"hovertemplate\":\"bbox_name=bbox6\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eumap_z=%{z}\\u003cbr\\u003ecentral_coord_1=%{customdata[0]}\\u003cbr\\u003ecentral_coord_2=%{customdata[1]}\\u003cbr\\u003ecentral_coord_3=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox6\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"bbox6\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[5.6789975,3.6360278,1.5838221,5.9859223,5.415544,5.779011,5.526578,6.802936,5.7377634,4.5908737,3.685119,5.280314,5.590004,6.6093254,5.4916763,6.0498414,6.2229896,3.994961,4.750249,5.3751473,5.7228565,6.127765,5.5168767,5.9932604,5.285295,5.724165,6.8560667,5.8528566,6.8017535,7.0122023,5.62182,6.5222015,6.9013886,2.6442113,6.986702,5.409631,6.8929367,5.7297163,6.66965,5.2093053,6.372627,5.4667974,5.596629,6.861378,6.430335,6.3307457,6.105537,5.43028,4.83471,5.667574,5.384726,4.978211,6.320123,6.1798687,5.6018944,5.263979,4.920869,4.6815877,5.800996,7.0669036,4.6596384,2.1968198,2.707873,5.314682,5.0750933,6.2522645,5.894956,4.843263,5.480191,5.868624,5.7787805,4.8001637,6.5652285,6.379112,7.05121,6.728693,6.2465005,6.550195,6.0030494,5.4085946,4.6144085,3.6281285,6.4062715,0.9417393,2.964179,1.177013,3.250967,3.509882,5.6918983,1.1994011,1.2394247,1.039507,1.9981482,0.93552035,0.897767,0.8694055,1.1303251,1.6345977],\"y\":[0.76823175,1.7733928,2.465135,-0.5668483,-0.42737156,-0.5586049,0.013952165,-0.11753085,0.719939,1.3284833,2.2474885,0.37762794,0.8275291,-0.25966492,0.8121925,0.3549786,0.23237447,1.0725019,0.3910105,0.31599763,-0.0007787129,0.17751503,0.54128516,-0.10552322,0.6491415,0.18183054,0.033324655,0.30077267,0.013822259,-0.08889604,0.47475058,-0.10737561,-0.023278726,1.4075,-0.1589724,0.86186355,0.031846926,0.7168552,-0.35996538,-0.3461831,-0.33362108,-0.41709983,-0.5436341,-0.0046823216,-0.50589657,-0.37664905,-0.5998616,0.22977115,0.3244986,-0.30329934,0.11234774,0.73879725,-0.39914164,-0.40192616,-0.30172274,0.1024794,0.3004911,0.30188137,0.5124636,-0.057510864,1.3307183,0.70159197,1.237959,-0.06998092,0.11100433,-0.48673597,-0.35405228,0.23498182,-0.17172115,0.44278276,0.49350867,0.21365432,-0.117739394,-0.10017654,-0.17490266,-0.029555049,0.15598282,-0.07292856,0.34204867,0.86244816,0.31755158,0.87327856,-0.50374705,1.7395099,1.1485902,1.6238654,2.564164,2.2895153,0.47830912,1.5803096,1.3457419,1.8946263,1.5441788,1.7491943,1.7986783,1.7929367,1.6097921,1.5350264],\"z\":[8.279474,6.8042665,7.648979,3.6406415,3.7234275,3.6249864,3.95592,3.5937486,8.334132,6.259629,6.7573833,4.1946087,8.173994,3.637176,8.062469,4.445091,4.2202644,6.1586785,4.1098704,4.263265,4.0831575,4.1467233,4.499368,3.9349415,4.473829,4.201471,3.2405486,4.3244157,3.8198125,3.6869044,4.5485535,4.1109695,3.8538814,6.7540426,3.5717564,7.931646,3.2869232,8.326118,3.8347297,3.8288631,3.383467,3.7427552,3.7973602,3.2404158,3.9046104,4.0519547,3.7514307,4.2435837,4.049206,3.946662,4.102854,4.2922125,3.8630328,3.6656017,3.9145296,4.04826,4.078801,4.018576,4.6670876,3.7080142,6.2299232,4.6291304,6.2171035,3.9500875,4.0533834,3.8609438,3.4790168,4.0048842,3.8989565,4.6345725,4.6034627,3.9710078,3.9741158,4.069631,3.7214463,3.6697466,4.0649633,3.735389,4.3490977,5.0552516,4.0708437,6.1327815,3.8113675,5.5870113,6.085675,5.7056174,7.0692945,6.8282986,4.9313188,5.741597,5.376025,5.377446,6.6949973,5.581029,5.606971,5.662995,5.387168,4.401536],\"type\":\"scatter3d\"},{\"customdata\":[[72,177,372],[145,392,310],[311,285,313],[396,378,346],[472,347,331],[428,224,399],[424,107,374],[131,177,497],[124,418,293],[359,144,161],[118,157,197],[356,341,331],[367,315,349],[129,142,371],[209,329,355],[121,323,216],[121,146,413],[159,131,435],[149,182,404],[144,216,306],[135,210,317],[232,330,363],[225,146,131],[372,316,209],[358,136,110],[453,237,123],[453,256,127],[453,275,125],[351,467,154],[321,455,150],[413,409,292],[295,404,344],[411,311,216],[317,306,147],[97,328,142],[104,335,142],[232,255,298],[383,459,159],[428,265,161],[461,220,335],[273,176,318],[377,96,386],[364,115,393],[259,250,466],[213,303,444],[402,378,441],[375,383,457],[303,428,337],[389,387,325],[160,324,227],[140,320,227],[207,231,338],[257,250,338],[262,271,306],[454,232,304],[436,245,304],[187,116,281],[395,320,237],[160,106,245],[180,114,247],[383,303,229],[389,294,229],[216,248,123],[348,183,113],[336,344,102]],\"hovertemplate\":\"bbox_name=bbox7\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eumap_z=%{z}\\u003cbr\\u003ecentral_coord_1=%{customdata[0]}\\u003cbr\\u003ecentral_coord_2=%{customdata[1]}\\u003cbr\\u003ecentral_coord_3=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox7\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"bbox7\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[3.0804224,-0.33493567,2.591155,1.1536641,2.85707,2.7266967,-0.61650026,2.4358292,-0.57618296,0.9077103,1.8221993,1.7791634,3.1995974,-0.60539967,3.2266023,0.96415895,6.0119724,6.1237464,6.3549376,1.7956539,1.7884183,2.58853,3.1076822,1.3453476,2.568352,0.40146843,0.38447338,0.35690057,0.86669457,1.0368863,0.45719555,1.5215083,0.5955562,3.374986,3.0276768,3.2264833,3.2944598,3.517221,1.28106,3.6106055,1.5110991,2.0399752,2.1160376,3.643127,3.1111162,2.8784502,2.2980149,3.533009,1.5536321,2.6620111,2.7270646,3.207562,2.6192546,2.5848532,3.6613712,3.7249324,1.9001629,3.1463704,3.390588,2.7939672,3.4435613,3.5000906,1.3095369,2.5949924,2.6505432],\"y\":[2.7655232,2.393841,1.4454954,1.8680328,1.7700044,2.40631,2.5620627,1.4865438,2.823127,1.7521359,1.6055545,2.2639105,1.5558243,2.8428748,2.4025607,2.6016634,0.08657857,-0.03363896,0.10441889,1.5030893,1.3873515,2.8683736,2.5455105,1.9564257,1.1175891,1.7585535,1.9870793,2.0647984,2.2500038,2.2690957,1.7659074,2.0623543,1.7849313,1.4014297,2.7090435,2.6968987,2.4520009,1.4392744,1.9294903,1.3573451,1.7464308,0.8900724,0.75009906,2.553729,2.6477458,2.0699527,1.9998578,1.5123222,1.8766506,2.9691079,2.9130638,2.6790307,1.4524256,2.8586793,1.344949,1.3325446,0.6955375,1.0758376,1.2062148,0.9168878,1.1434642,1.1200206,1.7606623,1.0899945,2.750742],\"z\":[7.0126762,8.981945,7.0168815,5.31649,6.9503684,7.3075304,9.411318,6.7991858,9.626831,7.1952376,4.5037766,4.737111,4.8573475,9.590493,6.8364325,7.623087,3.2705803,3.194401,3.387409,4.284925,4.3550615,4.6032157,7.124465,5.082313,6.428931,7.1289363,7.0075107,7.0376015,8.657453,8.626404,7.2073464,7.6258426,7.5412607,4.632761,3.9919891,4.0349436,7.105459,5.203869,5.3091865,4.366527,5.24328,4.535887,4.547343,4.225662,4.621583,4.3424163,4.903838,5.367071,5.4005895,4.0526233,4.028475,4.7641344,6.855131,3.9562187,4.3321686,4.3448358,4.454674,4.389905,4.4684176,4.5141015,4.291552,4.303883,7.4091663,6.403949,5.6811843],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"umap_x\"}},\"yaxis\":{\"title\":{\"text\":\"umap_y\"}},\"zaxis\":{\"title\":{\"text\":\"umap_z\"}}},\"legend\":{\"title\":{\"text\":\"bbox_name\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"width\":800,\"height\":600},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5803f9eb-fc06-4224-8c3f-3db7a5d05cdf');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating 2D UMAP projections...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"021bcba0-7ff5-490e-a7b3-92592434044c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"021bcba0-7ff5-490e-a7b3-92592434044c\")) {                    Plotly.newPlot(                        \"021bcba0-7ff5-490e-a7b3-92592434044c\",                        [{\"customdata\":[[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox1\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[2.41263,2.8405435,3.2112813,3.8682745,3.1008127,3.0664816,0.4044861,0.79632866,3.0943813,3.5137486,3.5055268,3.4992557,4.5665107,2.4502635,4.014919,2.6115563,2.8444645,0.60130125,0.7286513,2.6430306,2.3854818,2.299192,2.6573563,2.2870653,2.6042004,1.7968578,1.6444384,2.5996172,3.592825,2.9970717,1.6173,2.9493577,4.2568645,2.6474597,2.584508,2.2620986,2.2212753,2.94586,2.8997843,2.9144015,2.29175,2.6893935,2.6837502,2.8879633,1.3657045,3.8539345,2.526171,4.213559,2.1441433,2.4517722,2.318443,3.324269,2.16239,2.3999054,2.4838493,2.37132,2.3817031,2.3021975],\"xaxis\":\"x\",\"y\":[2.8237913,1.3639009,1.4746708,1.782302,2.780227,2.7588482,2.5451133,0.9149695,2.336175,1.474145,1.5371263,1.4629123,1.3355843,1.8233283,1.7744348,3.0366395,3.2409792,2.412801,2.4971313,2.9273474,2.524806,2.7466176,2.9953718,1.6864046,2.8750482,1.7544695,-0.5852801,2.2179105,2.1229663,1.8729151,-0.50735426,3.2971678,0.6319819,3.039042,2.8103507,1.6000838,2.6498215,3.3816595,3.3792968,3.3377938,2.8909895,2.8212097,2.8312333,3.3156557,1.4707342,1.2890917,2.7378936,2.206582,2.469945,1.570091,2.7683372,1.9187359,2.57266,1.7678288,1.6573162,1.6718076,1.6728762,1.6256465],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox2\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox2\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[2.008221,1.336219,1.5839216,1.2926859,5.6679783,5.7204094,1.7806102,0.4975393,2.2742248,4.7825227,6.2977533,2.9096558,1.5448548,1.4589165,1.3250877,1.0433645,2.0632598,6.149167,-0.23948579,5.6607537,-0.87918586,1.0533942,0.9267259,-0.34627217,0.42111593,1.2174463,-0.41945922,-0.4029661,-0.15415777,-0.91647434,-0.7721833,6.0333905,4.774583,3.9363217,5.8064303,5.966628,5.9779277,5.6594825,5.5016956,6.2872405,6.0906134,6.703046,6.9306126,6.3427415,5.867302,6.1693807,6.309946,6.6205363,5.074852,5.990981,5.2685018,5.3566594,4.961988,2.414478,-0.9338273,5.7061257,-0.7640931,-0.6356493,0.87244606,1.0172342,1.1461884,0.9282669,0.9177271,-0.7845986,-0.7531676,-0.68588865,-0.46461955,1.0236741,-0.5253631,0.24161917,-0.8701067,-0.08247039,-0.5515687,-0.7188221,0.0005509338,-0.9210684,-0.94093895,-0.985244,-0.80075645,-0.8978583,-0.1476687,-0.29172242,-0.7355936,0.10865344,0.121108346,0.29770857,-0.8394292,-0.69923097,-0.6403705,-0.572723,-0.86840445,-0.90806043,0.0017317804,-0.19602802,-0.23419082,-0.22158,-0.17519522,-0.59326625,-0.93495476,-0.9305485],\"xaxis\":\"x\",\"y\":[1.8505154,1.4626622,1.6144924,0.9546825,0.7600041,0.73134005,1.3810247,2.2993941,2.2108428,0.2746233,-0.37739682,3.223761,1.9617682,1.6937413,1.756045,1.6877652,1.6376649,-0.19830045,2.3780408,-0.20773582,3.0262082,2.4759626,2.6239746,2.7593174,2.560442,-0.094971694,2.496995,2.4141173,2.208344,2.819719,2.6770082,-0.3192255,0.4496833,0.8965682,-0.20094536,-0.44899496,-0.20639372,-0.12175842,-0.072279364,-0.113431275,-0.31900525,-0.09841616,-0.048884016,-0.15589328,-0.17622449,-0.08042106,-0.51107466,-0.1626262,0.08827878,-0.22535992,0.95604205,0.89804924,1.140321,1.8922007,2.6804514,0.7538889,2.775328,2.8684218,2.5673215,2.5777886,2.317145,2.64408,2.63956,2.5870767,2.5726695,2.6109257,2.6793115,0.26362512,2.8597262,2.5457113,2.6257532,2.5810955,2.7725341,2.92764,2.4546933,2.7200367,2.717126,2.56559,2.8730805,2.6555693,2.3736036,2.4186568,2.9044974,2.6553922,2.0573797,2.1072257,2.6206095,2.8920927,2.755993,2.6816428,2.740539,2.8260748,2.1172311,2.5680757,2.7676728,2.2157197,2.3354328,2.8899047,2.7791095,2.9622035],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox3\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox3\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.9052733,1.1630459,2.6986802,1.9035199,0.57788587,0.984772,0.944357,0.3740682,1.6131536,2.7531214,1.0577978,0.5773393,3.603735,2.999266,1.8518579,1.8554194,1.9224253,4.212235,1.5979263,2.9574292,2.0310466,1.6478426,2.0872319,2.2136881,1.3151621,1.3472785,3.336064,3.7339203,0.6536247,2.4019988,0.57846314,1.1108916,2.6350765,3.358898,1.8492073,0.37182957,0.58377016,3.1846998,3.2495668,3.3280342,0.4770127,0.7776553,0.9032831,0.91945386,1.0239565,1.5130506,2.8060424,0.62368923,1.1677976,1.2045504,1.7112211,0.9991664,0.80062443,3.321964,0.8438875,1.1857127,4.4543467,0.44081968,-1.0464584,2.7967875,2.876698,4.3568673],\"xaxis\":\"x\",\"y\":[2.8673117,1.878824,1.2108725,2.8885508,1.9379443,1.4206247,1.093815,1.7522157,0.036317006,0.57623106,1.3492024,1.6604799,1.4568688,3.2340093,1.4421904,1.2057422,0.86688465,0.9826603,2.309547,3.393512,2.9592986,2.5191245,2.925057,0.73982376,1.8679345,2.1006682,1.8798236,1.2622542,1.7942501,1.6336757,1.4473742,1.7676128,1.1311151,2.0205836,2.0119507,2.1197307,2.3667762,0.8504343,2.0785446,2.48844,2.2153924,2.021058,1.7614807,1.6400557,1.6484245,1.9963715,0.78526956,2.3758576,1.5516102,1.557142,2.1463332,1.899385,1.6533997,0.6845402,1.6838056,1.8137416,1.207868,1.665228,2.783216,2.378056,2.5087621,1.170352],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox4\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox4\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.8772495,4.1934566,1.558995,1.54429,3.813722,0.027272787,4.2300205,1.5296135,1.7250987,2.5098572,1.5857309,1.541672,1.4530039,2.2607589,4.2832117,3.435408,3.4404345,1.5393516,2.7874253,2.474262,1.7719675,4.263826,0.4480478,2.426688,2.6666708,2.5687523,2.7221773,3.347529,2.205035,1.1076539,1.8810905,1.6274514,2.9146316,2.6027238,2.8027103,3.0094962,2.8717475,3.4751725,2.9888306,1.3478801],\"xaxis\":\"x\",\"y\":[2.06553,2.3769598,-0.45451063,-0.23604158,1.8965014,2.223792,2.3292158,1.9429204,1.5272801,2.59867,1.5142825,1.676482,1.7339458,1.8354064,2.2507718,2.4774559,2.470766,-0.16611609,1.3194376,2.687437,1.9834803,2.3112764,2.346658,1.8064451,1.9593651,1.7804286,1.644105,0.8381123,1.9657755,2.2183635,0.8639911,-0.6780791,1.4616764,1.1923044,2.5441966,2.5987163,2.2711046,1.4708607,2.5405788,1.5690958],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox5\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox5\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.588025,0.077645175,1.5495262,1.6000838,3.1624737,3.321806,5.4331126,0.69958836,-0.73200244,1.3363643,-0.69135094,5.624961,5.4567394,5.8920603,1.2718222,4.6529107,3.3885114,2.8411257,1.2664549,1.0489063,2.5833333,4.8090477,6.086376,5.5969334,2.0291772,2.733407,2.1832144,2.3209338,2.742142,2.451839,2.3762646,2.3699603,1.7002002,1.0249623,4.199527,3.2500715,3.2553575,1.1022372,6.346275,4.5447416,1.7719276,3.612407,2.4890115,0.67095894,2.0016105,1.046322,3.5487578,4.248,3.4745238,3.367325,5.1334844,2.1857836,1.9421216,1.9208044,1.0988053,2.048632,3.2799015,3.3142698,1.2763399,4.242226,4.093433,1.5593165,1.539337,3.4519353,3.4634063,3.5755446,2.9299335,3.810611,3.7978735,2.8781905,2.9821901,3.0500486,3.6237648,5.63871,6.39578,1.3926187,1.2644144,1.2652044,1.286869,1.0896696,1.5102108,1.499594,1.5316225,1.2424681,1.5104152,1.009357],\"xaxis\":\"x\",\"y\":[1.763543,2.3599463,1.9034739,1.8836085,2.175861,1.5916611,0.8650379,2.231238,2.7242582,-0.37169293,2.7506557,0.81019735,0.8554583,-0.1500628,-0.7819022,1.403428,1.338938,1.8200352,-0.46477395,-0.45858407,1.7272884,1.2752345,0.5706823,0.8071125,1.2290692,2.2661664,2.2932515,2.5714712,2.3162827,2.2011862,2.3303058,1.959129,0.38610074,-0.4565882,1.8182478,1.8995968,1.6919334,-0.36942023,-0.11044071,1.6397489,1.8763597,2.2066865,1.9917114,2.3638558,2.941332,-0.10592871,2.3767984,2.1037362,2.1792824,1.8280481,1.0966597,2.058573,1.8421589,1.6257248,1.3213332,2.0070662,1.7456342,1.5891545,-0.5042989,1.7283028,1.8178942,-0.5125551,-0.68830836,2.146175,2.194834,2.3275955,2.602955,1.4256604,2.2759402,2.6198912,2.3661797,2.5708811,1.4346502,0.8000823,-0.0921387,-0.6752754,-0.7868431,-0.7498604,-0.7915217,-0.53851134,-0.66053635,-0.6893344,-0.6695493,-0.510769,-0.30617675,-0.39673364],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox6\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox6\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[5.6789975,3.6360278,1.5838221,5.9859223,5.415544,5.779011,5.526578,6.802936,5.7377634,4.5908737,3.685119,5.280314,5.590004,6.6093254,5.4916763,6.0498414,6.2229896,3.994961,4.750249,5.3751473,5.7228565,6.127765,5.5168767,5.9932604,5.285295,5.724165,6.8560667,5.8528566,6.8017535,7.0122023,5.62182,6.5222015,6.9013886,2.6442113,6.986702,5.409631,6.8929367,5.7297163,6.66965,5.2093053,6.372627,5.4667974,5.596629,6.861378,6.430335,6.3307457,6.105537,5.43028,4.83471,5.667574,5.384726,4.978211,6.320123,6.1798687,5.6018944,5.263979,4.920869,4.6815877,5.800996,7.0669036,4.6596384,2.1968198,2.707873,5.314682,5.0750933,6.2522645,5.894956,4.843263,5.480191,5.868624,5.7787805,4.8001637,6.5652285,6.379112,7.05121,6.728693,6.2465005,6.550195,6.0030494,5.4085946,4.6144085,3.6281285,6.4062715,0.9417393,2.964179,1.177013,3.250967,3.509882,5.6918983,1.1994011,1.2394247,1.039507,1.9981482,0.93552035,0.897767,0.8694055,1.1303251,1.6345977],\"xaxis\":\"x\",\"y\":[0.76823175,1.7733928,2.465135,-0.5668483,-0.42737156,-0.5586049,0.013952165,-0.11753085,0.719939,1.3284833,2.2474885,0.37762794,0.8275291,-0.25966492,0.8121925,0.3549786,0.23237447,1.0725019,0.3910105,0.31599763,-0.0007787129,0.17751503,0.54128516,-0.10552322,0.6491415,0.18183054,0.033324655,0.30077267,0.013822259,-0.08889604,0.47475058,-0.10737561,-0.023278726,1.4075,-0.1589724,0.86186355,0.031846926,0.7168552,-0.35996538,-0.3461831,-0.33362108,-0.41709983,-0.5436341,-0.0046823216,-0.50589657,-0.37664905,-0.5998616,0.22977115,0.3244986,-0.30329934,0.11234774,0.73879725,-0.39914164,-0.40192616,-0.30172274,0.1024794,0.3004911,0.30188137,0.5124636,-0.057510864,1.3307183,0.70159197,1.237959,-0.06998092,0.11100433,-0.48673597,-0.35405228,0.23498182,-0.17172115,0.44278276,0.49350867,0.21365432,-0.117739394,-0.10017654,-0.17490266,-0.029555049,0.15598282,-0.07292856,0.34204867,0.86244816,0.31755158,0.87327856,-0.50374705,1.7395099,1.1485902,1.6238654,2.564164,2.2895153,0.47830912,1.5803096,1.3457419,1.8946263,1.5441788,1.7491943,1.7986783,1.7929367,1.6097921,1.5350264],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox7\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox7\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[3.0804224,-0.33493567,2.591155,1.1536641,2.85707,2.7266967,-0.61650026,2.4358292,-0.57618296,0.9077103,1.8221993,1.7791634,3.1995974,-0.60539967,3.2266023,0.96415895,6.0119724,6.1237464,6.3549376,1.7956539,1.7884183,2.58853,3.1076822,1.3453476,2.568352,0.40146843,0.38447338,0.35690057,0.86669457,1.0368863,0.45719555,1.5215083,0.5955562,3.374986,3.0276768,3.2264833,3.2944598,3.517221,1.28106,3.6106055,1.5110991,2.0399752,2.1160376,3.643127,3.1111162,2.8784502,2.2980149,3.533009,1.5536321,2.6620111,2.7270646,3.207562,2.6192546,2.5848532,3.6613712,3.7249324,1.9001629,3.1463704,3.390588,2.7939672,3.4435613,3.5000906,1.3095369,2.5949924,2.6505432],\"xaxis\":\"x\",\"y\":[2.7655232,2.393841,1.4454954,1.8680328,1.7700044,2.40631,2.5620627,1.4865438,2.823127,1.7521359,1.6055545,2.2639105,1.5558243,2.8428748,2.4025607,2.6016634,0.08657857,-0.03363896,0.10441889,1.5030893,1.3873515,2.8683736,2.5455105,1.9564257,1.1175891,1.7585535,1.9870793,2.0647984,2.2500038,2.2690957,1.7659074,2.0623543,1.7849313,1.4014297,2.7090435,2.6968987,2.4520009,1.4392744,1.9294903,1.3573451,1.7464308,0.8900724,0.75009906,2.553729,2.6477458,2.0699527,1.9998578,1.5123222,1.8766506,2.9691079,2.9130638,2.6790307,1.4524256,2.8586793,1.344949,1.3325446,0.6955375,1.0758376,1.2062148,0.9168878,1.1434642,1.1200206,1.7606623,1.0899945,2.750742],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"umap_x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"umap_y\"}},\"legend\":{\"title\":{\"text\":\"bbox_name\"},\"tracegroupgap\":0},\"title\":{\"text\":\"UMAP (x vs y)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('021bcba0-7ff5-490e-a7b3-92592434044c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"c5b577fc-ea72-41ff-910a-a9edfa243da1\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c5b577fc-ea72-41ff-910a-a9edfa243da1\")) {                    Plotly.newPlot(                        \"c5b577fc-ea72-41ff-910a-a9edfa243da1\",                        [{\"customdata\":[[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox1\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[2.41263,2.8405435,3.2112813,3.8682745,3.1008127,3.0664816,0.4044861,0.79632866,3.0943813,3.5137486,3.5055268,3.4992557,4.5665107,2.4502635,4.014919,2.6115563,2.8444645,0.60130125,0.7286513,2.6430306,2.3854818,2.299192,2.6573563,2.2870653,2.6042004,1.7968578,1.6444384,2.5996172,3.592825,2.9970717,1.6173,2.9493577,4.2568645,2.6474597,2.584508,2.2620986,2.2212753,2.94586,2.8997843,2.9144015,2.29175,2.6893935,2.6837502,2.8879633,1.3657045,3.8539345,2.526171,4.213559,2.1441433,2.4517722,2.318443,3.324269,2.16239,2.3999054,2.4838493,2.37132,2.3817031,2.3021975],\"xaxis\":\"x\",\"y\":[5.631356,6.3855743,6.19007,6.6685767,6.9465404,6.958422,7.4157667,7.0224333,6.981039,5.4054184,5.9446616,5.6118107,6.246667,3.949304,6.507305,4.1582117,5.556969,6.873834,6.7992573,5.569264,5.408563,5.7114935,5.588109,3.7850096,4.013313,4.733913,5.3439198,4.10923,6.8709044,5.367771,5.335884,5.5705476,4.1391525,4.360586,3.9745467,4.4028463,5.7170324,5.482933,5.5886116,5.574605,4.8398647,6.148655,5.856585,5.088962,5.3788643,6.3805017,4.448108,5.5438743,5.6350513,4.2606025,5.67949,3.9987352,5.750781,3.7730644,3.6848764,3.6747694,3.6528862,3.8513348],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox2\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox2\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[2.008221,1.336219,1.5839216,1.2926859,5.6679783,5.7204094,1.7806102,0.4975393,2.2742248,4.7825227,6.2977533,2.9096558,1.5448548,1.4589165,1.3250877,1.0433645,2.0632598,6.149167,-0.23948579,5.6607537,-0.87918586,1.0533942,0.9267259,-0.34627217,0.42111593,1.2174463,-0.41945922,-0.4029661,-0.15415777,-0.91647434,-0.7721833,6.0333905,4.774583,3.9363217,5.8064303,5.966628,5.9779277,5.6594825,5.5016956,6.2872405,6.0906134,6.703046,6.9306126,6.3427415,5.867302,6.1693807,6.309946,6.6205363,5.074852,5.990981,5.2685018,5.3566594,4.961988,2.414478,-0.9338273,5.7061257,-0.7640931,-0.6356493,0.87244606,1.0172342,1.1461884,0.9282669,0.9177271,-0.7845986,-0.7531676,-0.68588865,-0.46461955,1.0236741,-0.5253631,0.24161917,-0.8701067,-0.08247039,-0.5515687,-0.7188221,0.0005509338,-0.9210684,-0.94093895,-0.985244,-0.80075645,-0.8978583,-0.1476687,-0.29172242,-0.7355936,0.10865344,0.121108346,0.29770857,-0.8394292,-0.69923097,-0.6403705,-0.572723,-0.86840445,-0.90806043,0.0017317804,-0.19602802,-0.23419082,-0.22158,-0.17519522,-0.59326625,-0.93495476,-0.9305485],\"xaxis\":\"x\",\"y\":[7.3282094,5.5198836,6.630788,5.8812685,8.259637,8.320213,6.480857,7.1207795,5.96315,3.9615548,3.9869735,5.280477,5.543113,7.3648715,5.464124,6.7784376,4.850018,3.24689,8.837676,3.4298618,8.586334,7.606988,7.634493,8.8736315,7.783554,6.047938,8.875458,9.096192,8.506387,8.735352,9.248197,3.3451247,4.186961,4.199299,3.14004,3.4051766,3.446027,3.2568247,3.263872,3.3992298,3.298712,3.4794793,3.6502054,3.2742789,3.2039557,3.2980266,3.7728434,3.4475179,3.800736,3.2131474,5.228587,5.118302,5.5170207,5.9442406,8.701098,8.306786,9.339785,9.373669,7.6328745,7.5944505,7.596474,7.6573057,7.6522694,8.711549,8.681561,8.776401,9.183939,6.3607183,9.519199,7.820033,8.562115,8.990291,9.543867,8.981859,8.919427,8.975507,8.977384,8.676827,8.9708805,9.188352,8.98262,8.958036,9.463988,8.321503,7.4269304,8.096784,8.67263,9.367487,8.878752,8.942838,8.65817,8.468514,8.292801,8.907651,8.872386,8.473357,8.817516,9.582705,8.504689,8.619688],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox3\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox3\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.9052733,1.1630459,2.6986802,1.9035199,0.57788587,0.984772,0.944357,0.3740682,1.6131536,2.7531214,1.0577978,0.5773393,3.603735,2.999266,1.8518579,1.8554194,1.9224253,4.212235,1.5979263,2.9574292,2.0310466,1.6478426,2.0872319,2.2136881,1.3151621,1.3472785,3.336064,3.7339203,0.6536247,2.4019988,0.57846314,1.1108916,2.6350765,3.358898,1.8492073,0.37182957,0.58377016,3.1846998,3.2495668,3.3280342,0.4770127,0.7776553,0.9032831,0.91945386,1.0239565,1.5130506,2.8060424,0.62368923,1.1677976,1.2045504,1.7112211,0.9991664,0.80062443,3.321964,0.8438875,1.1857127,4.4543467,0.44081968,-1.0464584,2.7967875,2.876698,4.3568673],\"xaxis\":\"x\",\"y\":[4.7178564,6.410591,6.379648,4.6965995,7.729536,7.8627677,7.0933857,7.190431,5.469369,5.796889,7.513017,7.511224,5.2571745,5.1577024,4.4345193,4.4543796,4.7435975,6.130148,5.0829887,5.6031084,4.6853275,4.82594,4.6426663,4.5230217,7.593933,7.540039,6.968928,6.3463764,8.13886,6.9063625,7.286349,7.147222,6.486513,6.895167,7.8606,6.9880185,6.907093,6.011961,6.718334,7.031431,7.0213614,7.0894513,7.727969,7.4214416,7.3184466,8.003321,6.1541877,6.865877,7.603844,7.670861,7.757605,7.2334065,7.369742,4.6073084,7.9004116,7.9646473,6.1782913,7.3396845,8.847228,6.998262,6.911678,6.270438],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox4\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox4\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.8772495,4.1934566,1.558995,1.54429,3.813722,0.027272787,4.2300205,1.5296135,1.7250987,2.5098572,1.5857309,1.541672,1.4530039,2.2607589,4.2832117,3.435408,3.4404345,1.5393516,2.7874253,2.474262,1.7719675,4.263826,0.4480478,2.426688,2.6666708,2.5687523,2.7221773,3.347529,2.205035,1.1076539,1.8810905,1.6274514,2.9146316,2.6027238,2.8027103,3.0094962,2.8717475,3.4751725,2.9888306,1.3478801],\"xaxis\":\"x\",\"y\":[7.4251256,5.604115,5.315252,5.251929,6.711703,8.417414,5.575988,7.490372,4.9291267,5.4680834,4.8154755,4.9259543,5.3737545,4.8785515,5.5587025,3.986453,3.9837615,5.5021677,6.4940267,5.5167184,7.5362463,5.6761894,7.6010995,7.068627,3.6680763,3.627251,3.6949556,4.248265,4.660692,7.2550797,4.7812057,5.3932843,5.0242395,4.897191,4.8185906,4.570175,4.334226,5.1769147,4.297608,5.7061343],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox5\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox5\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.588025,0.077645175,1.5495262,1.6000838,3.1624737,3.321806,5.4331126,0.69958836,-0.73200244,1.3363643,-0.69135094,5.624961,5.4567394,5.8920603,1.2718222,4.6529107,3.3885114,2.8411257,1.2664549,1.0489063,2.5833333,4.8090477,6.086376,5.5969334,2.0291772,2.733407,2.1832144,2.3209338,2.742142,2.451839,2.3762646,2.3699603,1.7002002,1.0249623,4.199527,3.2500715,3.2553575,1.1022372,6.346275,4.5447416,1.7719276,3.612407,2.4890115,0.67095894,2.0016105,1.046322,3.5487578,4.248,3.4745238,3.367325,5.1334844,2.1857836,1.9421216,1.9208044,1.0988053,2.048632,3.2799015,3.3142698,1.2763399,4.242226,4.093433,1.5593165,1.539337,3.4519353,3.4634063,3.5755446,2.9299335,3.810611,3.7978735,2.8781905,2.9821901,3.0500486,3.6237648,5.63871,6.39578,1.3926187,1.2644144,1.2652044,1.286869,1.0896696,1.5102108,1.499594,1.5316225,1.2424681,1.5104152,1.009357],\"xaxis\":\"x\",\"y\":[7.8903246,8.538729,8.05647,8.0954,4.2043805,5.3132625,7.9988866,7.0731406,9.615474,5.318421,9.557448,8.221569,8.032826,3.0855277,5.649372,6.3072386,4.223668,5.5153785,5.274838,5.3846607,5.8197885,6.5451555,4.368603,8.200048,4.410573,4.3485208,5.896638,5.854591,4.607909,4.868108,5.102589,4.282432,5.10367,5.3865333,6.3012905,5.975948,5.4694414,5.362009,3.6357346,6.1293154,4.9737496,6.7853413,5.11134,6.7492566,4.5730343,5.7521353,6.7187357,5.4897075,3.8752058,3.874288,5.29065,4.4971633,4.48515,4.82994,7.200268,4.3242784,3.8236763,3.8973143,5.221296,6.324513,6.3246784,5.6349154,5.476361,3.824498,3.8268232,3.9253776,4.3147326,5.215443,3.9980702,4.252205,4.3742027,4.430251,5.1775107,8.229855,3.7644355,5.7275405,5.6404076,5.701491,5.6941724,5.4756594,5.461679,5.489318,5.441674,5.268981,5.6709332,5.3885555],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox6\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox6\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[5.6789975,3.6360278,1.5838221,5.9859223,5.415544,5.779011,5.526578,6.802936,5.7377634,4.5908737,3.685119,5.280314,5.590004,6.6093254,5.4916763,6.0498414,6.2229896,3.994961,4.750249,5.3751473,5.7228565,6.127765,5.5168767,5.9932604,5.285295,5.724165,6.8560667,5.8528566,6.8017535,7.0122023,5.62182,6.5222015,6.9013886,2.6442113,6.986702,5.409631,6.8929367,5.7297163,6.66965,5.2093053,6.372627,5.4667974,5.596629,6.861378,6.430335,6.3307457,6.105537,5.43028,4.83471,5.667574,5.384726,4.978211,6.320123,6.1798687,5.6018944,5.263979,4.920869,4.6815877,5.800996,7.0669036,4.6596384,2.1968198,2.707873,5.314682,5.0750933,6.2522645,5.894956,4.843263,5.480191,5.868624,5.7787805,4.8001637,6.5652285,6.379112,7.05121,6.728693,6.2465005,6.550195,6.0030494,5.4085946,4.6144085,3.6281285,6.4062715,0.9417393,2.964179,1.177013,3.250967,3.509882,5.6918983,1.1994011,1.2394247,1.039507,1.9981482,0.93552035,0.897767,0.8694055,1.1303251,1.6345977],\"xaxis\":\"x\",\"y\":[8.279474,6.8042665,7.648979,3.6406415,3.7234275,3.6249864,3.95592,3.5937486,8.334132,6.259629,6.7573833,4.1946087,8.173994,3.637176,8.062469,4.445091,4.2202644,6.1586785,4.1098704,4.263265,4.0831575,4.1467233,4.499368,3.9349415,4.473829,4.201471,3.2405486,4.3244157,3.8198125,3.6869044,4.5485535,4.1109695,3.8538814,6.7540426,3.5717564,7.931646,3.2869232,8.326118,3.8347297,3.8288631,3.383467,3.7427552,3.7973602,3.2404158,3.9046104,4.0519547,3.7514307,4.2435837,4.049206,3.946662,4.102854,4.2922125,3.8630328,3.6656017,3.9145296,4.04826,4.078801,4.018576,4.6670876,3.7080142,6.2299232,4.6291304,6.2171035,3.9500875,4.0533834,3.8609438,3.4790168,4.0048842,3.8989565,4.6345725,4.6034627,3.9710078,3.9741158,4.069631,3.7214463,3.6697466,4.0649633,3.735389,4.3490977,5.0552516,4.0708437,6.1327815,3.8113675,5.5870113,6.085675,5.7056174,7.0692945,6.8282986,4.9313188,5.741597,5.376025,5.377446,6.6949973,5.581029,5.606971,5.662995,5.387168,4.401536],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox7\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox7\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[3.0804224,-0.33493567,2.591155,1.1536641,2.85707,2.7266967,-0.61650026,2.4358292,-0.57618296,0.9077103,1.8221993,1.7791634,3.1995974,-0.60539967,3.2266023,0.96415895,6.0119724,6.1237464,6.3549376,1.7956539,1.7884183,2.58853,3.1076822,1.3453476,2.568352,0.40146843,0.38447338,0.35690057,0.86669457,1.0368863,0.45719555,1.5215083,0.5955562,3.374986,3.0276768,3.2264833,3.2944598,3.517221,1.28106,3.6106055,1.5110991,2.0399752,2.1160376,3.643127,3.1111162,2.8784502,2.2980149,3.533009,1.5536321,2.6620111,2.7270646,3.207562,2.6192546,2.5848532,3.6613712,3.7249324,1.9001629,3.1463704,3.390588,2.7939672,3.4435613,3.5000906,1.3095369,2.5949924,2.6505432],\"xaxis\":\"x\",\"y\":[7.0126762,8.981945,7.0168815,5.31649,6.9503684,7.3075304,9.411318,6.7991858,9.626831,7.1952376,4.5037766,4.737111,4.8573475,9.590493,6.8364325,7.623087,3.2705803,3.194401,3.387409,4.284925,4.3550615,4.6032157,7.124465,5.082313,6.428931,7.1289363,7.0075107,7.0376015,8.657453,8.626404,7.2073464,7.6258426,7.5412607,4.632761,3.9919891,4.0349436,7.105459,5.203869,5.3091865,4.366527,5.24328,4.535887,4.547343,4.225662,4.621583,4.3424163,4.903838,5.367071,5.4005895,4.0526233,4.028475,4.7641344,6.855131,3.9562187,4.3321686,4.3448358,4.454674,4.389905,4.4684176,4.5141015,4.291552,4.303883,7.4091663,6.403949,5.6811843],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"umap_x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"umap_z\"}},\"legend\":{\"title\":{\"text\":\"bbox_name\"},\"tracegroupgap\":0},\"title\":{\"text\":\"UMAP (x vs z)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c5b577fc-ea72-41ff-910a-a9edfa243da1');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"d90d707c-80ed-4c6d-b830-0591edfac8b0\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d90d707c-80ed-4c6d-b830-0591edfac8b0\")) {                    Plotly.newPlot(                        \"d90d707c-80ed-4c6d-b830-0591edfac8b0\",                        [{\"customdata\":[[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"],[\"bbox1\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_y=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox1\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[2.8237913,1.3639009,1.4746708,1.782302,2.780227,2.7588482,2.5451133,0.9149695,2.336175,1.474145,1.5371263,1.4629123,1.3355843,1.8233283,1.7744348,3.0366395,3.2409792,2.412801,2.4971313,2.9273474,2.524806,2.7466176,2.9953718,1.6864046,2.8750482,1.7544695,-0.5852801,2.2179105,2.1229663,1.8729151,-0.50735426,3.2971678,0.6319819,3.039042,2.8103507,1.6000838,2.6498215,3.3816595,3.3792968,3.3377938,2.8909895,2.8212097,2.8312333,3.3156557,1.4707342,1.2890917,2.7378936,2.206582,2.469945,1.570091,2.7683372,1.9187359,2.57266,1.7678288,1.6573162,1.6718076,1.6728762,1.6256465],\"xaxis\":\"x\",\"y\":[5.631356,6.3855743,6.19007,6.6685767,6.9465404,6.958422,7.4157667,7.0224333,6.981039,5.4054184,5.9446616,5.6118107,6.246667,3.949304,6.507305,4.1582117,5.556969,6.873834,6.7992573,5.569264,5.408563,5.7114935,5.588109,3.7850096,4.013313,4.733913,5.3439198,4.10923,6.8709044,5.367771,5.335884,5.5705476,4.1391525,4.360586,3.9745467,4.4028463,5.7170324,5.482933,5.5886116,5.574605,4.8398647,6.148655,5.856585,5.088962,5.3788643,6.3805017,4.448108,5.5438743,5.6350513,4.2606025,5.67949,3.9987352,5.750781,3.7730644,3.6848764,3.6747694,3.6528862,3.8513348],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"],[\"bbox2\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_y=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox2\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox2\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.8505154,1.4626622,1.6144924,0.9546825,0.7600041,0.73134005,1.3810247,2.2993941,2.2108428,0.2746233,-0.37739682,3.223761,1.9617682,1.6937413,1.756045,1.6877652,1.6376649,-0.19830045,2.3780408,-0.20773582,3.0262082,2.4759626,2.6239746,2.7593174,2.560442,-0.094971694,2.496995,2.4141173,2.208344,2.819719,2.6770082,-0.3192255,0.4496833,0.8965682,-0.20094536,-0.44899496,-0.20639372,-0.12175842,-0.072279364,-0.113431275,-0.31900525,-0.09841616,-0.048884016,-0.15589328,-0.17622449,-0.08042106,-0.51107466,-0.1626262,0.08827878,-0.22535992,0.95604205,0.89804924,1.140321,1.8922007,2.6804514,0.7538889,2.775328,2.8684218,2.5673215,2.5777886,2.317145,2.64408,2.63956,2.5870767,2.5726695,2.6109257,2.6793115,0.26362512,2.8597262,2.5457113,2.6257532,2.5810955,2.7725341,2.92764,2.4546933,2.7200367,2.717126,2.56559,2.8730805,2.6555693,2.3736036,2.4186568,2.9044974,2.6553922,2.0573797,2.1072257,2.6206095,2.8920927,2.755993,2.6816428,2.740539,2.8260748,2.1172311,2.5680757,2.7676728,2.2157197,2.3354328,2.8899047,2.7791095,2.9622035],\"xaxis\":\"x\",\"y\":[7.3282094,5.5198836,6.630788,5.8812685,8.259637,8.320213,6.480857,7.1207795,5.96315,3.9615548,3.9869735,5.280477,5.543113,7.3648715,5.464124,6.7784376,4.850018,3.24689,8.837676,3.4298618,8.586334,7.606988,7.634493,8.8736315,7.783554,6.047938,8.875458,9.096192,8.506387,8.735352,9.248197,3.3451247,4.186961,4.199299,3.14004,3.4051766,3.446027,3.2568247,3.263872,3.3992298,3.298712,3.4794793,3.6502054,3.2742789,3.2039557,3.2980266,3.7728434,3.4475179,3.800736,3.2131474,5.228587,5.118302,5.5170207,5.9442406,8.701098,8.306786,9.339785,9.373669,7.6328745,7.5944505,7.596474,7.6573057,7.6522694,8.711549,8.681561,8.776401,9.183939,6.3607183,9.519199,7.820033,8.562115,8.990291,9.543867,8.981859,8.919427,8.975507,8.977384,8.676827,8.9708805,9.188352,8.98262,8.958036,9.463988,8.321503,7.4269304,8.096784,8.67263,9.367487,8.878752,8.942838,8.65817,8.468514,8.292801,8.907651,8.872386,8.473357,8.817516,9.582705,8.504689,8.619688],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"],[\"bbox3\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_y=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox3\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox3\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[2.8673117,1.878824,1.2108725,2.8885508,1.9379443,1.4206247,1.093815,1.7522157,0.036317006,0.57623106,1.3492024,1.6604799,1.4568688,3.2340093,1.4421904,1.2057422,0.86688465,0.9826603,2.309547,3.393512,2.9592986,2.5191245,2.925057,0.73982376,1.8679345,2.1006682,1.8798236,1.2622542,1.7942501,1.6336757,1.4473742,1.7676128,1.1311151,2.0205836,2.0119507,2.1197307,2.3667762,0.8504343,2.0785446,2.48844,2.2153924,2.021058,1.7614807,1.6400557,1.6484245,1.9963715,0.78526956,2.3758576,1.5516102,1.557142,2.1463332,1.899385,1.6533997,0.6845402,1.6838056,1.8137416,1.207868,1.665228,2.783216,2.378056,2.5087621,1.170352],\"xaxis\":\"x\",\"y\":[4.7178564,6.410591,6.379648,4.6965995,7.729536,7.8627677,7.0933857,7.190431,5.469369,5.796889,7.513017,7.511224,5.2571745,5.1577024,4.4345193,4.4543796,4.7435975,6.130148,5.0829887,5.6031084,4.6853275,4.82594,4.6426663,4.5230217,7.593933,7.540039,6.968928,6.3463764,8.13886,6.9063625,7.286349,7.147222,6.486513,6.895167,7.8606,6.9880185,6.907093,6.011961,6.718334,7.031431,7.0213614,7.0894513,7.727969,7.4214416,7.3184466,8.003321,6.1541877,6.865877,7.603844,7.670861,7.757605,7.2334065,7.369742,4.6073084,7.9004116,7.9646473,6.1782913,7.3396845,8.847228,6.998262,6.911678,6.270438],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"],[\"bbox4\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_y=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox4\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox4\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[2.06553,2.3769598,-0.45451063,-0.23604158,1.8965014,2.223792,2.3292158,1.9429204,1.5272801,2.59867,1.5142825,1.676482,1.7339458,1.8354064,2.2507718,2.4774559,2.470766,-0.16611609,1.3194376,2.687437,1.9834803,2.3112764,2.346658,1.8064451,1.9593651,1.7804286,1.644105,0.8381123,1.9657755,2.2183635,0.8639911,-0.6780791,1.4616764,1.1923044,2.5441966,2.5987163,2.2711046,1.4708607,2.5405788,1.5690958],\"xaxis\":\"x\",\"y\":[7.4251256,5.604115,5.315252,5.251929,6.711703,8.417414,5.575988,7.490372,4.9291267,5.4680834,4.8154755,4.9259543,5.3737545,4.8785515,5.5587025,3.986453,3.9837615,5.5021677,6.4940267,5.5167184,7.5362463,5.6761894,7.6010995,7.068627,3.6680763,3.627251,3.6949556,4.248265,4.660692,7.2550797,4.7812057,5.3932843,5.0242395,4.897191,4.8185906,4.570175,4.334226,5.1769147,4.297608,5.7061343],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"],[\"bbox5\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_y=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox5\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox5\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.763543,2.3599463,1.9034739,1.8836085,2.175861,1.5916611,0.8650379,2.231238,2.7242582,-0.37169293,2.7506557,0.81019735,0.8554583,-0.1500628,-0.7819022,1.403428,1.338938,1.8200352,-0.46477395,-0.45858407,1.7272884,1.2752345,0.5706823,0.8071125,1.2290692,2.2661664,2.2932515,2.5714712,2.3162827,2.2011862,2.3303058,1.959129,0.38610074,-0.4565882,1.8182478,1.8995968,1.6919334,-0.36942023,-0.11044071,1.6397489,1.8763597,2.2066865,1.9917114,2.3638558,2.941332,-0.10592871,2.3767984,2.1037362,2.1792824,1.8280481,1.0966597,2.058573,1.8421589,1.6257248,1.3213332,2.0070662,1.7456342,1.5891545,-0.5042989,1.7283028,1.8178942,-0.5125551,-0.68830836,2.146175,2.194834,2.3275955,2.602955,1.4256604,2.2759402,2.6198912,2.3661797,2.5708811,1.4346502,0.8000823,-0.0921387,-0.6752754,-0.7868431,-0.7498604,-0.7915217,-0.53851134,-0.66053635,-0.6893344,-0.6695493,-0.510769,-0.30617675,-0.39673364],\"xaxis\":\"x\",\"y\":[7.8903246,8.538729,8.05647,8.0954,4.2043805,5.3132625,7.9988866,7.0731406,9.615474,5.318421,9.557448,8.221569,8.032826,3.0855277,5.649372,6.3072386,4.223668,5.5153785,5.274838,5.3846607,5.8197885,6.5451555,4.368603,8.200048,4.410573,4.3485208,5.896638,5.854591,4.607909,4.868108,5.102589,4.282432,5.10367,5.3865333,6.3012905,5.975948,5.4694414,5.362009,3.6357346,6.1293154,4.9737496,6.7853413,5.11134,6.7492566,4.5730343,5.7521353,6.7187357,5.4897075,3.8752058,3.874288,5.29065,4.4971633,4.48515,4.82994,7.200268,4.3242784,3.8236763,3.8973143,5.221296,6.324513,6.3246784,5.6349154,5.476361,3.824498,3.8268232,3.9253776,4.3147326,5.215443,3.9980702,4.252205,4.3742027,4.430251,5.1775107,8.229855,3.7644355,5.7275405,5.6404076,5.701491,5.6941724,5.4756594,5.461679,5.489318,5.441674,5.268981,5.6709332,5.3885555],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"],[\"bbox6\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_y=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox6\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox6\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0.76823175,1.7733928,2.465135,-0.5668483,-0.42737156,-0.5586049,0.013952165,-0.11753085,0.719939,1.3284833,2.2474885,0.37762794,0.8275291,-0.25966492,0.8121925,0.3549786,0.23237447,1.0725019,0.3910105,0.31599763,-0.0007787129,0.17751503,0.54128516,-0.10552322,0.6491415,0.18183054,0.033324655,0.30077267,0.013822259,-0.08889604,0.47475058,-0.10737561,-0.023278726,1.4075,-0.1589724,0.86186355,0.031846926,0.7168552,-0.35996538,-0.3461831,-0.33362108,-0.41709983,-0.5436341,-0.0046823216,-0.50589657,-0.37664905,-0.5998616,0.22977115,0.3244986,-0.30329934,0.11234774,0.73879725,-0.39914164,-0.40192616,-0.30172274,0.1024794,0.3004911,0.30188137,0.5124636,-0.057510864,1.3307183,0.70159197,1.237959,-0.06998092,0.11100433,-0.48673597,-0.35405228,0.23498182,-0.17172115,0.44278276,0.49350867,0.21365432,-0.117739394,-0.10017654,-0.17490266,-0.029555049,0.15598282,-0.07292856,0.34204867,0.86244816,0.31755158,0.87327856,-0.50374705,1.7395099,1.1485902,1.6238654,2.564164,2.2895153,0.47830912,1.5803096,1.3457419,1.8946263,1.5441788,1.7491943,1.7986783,1.7929367,1.6097921,1.5350264],\"xaxis\":\"x\",\"y\":[8.279474,6.8042665,7.648979,3.6406415,3.7234275,3.6249864,3.95592,3.5937486,8.334132,6.259629,6.7573833,4.1946087,8.173994,3.637176,8.062469,4.445091,4.2202644,6.1586785,4.1098704,4.263265,4.0831575,4.1467233,4.499368,3.9349415,4.473829,4.201471,3.2405486,4.3244157,3.8198125,3.6869044,4.5485535,4.1109695,3.8538814,6.7540426,3.5717564,7.931646,3.2869232,8.326118,3.8347297,3.8288631,3.383467,3.7427552,3.7973602,3.2404158,3.9046104,4.0519547,3.7514307,4.2435837,4.049206,3.946662,4.102854,4.2922125,3.8630328,3.6656017,3.9145296,4.04826,4.078801,4.018576,4.6670876,3.7080142,6.2299232,4.6291304,6.2171035,3.9500875,4.0533834,3.8609438,3.4790168,4.0048842,3.8989565,4.6345725,4.6034627,3.9710078,3.9741158,4.069631,3.7214463,3.6697466,4.0649633,3.735389,4.3490977,5.0552516,4.0708437,6.1327815,3.8113675,5.5870113,6.085675,5.7056174,7.0692945,6.8282986,4.9313188,5.741597,5.376025,5.377446,6.6949973,5.581029,5.606971,5.662995,5.387168,4.401536],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"],[\"bbox7\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_y=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox7\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox7\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[2.7655232,2.393841,1.4454954,1.8680328,1.7700044,2.40631,2.5620627,1.4865438,2.823127,1.7521359,1.6055545,2.2639105,1.5558243,2.8428748,2.4025607,2.6016634,0.08657857,-0.03363896,0.10441889,1.5030893,1.3873515,2.8683736,2.5455105,1.9564257,1.1175891,1.7585535,1.9870793,2.0647984,2.2500038,2.2690957,1.7659074,2.0623543,1.7849313,1.4014297,2.7090435,2.6968987,2.4520009,1.4392744,1.9294903,1.3573451,1.7464308,0.8900724,0.75009906,2.553729,2.6477458,2.0699527,1.9998578,1.5123222,1.8766506,2.9691079,2.9130638,2.6790307,1.4524256,2.8586793,1.344949,1.3325446,0.6955375,1.0758376,1.2062148,0.9168878,1.1434642,1.1200206,1.7606623,1.0899945,2.750742],\"xaxis\":\"x\",\"y\":[7.0126762,8.981945,7.0168815,5.31649,6.9503684,7.3075304,9.411318,6.7991858,9.626831,7.1952376,4.5037766,4.737111,4.8573475,9.590493,6.8364325,7.623087,3.2705803,3.194401,3.387409,4.284925,4.3550615,4.6032157,7.124465,5.082313,6.428931,7.1289363,7.0075107,7.0376015,8.657453,8.626404,7.2073464,7.6258426,7.5412607,4.632761,3.9919891,4.0349436,7.105459,5.203869,5.3091865,4.366527,5.24328,4.535887,4.547343,4.225662,4.621583,4.3424163,4.903838,5.367071,5.4005895,4.0526233,4.028475,4.7641344,6.855131,3.9562187,4.3321686,4.3448358,4.454674,4.389905,4.4684176,4.5141015,4.291552,4.303883,7.4091663,6.403949,5.6811843],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"umap_y\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"umap_z\"}},\"legend\":{\"title\":{\"text\":\"bbox_name\"},\"tracegroupgap\":0},\"title\":{\"text\":\"UMAP (y vs z)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d90d707c-80ed-4c6d-b830-0591edfac8b0');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating combined 2D UMAP projections...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"7943f379-ceb0-412b-b17e-e78a9cfef8c7\" class=\"plotly-graph-div\" style=\"height:600px; width:1800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7943f379-ceb0-412b-b17e-e78a9cfef8c7\")) {                    Plotly.newPlot(                        \"7943f379-ceb0-412b-b17e-e78a9cfef8c7\",                        [{\"hovertemplate\":\"bbox_name:%{text}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\",\"marker\":{\"color\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"showscale\":true,\"size\":5},\"mode\":\"markers\",\"name\":\"(x vs y)\",\"text\":[\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\"],\"x\":[2.41263,2.8405435,3.2112813,3.8682745,3.1008127,3.0664816,0.4044861,0.79632866,3.0943813,3.5137486,3.5055268,3.4992557,4.5665107,2.4502635,4.014919,2.6115563,2.8444645,0.60130125,0.7286513,2.6430306,2.3854818,2.299192,2.6573563,2.2870653,2.6042004,1.7968578,1.6444384,2.5996172,3.592825,2.9970717,1.6173,2.9493577,4.2568645,2.6474597,2.584508,2.2620986,2.2212753,2.94586,2.8997843,2.9144015,2.29175,2.6893935,2.6837502,2.8879633,1.3657045,3.8539345,2.526171,4.213559,2.1441433,2.4517722,2.318443,3.324269,2.16239,2.3999054,2.4838493,2.37132,2.3817031,2.3021975,2.008221,1.336219,1.5839216,1.2926859,5.6679783,5.7204094,1.7806102,0.4975393,2.2742248,4.7825227,6.2977533,2.9096558,1.5448548,1.4589165,1.3250877,1.0433645,2.0632598,6.149167,-0.23948579,5.6607537,-0.87918586,1.0533942,0.9267259,-0.34627217,0.42111593,1.2174463,-0.41945922,-0.4029661,-0.15415777,-0.91647434,-0.7721833,6.0333905,4.774583,3.9363217,5.8064303,5.966628,5.9779277,5.6594825,5.5016956,6.2872405,6.0906134,6.703046,6.9306126,6.3427415,5.867302,6.1693807,6.309946,6.6205363,5.074852,5.990981,5.2685018,5.3566594,4.961988,2.414478,-0.9338273,5.7061257,-0.7640931,-0.6356493,0.87244606,1.0172342,1.1461884,0.9282669,0.9177271,-0.7845986,-0.7531676,-0.68588865,-0.46461955,1.0236741,-0.5253631,0.24161917,-0.8701067,-0.08247039,-0.5515687,-0.7188221,0.0005509338,-0.9210684,-0.94093895,-0.985244,-0.80075645,-0.8978583,-0.1476687,-0.29172242,-0.7355936,0.10865344,0.121108346,0.29770857,-0.8394292,-0.69923097,-0.6403705,-0.572723,-0.86840445,-0.90806043,0.0017317804,-0.19602802,-0.23419082,-0.22158,-0.17519522,-0.59326625,-0.93495476,-0.9305485,1.9052733,1.1630459,2.6986802,1.9035199,0.57788587,0.984772,0.944357,0.3740682,1.6131536,2.7531214,1.0577978,0.5773393,3.603735,2.999266,1.8518579,1.8554194,1.9224253,4.212235,1.5979263,2.9574292,2.0310466,1.6478426,2.0872319,2.2136881,1.3151621,1.3472785,3.336064,3.7339203,0.6536247,2.4019988,0.57846314,1.1108916,2.6350765,3.358898,1.8492073,0.37182957,0.58377016,3.1846998,3.2495668,3.3280342,0.4770127,0.7776553,0.9032831,0.91945386,1.0239565,1.5130506,2.8060424,0.62368923,1.1677976,1.2045504,1.7112211,0.9991664,0.80062443,3.321964,0.8438875,1.1857127,4.4543467,0.44081968,-1.0464584,2.7967875,2.876698,4.3568673,1.8772495,4.1934566,1.558995,1.54429,3.813722,0.027272787,4.2300205,1.5296135,1.7250987,2.5098572,1.5857309,1.541672,1.4530039,2.2607589,4.2832117,3.435408,3.4404345,1.5393516,2.7874253,2.474262,1.7719675,4.263826,0.4480478,2.426688,2.6666708,2.5687523,2.7221773,3.347529,2.205035,1.1076539,1.8810905,1.6274514,2.9146316,2.6027238,2.8027103,3.0094962,2.8717475,3.4751725,2.9888306,1.3478801,1.588025,0.077645175,1.5495262,1.6000838,3.1624737,3.321806,5.4331126,0.69958836,-0.73200244,1.3363643,-0.69135094,5.624961,5.4567394,5.8920603,1.2718222,4.6529107,3.3885114,2.8411257,1.2664549,1.0489063,2.5833333,4.8090477,6.086376,5.5969334,2.0291772,2.733407,2.1832144,2.3209338,2.742142,2.451839,2.3762646,2.3699603,1.7002002,1.0249623,4.199527,3.2500715,3.2553575,1.1022372,6.346275,4.5447416,1.7719276,3.612407,2.4890115,0.67095894,2.0016105,1.046322,3.5487578,4.248,3.4745238,3.367325,5.1334844,2.1857836,1.9421216,1.9208044,1.0988053,2.048632,3.2799015,3.3142698,1.2763399,4.242226,4.093433,1.5593165,1.539337,3.4519353,3.4634063,3.5755446,2.9299335,3.810611,3.7978735,2.8781905,2.9821901,3.0500486,3.6237648,5.63871,6.39578,1.3926187,1.2644144,1.2652044,1.286869,1.0896696,1.5102108,1.499594,1.5316225,1.2424681,1.5104152,1.009357,5.6789975,3.6360278,1.5838221,5.9859223,5.415544,5.779011,5.526578,6.802936,5.7377634,4.5908737,3.685119,5.280314,5.590004,6.6093254,5.4916763,6.0498414,6.2229896,3.994961,4.750249,5.3751473,5.7228565,6.127765,5.5168767,5.9932604,5.285295,5.724165,6.8560667,5.8528566,6.8017535,7.0122023,5.62182,6.5222015,6.9013886,2.6442113,6.986702,5.409631,6.8929367,5.7297163,6.66965,5.2093053,6.372627,5.4667974,5.596629,6.861378,6.430335,6.3307457,6.105537,5.43028,4.83471,5.667574,5.384726,4.978211,6.320123,6.1798687,5.6018944,5.263979,4.920869,4.6815877,5.800996,7.0669036,4.6596384,2.1968198,2.707873,5.314682,5.0750933,6.2522645,5.894956,4.843263,5.480191,5.868624,5.7787805,4.8001637,6.5652285,6.379112,7.05121,6.728693,6.2465005,6.550195,6.0030494,5.4085946,4.6144085,3.6281285,6.4062715,0.9417393,2.964179,1.177013,3.250967,3.509882,5.6918983,1.1994011,1.2394247,1.039507,1.9981482,0.93552035,0.897767,0.8694055,1.1303251,1.6345977,3.0804224,-0.33493567,2.591155,1.1536641,2.85707,2.7266967,-0.61650026,2.4358292,-0.57618296,0.9077103,1.8221993,1.7791634,3.1995974,-0.60539967,3.2266023,0.96415895,6.0119724,6.1237464,6.3549376,1.7956539,1.7884183,2.58853,3.1076822,1.3453476,2.568352,0.40146843,0.38447338,0.35690057,0.86669457,1.0368863,0.45719555,1.5215083,0.5955562,3.374986,3.0276768,3.2264833,3.2944598,3.517221,1.28106,3.6106055,1.5110991,2.0399752,2.1160376,3.643127,3.1111162,2.8784502,2.2980149,3.533009,1.5536321,2.6620111,2.7270646,3.207562,2.6192546,2.5848532,3.6613712,3.7249324,1.9001629,3.1463704,3.390588,2.7939672,3.4435613,3.5000906,1.3095369,2.5949924,2.6505432],\"y\":[2.8237913,1.3639009,1.4746708,1.782302,2.780227,2.7588482,2.5451133,0.9149695,2.336175,1.474145,1.5371263,1.4629123,1.3355843,1.8233283,1.7744348,3.0366395,3.2409792,2.412801,2.4971313,2.9273474,2.524806,2.7466176,2.9953718,1.6864046,2.8750482,1.7544695,-0.5852801,2.2179105,2.1229663,1.8729151,-0.50735426,3.2971678,0.6319819,3.039042,2.8103507,1.6000838,2.6498215,3.3816595,3.3792968,3.3377938,2.8909895,2.8212097,2.8312333,3.3156557,1.4707342,1.2890917,2.7378936,2.206582,2.469945,1.570091,2.7683372,1.9187359,2.57266,1.7678288,1.6573162,1.6718076,1.6728762,1.6256465,1.8505154,1.4626622,1.6144924,0.9546825,0.7600041,0.73134005,1.3810247,2.2993941,2.2108428,0.2746233,-0.37739682,3.223761,1.9617682,1.6937413,1.756045,1.6877652,1.6376649,-0.19830045,2.3780408,-0.20773582,3.0262082,2.4759626,2.6239746,2.7593174,2.560442,-0.094971694,2.496995,2.4141173,2.208344,2.819719,2.6770082,-0.3192255,0.4496833,0.8965682,-0.20094536,-0.44899496,-0.20639372,-0.12175842,-0.072279364,-0.113431275,-0.31900525,-0.09841616,-0.048884016,-0.15589328,-0.17622449,-0.08042106,-0.51107466,-0.1626262,0.08827878,-0.22535992,0.95604205,0.89804924,1.140321,1.8922007,2.6804514,0.7538889,2.775328,2.8684218,2.5673215,2.5777886,2.317145,2.64408,2.63956,2.5870767,2.5726695,2.6109257,2.6793115,0.26362512,2.8597262,2.5457113,2.6257532,2.5810955,2.7725341,2.92764,2.4546933,2.7200367,2.717126,2.56559,2.8730805,2.6555693,2.3736036,2.4186568,2.9044974,2.6553922,2.0573797,2.1072257,2.6206095,2.8920927,2.755993,2.6816428,2.740539,2.8260748,2.1172311,2.5680757,2.7676728,2.2157197,2.3354328,2.8899047,2.7791095,2.9622035,2.8673117,1.878824,1.2108725,2.8885508,1.9379443,1.4206247,1.093815,1.7522157,0.036317006,0.57623106,1.3492024,1.6604799,1.4568688,3.2340093,1.4421904,1.2057422,0.86688465,0.9826603,2.309547,3.393512,2.9592986,2.5191245,2.925057,0.73982376,1.8679345,2.1006682,1.8798236,1.2622542,1.7942501,1.6336757,1.4473742,1.7676128,1.1311151,2.0205836,2.0119507,2.1197307,2.3667762,0.8504343,2.0785446,2.48844,2.2153924,2.021058,1.7614807,1.6400557,1.6484245,1.9963715,0.78526956,2.3758576,1.5516102,1.557142,2.1463332,1.899385,1.6533997,0.6845402,1.6838056,1.8137416,1.207868,1.665228,2.783216,2.378056,2.5087621,1.170352,2.06553,2.3769598,-0.45451063,-0.23604158,1.8965014,2.223792,2.3292158,1.9429204,1.5272801,2.59867,1.5142825,1.676482,1.7339458,1.8354064,2.2507718,2.4774559,2.470766,-0.16611609,1.3194376,2.687437,1.9834803,2.3112764,2.346658,1.8064451,1.9593651,1.7804286,1.644105,0.8381123,1.9657755,2.2183635,0.8639911,-0.6780791,1.4616764,1.1923044,2.5441966,2.5987163,2.2711046,1.4708607,2.5405788,1.5690958,1.763543,2.3599463,1.9034739,1.8836085,2.175861,1.5916611,0.8650379,2.231238,2.7242582,-0.37169293,2.7506557,0.81019735,0.8554583,-0.1500628,-0.7819022,1.403428,1.338938,1.8200352,-0.46477395,-0.45858407,1.7272884,1.2752345,0.5706823,0.8071125,1.2290692,2.2661664,2.2932515,2.5714712,2.3162827,2.2011862,2.3303058,1.959129,0.38610074,-0.4565882,1.8182478,1.8995968,1.6919334,-0.36942023,-0.11044071,1.6397489,1.8763597,2.2066865,1.9917114,2.3638558,2.941332,-0.10592871,2.3767984,2.1037362,2.1792824,1.8280481,1.0966597,2.058573,1.8421589,1.6257248,1.3213332,2.0070662,1.7456342,1.5891545,-0.5042989,1.7283028,1.8178942,-0.5125551,-0.68830836,2.146175,2.194834,2.3275955,2.602955,1.4256604,2.2759402,2.6198912,2.3661797,2.5708811,1.4346502,0.8000823,-0.0921387,-0.6752754,-0.7868431,-0.7498604,-0.7915217,-0.53851134,-0.66053635,-0.6893344,-0.6695493,-0.510769,-0.30617675,-0.39673364,0.76823175,1.7733928,2.465135,-0.5668483,-0.42737156,-0.5586049,0.013952165,-0.11753085,0.719939,1.3284833,2.2474885,0.37762794,0.8275291,-0.25966492,0.8121925,0.3549786,0.23237447,1.0725019,0.3910105,0.31599763,-0.0007787129,0.17751503,0.54128516,-0.10552322,0.6491415,0.18183054,0.033324655,0.30077267,0.013822259,-0.08889604,0.47475058,-0.10737561,-0.023278726,1.4075,-0.1589724,0.86186355,0.031846926,0.7168552,-0.35996538,-0.3461831,-0.33362108,-0.41709983,-0.5436341,-0.0046823216,-0.50589657,-0.37664905,-0.5998616,0.22977115,0.3244986,-0.30329934,0.11234774,0.73879725,-0.39914164,-0.40192616,-0.30172274,0.1024794,0.3004911,0.30188137,0.5124636,-0.057510864,1.3307183,0.70159197,1.237959,-0.06998092,0.11100433,-0.48673597,-0.35405228,0.23498182,-0.17172115,0.44278276,0.49350867,0.21365432,-0.117739394,-0.10017654,-0.17490266,-0.029555049,0.15598282,-0.07292856,0.34204867,0.86244816,0.31755158,0.87327856,-0.50374705,1.7395099,1.1485902,1.6238654,2.564164,2.2895153,0.47830912,1.5803096,1.3457419,1.8946263,1.5441788,1.7491943,1.7986783,1.7929367,1.6097921,1.5350264,2.7655232,2.393841,1.4454954,1.8680328,1.7700044,2.40631,2.5620627,1.4865438,2.823127,1.7521359,1.6055545,2.2639105,1.5558243,2.8428748,2.4025607,2.6016634,0.08657857,-0.03363896,0.10441889,1.5030893,1.3873515,2.8683736,2.5455105,1.9564257,1.1175891,1.7585535,1.9870793,2.0647984,2.2500038,2.2690957,1.7659074,2.0623543,1.7849313,1.4014297,2.7090435,2.6968987,2.4520009,1.4392744,1.9294903,1.3573451,1.7464308,0.8900724,0.75009906,2.553729,2.6477458,2.0699527,1.9998578,1.5123222,1.8766506,2.9691079,2.9130638,2.6790307,1.4524256,2.8586793,1.344949,1.3325446,0.6955375,1.0758376,1.2062148,0.9168878,1.1434642,1.1200206,1.7606623,1.0899945,2.750742],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"hovertemplate\":\"bbox_name:%{text}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_z=%{y}\",\"marker\":{\"color\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"showscale\":false,\"size\":5},\"mode\":\"markers\",\"name\":\"(x vs z)\",\"text\":[\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\"],\"x\":[2.41263,2.8405435,3.2112813,3.8682745,3.1008127,3.0664816,0.4044861,0.79632866,3.0943813,3.5137486,3.5055268,3.4992557,4.5665107,2.4502635,4.014919,2.6115563,2.8444645,0.60130125,0.7286513,2.6430306,2.3854818,2.299192,2.6573563,2.2870653,2.6042004,1.7968578,1.6444384,2.5996172,3.592825,2.9970717,1.6173,2.9493577,4.2568645,2.6474597,2.584508,2.2620986,2.2212753,2.94586,2.8997843,2.9144015,2.29175,2.6893935,2.6837502,2.8879633,1.3657045,3.8539345,2.526171,4.213559,2.1441433,2.4517722,2.318443,3.324269,2.16239,2.3999054,2.4838493,2.37132,2.3817031,2.3021975,2.008221,1.336219,1.5839216,1.2926859,5.6679783,5.7204094,1.7806102,0.4975393,2.2742248,4.7825227,6.2977533,2.9096558,1.5448548,1.4589165,1.3250877,1.0433645,2.0632598,6.149167,-0.23948579,5.6607537,-0.87918586,1.0533942,0.9267259,-0.34627217,0.42111593,1.2174463,-0.41945922,-0.4029661,-0.15415777,-0.91647434,-0.7721833,6.0333905,4.774583,3.9363217,5.8064303,5.966628,5.9779277,5.6594825,5.5016956,6.2872405,6.0906134,6.703046,6.9306126,6.3427415,5.867302,6.1693807,6.309946,6.6205363,5.074852,5.990981,5.2685018,5.3566594,4.961988,2.414478,-0.9338273,5.7061257,-0.7640931,-0.6356493,0.87244606,1.0172342,1.1461884,0.9282669,0.9177271,-0.7845986,-0.7531676,-0.68588865,-0.46461955,1.0236741,-0.5253631,0.24161917,-0.8701067,-0.08247039,-0.5515687,-0.7188221,0.0005509338,-0.9210684,-0.94093895,-0.985244,-0.80075645,-0.8978583,-0.1476687,-0.29172242,-0.7355936,0.10865344,0.121108346,0.29770857,-0.8394292,-0.69923097,-0.6403705,-0.572723,-0.86840445,-0.90806043,0.0017317804,-0.19602802,-0.23419082,-0.22158,-0.17519522,-0.59326625,-0.93495476,-0.9305485,1.9052733,1.1630459,2.6986802,1.9035199,0.57788587,0.984772,0.944357,0.3740682,1.6131536,2.7531214,1.0577978,0.5773393,3.603735,2.999266,1.8518579,1.8554194,1.9224253,4.212235,1.5979263,2.9574292,2.0310466,1.6478426,2.0872319,2.2136881,1.3151621,1.3472785,3.336064,3.7339203,0.6536247,2.4019988,0.57846314,1.1108916,2.6350765,3.358898,1.8492073,0.37182957,0.58377016,3.1846998,3.2495668,3.3280342,0.4770127,0.7776553,0.9032831,0.91945386,1.0239565,1.5130506,2.8060424,0.62368923,1.1677976,1.2045504,1.7112211,0.9991664,0.80062443,3.321964,0.8438875,1.1857127,4.4543467,0.44081968,-1.0464584,2.7967875,2.876698,4.3568673,1.8772495,4.1934566,1.558995,1.54429,3.813722,0.027272787,4.2300205,1.5296135,1.7250987,2.5098572,1.5857309,1.541672,1.4530039,2.2607589,4.2832117,3.435408,3.4404345,1.5393516,2.7874253,2.474262,1.7719675,4.263826,0.4480478,2.426688,2.6666708,2.5687523,2.7221773,3.347529,2.205035,1.1076539,1.8810905,1.6274514,2.9146316,2.6027238,2.8027103,3.0094962,2.8717475,3.4751725,2.9888306,1.3478801,1.588025,0.077645175,1.5495262,1.6000838,3.1624737,3.321806,5.4331126,0.69958836,-0.73200244,1.3363643,-0.69135094,5.624961,5.4567394,5.8920603,1.2718222,4.6529107,3.3885114,2.8411257,1.2664549,1.0489063,2.5833333,4.8090477,6.086376,5.5969334,2.0291772,2.733407,2.1832144,2.3209338,2.742142,2.451839,2.3762646,2.3699603,1.7002002,1.0249623,4.199527,3.2500715,3.2553575,1.1022372,6.346275,4.5447416,1.7719276,3.612407,2.4890115,0.67095894,2.0016105,1.046322,3.5487578,4.248,3.4745238,3.367325,5.1334844,2.1857836,1.9421216,1.9208044,1.0988053,2.048632,3.2799015,3.3142698,1.2763399,4.242226,4.093433,1.5593165,1.539337,3.4519353,3.4634063,3.5755446,2.9299335,3.810611,3.7978735,2.8781905,2.9821901,3.0500486,3.6237648,5.63871,6.39578,1.3926187,1.2644144,1.2652044,1.286869,1.0896696,1.5102108,1.499594,1.5316225,1.2424681,1.5104152,1.009357,5.6789975,3.6360278,1.5838221,5.9859223,5.415544,5.779011,5.526578,6.802936,5.7377634,4.5908737,3.685119,5.280314,5.590004,6.6093254,5.4916763,6.0498414,6.2229896,3.994961,4.750249,5.3751473,5.7228565,6.127765,5.5168767,5.9932604,5.285295,5.724165,6.8560667,5.8528566,6.8017535,7.0122023,5.62182,6.5222015,6.9013886,2.6442113,6.986702,5.409631,6.8929367,5.7297163,6.66965,5.2093053,6.372627,5.4667974,5.596629,6.861378,6.430335,6.3307457,6.105537,5.43028,4.83471,5.667574,5.384726,4.978211,6.320123,6.1798687,5.6018944,5.263979,4.920869,4.6815877,5.800996,7.0669036,4.6596384,2.1968198,2.707873,5.314682,5.0750933,6.2522645,5.894956,4.843263,5.480191,5.868624,5.7787805,4.8001637,6.5652285,6.379112,7.05121,6.728693,6.2465005,6.550195,6.0030494,5.4085946,4.6144085,3.6281285,6.4062715,0.9417393,2.964179,1.177013,3.250967,3.509882,5.6918983,1.1994011,1.2394247,1.039507,1.9981482,0.93552035,0.897767,0.8694055,1.1303251,1.6345977,3.0804224,-0.33493567,2.591155,1.1536641,2.85707,2.7266967,-0.61650026,2.4358292,-0.57618296,0.9077103,1.8221993,1.7791634,3.1995974,-0.60539967,3.2266023,0.96415895,6.0119724,6.1237464,6.3549376,1.7956539,1.7884183,2.58853,3.1076822,1.3453476,2.568352,0.40146843,0.38447338,0.35690057,0.86669457,1.0368863,0.45719555,1.5215083,0.5955562,3.374986,3.0276768,3.2264833,3.2944598,3.517221,1.28106,3.6106055,1.5110991,2.0399752,2.1160376,3.643127,3.1111162,2.8784502,2.2980149,3.533009,1.5536321,2.6620111,2.7270646,3.207562,2.6192546,2.5848532,3.6613712,3.7249324,1.9001629,3.1463704,3.390588,2.7939672,3.4435613,3.5000906,1.3095369,2.5949924,2.6505432],\"y\":[5.631356,6.3855743,6.19007,6.6685767,6.9465404,6.958422,7.4157667,7.0224333,6.981039,5.4054184,5.9446616,5.6118107,6.246667,3.949304,6.507305,4.1582117,5.556969,6.873834,6.7992573,5.569264,5.408563,5.7114935,5.588109,3.7850096,4.013313,4.733913,5.3439198,4.10923,6.8709044,5.367771,5.335884,5.5705476,4.1391525,4.360586,3.9745467,4.4028463,5.7170324,5.482933,5.5886116,5.574605,4.8398647,6.148655,5.856585,5.088962,5.3788643,6.3805017,4.448108,5.5438743,5.6350513,4.2606025,5.67949,3.9987352,5.750781,3.7730644,3.6848764,3.6747694,3.6528862,3.8513348,7.3282094,5.5198836,6.630788,5.8812685,8.259637,8.320213,6.480857,7.1207795,5.96315,3.9615548,3.9869735,5.280477,5.543113,7.3648715,5.464124,6.7784376,4.850018,3.24689,8.837676,3.4298618,8.586334,7.606988,7.634493,8.8736315,7.783554,6.047938,8.875458,9.096192,8.506387,8.735352,9.248197,3.3451247,4.186961,4.199299,3.14004,3.4051766,3.446027,3.2568247,3.263872,3.3992298,3.298712,3.4794793,3.6502054,3.2742789,3.2039557,3.2980266,3.7728434,3.4475179,3.800736,3.2131474,5.228587,5.118302,5.5170207,5.9442406,8.701098,8.306786,9.339785,9.373669,7.6328745,7.5944505,7.596474,7.6573057,7.6522694,8.711549,8.681561,8.776401,9.183939,6.3607183,9.519199,7.820033,8.562115,8.990291,9.543867,8.981859,8.919427,8.975507,8.977384,8.676827,8.9708805,9.188352,8.98262,8.958036,9.463988,8.321503,7.4269304,8.096784,8.67263,9.367487,8.878752,8.942838,8.65817,8.468514,8.292801,8.907651,8.872386,8.473357,8.817516,9.582705,8.504689,8.619688,4.7178564,6.410591,6.379648,4.6965995,7.729536,7.8627677,7.0933857,7.190431,5.469369,5.796889,7.513017,7.511224,5.2571745,5.1577024,4.4345193,4.4543796,4.7435975,6.130148,5.0829887,5.6031084,4.6853275,4.82594,4.6426663,4.5230217,7.593933,7.540039,6.968928,6.3463764,8.13886,6.9063625,7.286349,7.147222,6.486513,6.895167,7.8606,6.9880185,6.907093,6.011961,6.718334,7.031431,7.0213614,7.0894513,7.727969,7.4214416,7.3184466,8.003321,6.1541877,6.865877,7.603844,7.670861,7.757605,7.2334065,7.369742,4.6073084,7.9004116,7.9646473,6.1782913,7.3396845,8.847228,6.998262,6.911678,6.270438,7.4251256,5.604115,5.315252,5.251929,6.711703,8.417414,5.575988,7.490372,4.9291267,5.4680834,4.8154755,4.9259543,5.3737545,4.8785515,5.5587025,3.986453,3.9837615,5.5021677,6.4940267,5.5167184,7.5362463,5.6761894,7.6010995,7.068627,3.6680763,3.627251,3.6949556,4.248265,4.660692,7.2550797,4.7812057,5.3932843,5.0242395,4.897191,4.8185906,4.570175,4.334226,5.1769147,4.297608,5.7061343,7.8903246,8.538729,8.05647,8.0954,4.2043805,5.3132625,7.9988866,7.0731406,9.615474,5.318421,9.557448,8.221569,8.032826,3.0855277,5.649372,6.3072386,4.223668,5.5153785,5.274838,5.3846607,5.8197885,6.5451555,4.368603,8.200048,4.410573,4.3485208,5.896638,5.854591,4.607909,4.868108,5.102589,4.282432,5.10367,5.3865333,6.3012905,5.975948,5.4694414,5.362009,3.6357346,6.1293154,4.9737496,6.7853413,5.11134,6.7492566,4.5730343,5.7521353,6.7187357,5.4897075,3.8752058,3.874288,5.29065,4.4971633,4.48515,4.82994,7.200268,4.3242784,3.8236763,3.8973143,5.221296,6.324513,6.3246784,5.6349154,5.476361,3.824498,3.8268232,3.9253776,4.3147326,5.215443,3.9980702,4.252205,4.3742027,4.430251,5.1775107,8.229855,3.7644355,5.7275405,5.6404076,5.701491,5.6941724,5.4756594,5.461679,5.489318,5.441674,5.268981,5.6709332,5.3885555,8.279474,6.8042665,7.648979,3.6406415,3.7234275,3.6249864,3.95592,3.5937486,8.334132,6.259629,6.7573833,4.1946087,8.173994,3.637176,8.062469,4.445091,4.2202644,6.1586785,4.1098704,4.263265,4.0831575,4.1467233,4.499368,3.9349415,4.473829,4.201471,3.2405486,4.3244157,3.8198125,3.6869044,4.5485535,4.1109695,3.8538814,6.7540426,3.5717564,7.931646,3.2869232,8.326118,3.8347297,3.8288631,3.383467,3.7427552,3.7973602,3.2404158,3.9046104,4.0519547,3.7514307,4.2435837,4.049206,3.946662,4.102854,4.2922125,3.8630328,3.6656017,3.9145296,4.04826,4.078801,4.018576,4.6670876,3.7080142,6.2299232,4.6291304,6.2171035,3.9500875,4.0533834,3.8609438,3.4790168,4.0048842,3.8989565,4.6345725,4.6034627,3.9710078,3.9741158,4.069631,3.7214463,3.6697466,4.0649633,3.735389,4.3490977,5.0552516,4.0708437,6.1327815,3.8113675,5.5870113,6.085675,5.7056174,7.0692945,6.8282986,4.9313188,5.741597,5.376025,5.377446,6.6949973,5.581029,5.606971,5.662995,5.387168,4.401536,7.0126762,8.981945,7.0168815,5.31649,6.9503684,7.3075304,9.411318,6.7991858,9.626831,7.1952376,4.5037766,4.737111,4.8573475,9.590493,6.8364325,7.623087,3.2705803,3.194401,3.387409,4.284925,4.3550615,4.6032157,7.124465,5.082313,6.428931,7.1289363,7.0075107,7.0376015,8.657453,8.626404,7.2073464,7.6258426,7.5412607,4.632761,3.9919891,4.0349436,7.105459,5.203869,5.3091865,4.366527,5.24328,4.535887,4.547343,4.225662,4.621583,4.3424163,4.903838,5.367071,5.4005895,4.0526233,4.028475,4.7641344,6.855131,3.9562187,4.3321686,4.3448358,4.454674,4.389905,4.4684176,4.5141015,4.291552,4.303883,7.4091663,6.403949,5.6811843],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"hovertemplate\":\"bbox_name:%{text}\\u003cbr\\u003eumap_y=%{x}\\u003cbr\\u003eumap_z=%{y}\",\"marker\":{\"color\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"showscale\":false,\"size\":5},\"mode\":\"markers\",\"name\":\"(y vs z)\",\"text\":[\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\",\"bbox7\"],\"x\":[2.8237913,1.3639009,1.4746708,1.782302,2.780227,2.7588482,2.5451133,0.9149695,2.336175,1.474145,1.5371263,1.4629123,1.3355843,1.8233283,1.7744348,3.0366395,3.2409792,2.412801,2.4971313,2.9273474,2.524806,2.7466176,2.9953718,1.6864046,2.8750482,1.7544695,-0.5852801,2.2179105,2.1229663,1.8729151,-0.50735426,3.2971678,0.6319819,3.039042,2.8103507,1.6000838,2.6498215,3.3816595,3.3792968,3.3377938,2.8909895,2.8212097,2.8312333,3.3156557,1.4707342,1.2890917,2.7378936,2.206582,2.469945,1.570091,2.7683372,1.9187359,2.57266,1.7678288,1.6573162,1.6718076,1.6728762,1.6256465,1.8505154,1.4626622,1.6144924,0.9546825,0.7600041,0.73134005,1.3810247,2.2993941,2.2108428,0.2746233,-0.37739682,3.223761,1.9617682,1.6937413,1.756045,1.6877652,1.6376649,-0.19830045,2.3780408,-0.20773582,3.0262082,2.4759626,2.6239746,2.7593174,2.560442,-0.094971694,2.496995,2.4141173,2.208344,2.819719,2.6770082,-0.3192255,0.4496833,0.8965682,-0.20094536,-0.44899496,-0.20639372,-0.12175842,-0.072279364,-0.113431275,-0.31900525,-0.09841616,-0.048884016,-0.15589328,-0.17622449,-0.08042106,-0.51107466,-0.1626262,0.08827878,-0.22535992,0.95604205,0.89804924,1.140321,1.8922007,2.6804514,0.7538889,2.775328,2.8684218,2.5673215,2.5777886,2.317145,2.64408,2.63956,2.5870767,2.5726695,2.6109257,2.6793115,0.26362512,2.8597262,2.5457113,2.6257532,2.5810955,2.7725341,2.92764,2.4546933,2.7200367,2.717126,2.56559,2.8730805,2.6555693,2.3736036,2.4186568,2.9044974,2.6553922,2.0573797,2.1072257,2.6206095,2.8920927,2.755993,2.6816428,2.740539,2.8260748,2.1172311,2.5680757,2.7676728,2.2157197,2.3354328,2.8899047,2.7791095,2.9622035,2.8673117,1.878824,1.2108725,2.8885508,1.9379443,1.4206247,1.093815,1.7522157,0.036317006,0.57623106,1.3492024,1.6604799,1.4568688,3.2340093,1.4421904,1.2057422,0.86688465,0.9826603,2.309547,3.393512,2.9592986,2.5191245,2.925057,0.73982376,1.8679345,2.1006682,1.8798236,1.2622542,1.7942501,1.6336757,1.4473742,1.7676128,1.1311151,2.0205836,2.0119507,2.1197307,2.3667762,0.8504343,2.0785446,2.48844,2.2153924,2.021058,1.7614807,1.6400557,1.6484245,1.9963715,0.78526956,2.3758576,1.5516102,1.557142,2.1463332,1.899385,1.6533997,0.6845402,1.6838056,1.8137416,1.207868,1.665228,2.783216,2.378056,2.5087621,1.170352,2.06553,2.3769598,-0.45451063,-0.23604158,1.8965014,2.223792,2.3292158,1.9429204,1.5272801,2.59867,1.5142825,1.676482,1.7339458,1.8354064,2.2507718,2.4774559,2.470766,-0.16611609,1.3194376,2.687437,1.9834803,2.3112764,2.346658,1.8064451,1.9593651,1.7804286,1.644105,0.8381123,1.9657755,2.2183635,0.8639911,-0.6780791,1.4616764,1.1923044,2.5441966,2.5987163,2.2711046,1.4708607,2.5405788,1.5690958,1.763543,2.3599463,1.9034739,1.8836085,2.175861,1.5916611,0.8650379,2.231238,2.7242582,-0.37169293,2.7506557,0.81019735,0.8554583,-0.1500628,-0.7819022,1.403428,1.338938,1.8200352,-0.46477395,-0.45858407,1.7272884,1.2752345,0.5706823,0.8071125,1.2290692,2.2661664,2.2932515,2.5714712,2.3162827,2.2011862,2.3303058,1.959129,0.38610074,-0.4565882,1.8182478,1.8995968,1.6919334,-0.36942023,-0.11044071,1.6397489,1.8763597,2.2066865,1.9917114,2.3638558,2.941332,-0.10592871,2.3767984,2.1037362,2.1792824,1.8280481,1.0966597,2.058573,1.8421589,1.6257248,1.3213332,2.0070662,1.7456342,1.5891545,-0.5042989,1.7283028,1.8178942,-0.5125551,-0.68830836,2.146175,2.194834,2.3275955,2.602955,1.4256604,2.2759402,2.6198912,2.3661797,2.5708811,1.4346502,0.8000823,-0.0921387,-0.6752754,-0.7868431,-0.7498604,-0.7915217,-0.53851134,-0.66053635,-0.6893344,-0.6695493,-0.510769,-0.30617675,-0.39673364,0.76823175,1.7733928,2.465135,-0.5668483,-0.42737156,-0.5586049,0.013952165,-0.11753085,0.719939,1.3284833,2.2474885,0.37762794,0.8275291,-0.25966492,0.8121925,0.3549786,0.23237447,1.0725019,0.3910105,0.31599763,-0.0007787129,0.17751503,0.54128516,-0.10552322,0.6491415,0.18183054,0.033324655,0.30077267,0.013822259,-0.08889604,0.47475058,-0.10737561,-0.023278726,1.4075,-0.1589724,0.86186355,0.031846926,0.7168552,-0.35996538,-0.3461831,-0.33362108,-0.41709983,-0.5436341,-0.0046823216,-0.50589657,-0.37664905,-0.5998616,0.22977115,0.3244986,-0.30329934,0.11234774,0.73879725,-0.39914164,-0.40192616,-0.30172274,0.1024794,0.3004911,0.30188137,0.5124636,-0.057510864,1.3307183,0.70159197,1.237959,-0.06998092,0.11100433,-0.48673597,-0.35405228,0.23498182,-0.17172115,0.44278276,0.49350867,0.21365432,-0.117739394,-0.10017654,-0.17490266,-0.029555049,0.15598282,-0.07292856,0.34204867,0.86244816,0.31755158,0.87327856,-0.50374705,1.7395099,1.1485902,1.6238654,2.564164,2.2895153,0.47830912,1.5803096,1.3457419,1.8946263,1.5441788,1.7491943,1.7986783,1.7929367,1.6097921,1.5350264,2.7655232,2.393841,1.4454954,1.8680328,1.7700044,2.40631,2.5620627,1.4865438,2.823127,1.7521359,1.6055545,2.2639105,1.5558243,2.8428748,2.4025607,2.6016634,0.08657857,-0.03363896,0.10441889,1.5030893,1.3873515,2.8683736,2.5455105,1.9564257,1.1175891,1.7585535,1.9870793,2.0647984,2.2500038,2.2690957,1.7659074,2.0623543,1.7849313,1.4014297,2.7090435,2.6968987,2.4520009,1.4392744,1.9294903,1.3573451,1.7464308,0.8900724,0.75009906,2.553729,2.6477458,2.0699527,1.9998578,1.5123222,1.8766506,2.9691079,2.9130638,2.6790307,1.4524256,2.8586793,1.344949,1.3325446,0.6955375,1.0758376,1.2062148,0.9168878,1.1434642,1.1200206,1.7606623,1.0899945,2.750742],\"y\":[5.631356,6.3855743,6.19007,6.6685767,6.9465404,6.958422,7.4157667,7.0224333,6.981039,5.4054184,5.9446616,5.6118107,6.246667,3.949304,6.507305,4.1582117,5.556969,6.873834,6.7992573,5.569264,5.408563,5.7114935,5.588109,3.7850096,4.013313,4.733913,5.3439198,4.10923,6.8709044,5.367771,5.335884,5.5705476,4.1391525,4.360586,3.9745467,4.4028463,5.7170324,5.482933,5.5886116,5.574605,4.8398647,6.148655,5.856585,5.088962,5.3788643,6.3805017,4.448108,5.5438743,5.6350513,4.2606025,5.67949,3.9987352,5.750781,3.7730644,3.6848764,3.6747694,3.6528862,3.8513348,7.3282094,5.5198836,6.630788,5.8812685,8.259637,8.320213,6.480857,7.1207795,5.96315,3.9615548,3.9869735,5.280477,5.543113,7.3648715,5.464124,6.7784376,4.850018,3.24689,8.837676,3.4298618,8.586334,7.606988,7.634493,8.8736315,7.783554,6.047938,8.875458,9.096192,8.506387,8.735352,9.248197,3.3451247,4.186961,4.199299,3.14004,3.4051766,3.446027,3.2568247,3.263872,3.3992298,3.298712,3.4794793,3.6502054,3.2742789,3.2039557,3.2980266,3.7728434,3.4475179,3.800736,3.2131474,5.228587,5.118302,5.5170207,5.9442406,8.701098,8.306786,9.339785,9.373669,7.6328745,7.5944505,7.596474,7.6573057,7.6522694,8.711549,8.681561,8.776401,9.183939,6.3607183,9.519199,7.820033,8.562115,8.990291,9.543867,8.981859,8.919427,8.975507,8.977384,8.676827,8.9708805,9.188352,8.98262,8.958036,9.463988,8.321503,7.4269304,8.096784,8.67263,9.367487,8.878752,8.942838,8.65817,8.468514,8.292801,8.907651,8.872386,8.473357,8.817516,9.582705,8.504689,8.619688,4.7178564,6.410591,6.379648,4.6965995,7.729536,7.8627677,7.0933857,7.190431,5.469369,5.796889,7.513017,7.511224,5.2571745,5.1577024,4.4345193,4.4543796,4.7435975,6.130148,5.0829887,5.6031084,4.6853275,4.82594,4.6426663,4.5230217,7.593933,7.540039,6.968928,6.3463764,8.13886,6.9063625,7.286349,7.147222,6.486513,6.895167,7.8606,6.9880185,6.907093,6.011961,6.718334,7.031431,7.0213614,7.0894513,7.727969,7.4214416,7.3184466,8.003321,6.1541877,6.865877,7.603844,7.670861,7.757605,7.2334065,7.369742,4.6073084,7.9004116,7.9646473,6.1782913,7.3396845,8.847228,6.998262,6.911678,6.270438,7.4251256,5.604115,5.315252,5.251929,6.711703,8.417414,5.575988,7.490372,4.9291267,5.4680834,4.8154755,4.9259543,5.3737545,4.8785515,5.5587025,3.986453,3.9837615,5.5021677,6.4940267,5.5167184,7.5362463,5.6761894,7.6010995,7.068627,3.6680763,3.627251,3.6949556,4.248265,4.660692,7.2550797,4.7812057,5.3932843,5.0242395,4.897191,4.8185906,4.570175,4.334226,5.1769147,4.297608,5.7061343,7.8903246,8.538729,8.05647,8.0954,4.2043805,5.3132625,7.9988866,7.0731406,9.615474,5.318421,9.557448,8.221569,8.032826,3.0855277,5.649372,6.3072386,4.223668,5.5153785,5.274838,5.3846607,5.8197885,6.5451555,4.368603,8.200048,4.410573,4.3485208,5.896638,5.854591,4.607909,4.868108,5.102589,4.282432,5.10367,5.3865333,6.3012905,5.975948,5.4694414,5.362009,3.6357346,6.1293154,4.9737496,6.7853413,5.11134,6.7492566,4.5730343,5.7521353,6.7187357,5.4897075,3.8752058,3.874288,5.29065,4.4971633,4.48515,4.82994,7.200268,4.3242784,3.8236763,3.8973143,5.221296,6.324513,6.3246784,5.6349154,5.476361,3.824498,3.8268232,3.9253776,4.3147326,5.215443,3.9980702,4.252205,4.3742027,4.430251,5.1775107,8.229855,3.7644355,5.7275405,5.6404076,5.701491,5.6941724,5.4756594,5.461679,5.489318,5.441674,5.268981,5.6709332,5.3885555,8.279474,6.8042665,7.648979,3.6406415,3.7234275,3.6249864,3.95592,3.5937486,8.334132,6.259629,6.7573833,4.1946087,8.173994,3.637176,8.062469,4.445091,4.2202644,6.1586785,4.1098704,4.263265,4.0831575,4.1467233,4.499368,3.9349415,4.473829,4.201471,3.2405486,4.3244157,3.8198125,3.6869044,4.5485535,4.1109695,3.8538814,6.7540426,3.5717564,7.931646,3.2869232,8.326118,3.8347297,3.8288631,3.383467,3.7427552,3.7973602,3.2404158,3.9046104,4.0519547,3.7514307,4.2435837,4.049206,3.946662,4.102854,4.2922125,3.8630328,3.6656017,3.9145296,4.04826,4.078801,4.018576,4.6670876,3.7080142,6.2299232,4.6291304,6.2171035,3.9500875,4.0533834,3.8609438,3.4790168,4.0048842,3.8989565,4.6345725,4.6034627,3.9710078,3.9741158,4.069631,3.7214463,3.6697466,4.0649633,3.735389,4.3490977,5.0552516,4.0708437,6.1327815,3.8113675,5.5870113,6.085675,5.7056174,7.0692945,6.8282986,4.9313188,5.741597,5.376025,5.377446,6.6949973,5.581029,5.606971,5.662995,5.387168,4.401536,7.0126762,8.981945,7.0168815,5.31649,6.9503684,7.3075304,9.411318,6.7991858,9.626831,7.1952376,4.5037766,4.737111,4.8573475,9.590493,6.8364325,7.623087,3.2705803,3.194401,3.387409,4.284925,4.3550615,4.6032157,7.124465,5.082313,6.428931,7.1289363,7.0075107,7.0376015,8.657453,8.626404,7.2073464,7.6258426,7.5412607,4.632761,3.9919891,4.0349436,7.105459,5.203869,5.3091865,4.366527,5.24328,4.535887,4.547343,4.225662,4.621583,4.3424163,4.903838,5.367071,5.4005895,4.0526233,4.028475,4.7641344,6.855131,3.9562187,4.3321686,4.3448358,4.454674,4.389905,4.4684176,4.5141015,4.291552,4.303883,7.4091663,6.403949,5.6811843],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.2888888888888889]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.35555555555555557,0.6444444444444445]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.7111111111111111,1.0]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,1.0]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"UMAP (x vs y)\",\"x\":0.14444444444444446,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"UMAP (x vs z)\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"UMAP (y vs z)\",\"x\":0.8555555555555556,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"2D UMAP Projections (All Pairwise Components)\"},\"width\":1800,\"height\":600,\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7943f379-ceb0-412b-b17e-e78a9cfef8c7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensionality reduction and visualization completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradcam\n"
      ],
      "metadata": {
        "id": "2tAZZM8QJUqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir videomae_gradcam\n"
      ],
      "metadata": {
        "id": "J01StSGvJW5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_videomae.synapse_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "nsGSqvEVJorv",
        "outputId": "9bf8ac95-d88e-401e-b621-c435a0684876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     Var1  central_coord_1  central_coord_2  central_coord_3  \\\n",
              "0  non_spine_synapsed_056              171              260              350   \n",
              "1   non_spine_synapse_057              223              113              425   \n",
              "2   non_spine_synapse_058              280              102              377   \n",
              "3   non_spine_synapse_063              455              131              162   \n",
              "4   non_spine_synapse_062              138              121              302   \n",
              "\n",
              "   side_1_coord_1  side_1_coord_2  side_1_coord_3  side_2_coord_1  \\\n",
              "0             171             268             359             171   \n",
              "1             223             112             438             223   \n",
              "2             280              94             400             280   \n",
              "3             455             134             181             455   \n",
              "4             135             113             298             140   \n",
              "\n",
              "   side_2_coord_2  side_2_coord_3  bbox_index bbox_name  \n",
              "0             260             340           0     bbox1  \n",
              "1             114             407           0     bbox1  \n",
              "2             108             364           0     bbox1  \n",
              "3             127             145           0     bbox1  \n",
              "4             127             312           0     bbox1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b63bd6c7-b48c-4031-88c8-728959dbad70\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Var1</th>\n",
              "      <th>central_coord_1</th>\n",
              "      <th>central_coord_2</th>\n",
              "      <th>central_coord_3</th>\n",
              "      <th>side_1_coord_1</th>\n",
              "      <th>side_1_coord_2</th>\n",
              "      <th>side_1_coord_3</th>\n",
              "      <th>side_2_coord_1</th>\n",
              "      <th>side_2_coord_2</th>\n",
              "      <th>side_2_coord_3</th>\n",
              "      <th>bbox_index</th>\n",
              "      <th>bbox_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>non_spine_synapsed_056</td>\n",
              "      <td>171</td>\n",
              "      <td>260</td>\n",
              "      <td>350</td>\n",
              "      <td>171</td>\n",
              "      <td>268</td>\n",
              "      <td>359</td>\n",
              "      <td>171</td>\n",
              "      <td>260</td>\n",
              "      <td>340</td>\n",
              "      <td>0</td>\n",
              "      <td>bbox1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>non_spine_synapse_057</td>\n",
              "      <td>223</td>\n",
              "      <td>113</td>\n",
              "      <td>425</td>\n",
              "      <td>223</td>\n",
              "      <td>112</td>\n",
              "      <td>438</td>\n",
              "      <td>223</td>\n",
              "      <td>114</td>\n",
              "      <td>407</td>\n",
              "      <td>0</td>\n",
              "      <td>bbox1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>non_spine_synapse_058</td>\n",
              "      <td>280</td>\n",
              "      <td>102</td>\n",
              "      <td>377</td>\n",
              "      <td>280</td>\n",
              "      <td>94</td>\n",
              "      <td>400</td>\n",
              "      <td>280</td>\n",
              "      <td>108</td>\n",
              "      <td>364</td>\n",
              "      <td>0</td>\n",
              "      <td>bbox1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>non_spine_synapse_063</td>\n",
              "      <td>455</td>\n",
              "      <td>131</td>\n",
              "      <td>162</td>\n",
              "      <td>455</td>\n",
              "      <td>134</td>\n",
              "      <td>181</td>\n",
              "      <td>455</td>\n",
              "      <td>127</td>\n",
              "      <td>145</td>\n",
              "      <td>0</td>\n",
              "      <td>bbox1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>non_spine_synapse_062</td>\n",
              "      <td>138</td>\n",
              "      <td>121</td>\n",
              "      <td>302</td>\n",
              "      <td>135</td>\n",
              "      <td>113</td>\n",
              "      <td>298</td>\n",
              "      <td>140</td>\n",
              "      <td>127</td>\n",
              "      <td>312</td>\n",
              "      <td>0</td>\n",
              "      <td>bbox1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b63bd6c7-b48c-4031-88c8-728959dbad70')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b63bd6c7-b48c-4031-88c8-728959dbad70 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b63bd6c7-b48c-4031-88c8-728959dbad70');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-02fd1f27-185a-4d2b-b05f-aff3b3d0863c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-02fd1f27-185a-4d2b-b05f-aff3b3d0863c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-02fd1f27-185a-4d2b-b05f-aff3b3d0863c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"dataset_videomae\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Var1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"non_spine_synapse_057\",\n          \"non_spine_synapse_062\",\n          \"non_spine_synapse_058\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"central_coord_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 124,\n        \"min\": 138,\n        \"max\": 455,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          223,\n          138,\n          280\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"central_coord_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 64,\n        \"min\": 102,\n        \"max\": 260,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          113,\n          121,\n          102\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"central_coord_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 100,\n        \"min\": 162,\n        \"max\": 425,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          425,\n          302,\n          377\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"side_1_coord_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 125,\n        \"min\": 135,\n        \"max\": 455,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          223,\n          135,\n          280\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"side_1_coord_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 70,\n        \"min\": 94,\n        \"max\": 268,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          112,\n          113,\n          94\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"side_1_coord_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 100,\n        \"min\": 181,\n        \"max\": 438,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          438,\n          298,\n          400\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"side_2_coord_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 124,\n        \"min\": 140,\n        \"max\": 455,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          223,\n          140,\n          280\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"side_2_coord_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 63,\n        \"min\": 108,\n        \"max\": 260,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          114,\n          127,\n          260\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"side_2_coord_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 100,\n        \"min\": 145,\n        \"max\": 407,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          407,\n          312,\n          364\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bbox_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bbox_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"bbox1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from typing import List, Optional\n",
        "\n",
        "class VideoMAEGradCAM:\n",
        "    \"\"\"\n",
        "    GradCAM implementation for VideoMAE model.\n",
        "    Generates attention maps for video input.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: torch.nn.Module, layer_idx: int = 11):\n",
        "        torch.backends.cudnn.enabled = False  # Temporary fix for some CUDA issues\n",
        "        self.model = model\n",
        "        self.device = next(model.parameters()).device\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        # Register hooks for the attention output\n",
        "        target_layer = self.model.encoder.layer[layer_idx].attention.output.dense\n",
        "        self.forward_hook = target_layer.register_forward_hook(self._save_activation)\n",
        "        self.backward_hook = target_layer.register_backward_hook(self._save_gradient)\n",
        "\n",
        "    def _save_activation(self, module, input, output):\n",
        "        self.activations = output\n",
        "\n",
        "    def __del__(self):\n",
        "        # Remove hooks when the object is deleted\n",
        "        self.forward_hook.remove()\n",
        "        self.backward_hook.remove()\n",
        "\n",
        "    def _save_gradient(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0]\n",
        "\n",
        "    def generate_cam(self, video_input: torch.Tensor) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate attention map for video input.\n",
        "        \"\"\"\n",
        "        # Ensure input is on the same device as model\n",
        "        video_input = video_input.to(self.device)\n",
        "\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = self.model(pixel_values=video_input)\n",
        "\n",
        "        # Use mean of output features as target for visualization\n",
        "        target = outputs.last_hidden_state.mean()\n",
        "        target.backward()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get gradients and activations\n",
        "            gradients = self.gradients.detach()\n",
        "            activations = self.activations.detach()\n",
        "\n",
        "            # Calculate importance weights\n",
        "            weights = torch.mean(gradients, dim=(0, 1))\n",
        "\n",
        "            # Weight the activations by the gradients\n",
        "            weighted_activations = torch.einsum('ntd,d->nt', activations, weights)\n",
        "\n",
        "            # Get video dimensions\n",
        "            num_frames = video_input.size(1)  # Number of input frames\n",
        "            patch_size = self.model.config.patch_size\n",
        "            tubelet_size = self.model.config.tubelet_size\n",
        "            image_size = video_input.size(-1)  # Height/width of input frames\n",
        "\n",
        "            # Calculate patches\n",
        "            patches_per_frame = (image_size // patch_size) ** 2  # Spatial patches per frame\n",
        "            num_total_patches = weighted_activations.size(1)  # Total patches in sequence\n",
        "            temporal_patches = num_frames // tubelet_size  # Number of temporal patches\n",
        "\n",
        "            # First reshape to (temporal_patches, spatial_patches)\n",
        "            attention_map = weighted_activations.view(temporal_patches, patches_per_frame)\n",
        "\n",
        "            # Then reshape spatial dimension to square grid\n",
        "            spatial_size = int(np.sqrt(patches_per_frame))\n",
        "            attention_map = attention_map.view(temporal_patches, spatial_size, spatial_size)\n",
        "\n",
        "            # Upsample temporal dimension to match number of frames\n",
        "            attention_map = attention_map.unsqueeze(1)\n",
        "            attention_map = attention_map.repeat(1, tubelet_size, 1, 1)\n",
        "            attention_map = attention_map.reshape(num_frames, spatial_size, spatial_size)\n",
        "\n",
        "            # Apply ReLU and normalize\n",
        "            attention_map = torch.relu(attention_map)\n",
        "            if attention_map.max() > attention_map.min():\n",
        "                attention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min())\n",
        "\n",
        "            return attention_map.cpu().numpy()\n",
        "\n",
        "    def apply_attention_map(self,\n",
        "                          video_frames: List[np.ndarray],\n",
        "                          attention_map: np.ndarray,\n",
        "                          alpha: float = 0.6) -> List[np.ndarray]:\n",
        "        \"\"\"\n",
        "        Apply attention map to original video frames.\n",
        "        \"\"\"\n",
        "        result_frames = []\n",
        "\n",
        "        for frame_idx, frame in enumerate(video_frames):\n",
        "            # Resize attention map to match frame size\n",
        "            attention = cv2.resize(attention_map[frame_idx],\n",
        "                                 (frame.shape[1], frame.shape[0]),\n",
        "                                 interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "            # Create heatmap\n",
        "            heatmap = cv2.applyColorMap(\n",
        "                (attention * 255).astype(np.uint8),\n",
        "                cv2.COLORMAP_JET\n",
        "            )\n",
        "            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Overlay heatmap on frame\n",
        "            superimposed = cv2.addWeighted(frame, 1-alpha, heatmap, alpha, 0)\n",
        "            result_frames.append(superimposed)\n",
        "\n",
        "        return result_frames\n",
        "\n",
        "    def visualize(self,\n",
        "                 video_input: torch.Tensor,\n",
        "                 original_frames: List[np.ndarray],\n",
        "                 output_path: Optional[str] = None,\n",
        "                 alpha: float = 0.6,\n",
        "                 fps: int = 4) -> List[np.ndarray]:\n",
        "        \"\"\"\n",
        "        Generate and save complete visualization.\n",
        "        \"\"\"\n",
        "        # Generate attention map\n",
        "        attention_map = self.generate_cam(video_input)\n",
        "\n",
        "        # Apply attention map to frames\n",
        "        visualization_frames = self.apply_attention_map(\n",
        "            original_frames,\n",
        "            attention_map,\n",
        "            alpha\n",
        "        )\n",
        "\n",
        "        # Save if output path provided\n",
        "        if output_path is not None:\n",
        "            if output_path.endswith('.gif'):\n",
        "                # Save as GIF\n",
        "                pil_frames = [Image.fromarray(frame) for frame in visualization_frames]\n",
        "                pil_frames[0].save(\n",
        "                    output_path,\n",
        "                    save_all=True,\n",
        "                    append_images=pil_frames[1:],\n",
        "                    duration=int(1000/fps),\n",
        "                    loop=0\n",
        "                )\n",
        "            elif output_path.endswith('.mp4'):\n",
        "                # Save as MP4\n",
        "                height, width = visualization_frames[0].shape[:2]\n",
        "                fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "                out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "                for frame in visualization_frames:\n",
        "                    out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "                out.release()\n",
        "\n",
        "        return visualization_frames\n",
        "\n",
        "def visualize_synapse_attention(model, dataset, idx: int,\n",
        "                              layer_idx: int = 11,\n",
        "                              output_path: Optional[str] = None):\n",
        "    \"\"\"\n",
        "    Generate attention visualization for a specific synapse.\n",
        "    \"\"\"\n",
        "    # Get video data and move to model device\n",
        "    pixel_values, synapse_info = dataset[idx]\n",
        "    pixel_values = pixel_values.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Get original frames (on CPU)\n",
        "    original_frames = []\n",
        "    for frame_idx in range(pixel_values.size(1)):\n",
        "        frame = pixel_values[0, frame_idx].cpu().permute(1, 2, 0).numpy()\n",
        "        frame = (frame * 255).astype(np.uint8)\n",
        "        original_frames.append(frame)\n",
        "\n",
        "    # Initialize GradCAM\n",
        "    gradcam = VideoMAEGradCAM(model, layer_idx=layer_idx)\n",
        "\n",
        "    # Generate visualization\n",
        "    vis_frames = gradcam.visualize(\n",
        "        pixel_values,\n",
        "        original_frames,\n",
        "        output_path=output_path\n",
        "    )\n",
        "\n",
        "    return vis_frames, synapse_info\n",
        "\n",
        "\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Create output directory\n",
        "output_dir = \"gradcam_outpu2ts2\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Get 20 random indices\n",
        "n_samples = 2\n",
        "total_samples = len(dataset_videomae)\n",
        "random_indices = random.sample(range(total_samples), min(n_samples, total_samples))\n",
        "\n",
        "# Get the synapse DataFrame\n",
        "synapse_df = dataset_videomae.synapse_df\n",
        "\n",
        "# Generate visualizations\n",
        "for i, idx in enumerate(random_indices):\n",
        "    # Get Var1 value for this synapse\n",
        "    var1_value = synapse_df.iloc[idx]['Var1']\n",
        "\n",
        "    # Create output filename using Var1\n",
        "    output_path = os.path.join(output_dir, f\"{var1_value}_attention.gif\")\n",
        "\n",
        "    print(f\"Generating visualization {i+1}/20 for synapse {var1_value}\")\n",
        "\n",
        "    vis_frames = visualize_synapse_attention(\n",
        "        model_videomae_feature,\n",
        "        dataset_videomae,\n",
        "        idx,\n",
        "        layer_idx=11,\n",
        "        output_path=output_path\n",
        "    )\n",
        "\n",
        "    print(f\"Saved visualization to {output_path}\")"
      ],
      "metadata": {
        "id": "XlEaulEOKY5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e5c2cff-7c23-41ba-ab2c-32bbaf2a75f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating visualization 1/20 for synapse explorative_2024-08-03_Ali_Karimi_025\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Saved visualization to gradcam_outpu2ts2/explorative_2024-08-03_Ali_Karimi_025_attention.gif\n",
            "Generating visualization 2/20 for synapse non_spine_synapse_071\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Forward hook activated: Activations captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Backward hook activated: Gradients captured.\n",
            "Saved visualization to gradcam_outpu2ts2/non_spine_synapse_071_attention.gif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"/content/gradcam_outputs_archive\", 'zip', \"/content/gradcam_outputs\")\n",
        "\n",
        "print(\"Folder has been zipped to: /content/gradcam_outputs_archive.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouWHy8QoLjLB",
        "outputId": "f86b1a23-ffeb-4867-d350-03f23ae17593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder has been zipped to: /content/gradcam_outputs_archive.zip\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "daa58lj6b7qc",
        "HAeO_QEKZcF8"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alim98/AliMikaeili.github.io/blob/master/VGG_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Essential downloads"
      ],
      "metadata": {
        "id": "daa58lj6b7qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O downloaded_file.zip \"https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\"\n",
        "\n",
        "\n",
        "\n",
        "!wget -O vesicle_cloud__syn_interface__mitochondria_annotation.zip \"https://drive.usercontent.google.com/download?id=1qRibZL3kr7MQJQRgDFRquHMQlIGCN4XP&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\"\n",
        "\n",
        "!unzip -q downloaded_file.zip\n",
        "!unzip -q vesicle_cloud__syn_interface__mitochondria_annotation.zip"
      ],
      "metadata": {
        "id": "AIIIxarwTVT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b1b513-14cf-41d1-c10f-0f81f63ddc68"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-24 18:58:30--  https://drive.usercontent.google.com/download?id=1iHPBdBOPEagvPTHZmrN__LD49emXwReY&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.24.132, 2404:6800:4003:c03::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.24.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1264688649 (1.2G) [application/octet-stream]\n",
            "Saving to: ‘downloaded_file.zip’\n",
            "\n",
            "downloaded_file.zip 100%[===================>]   1.18G  51.8MB/s    in 22s     \n",
            "\n",
            "2025-01-24 18:58:54 (54.9 MB/s) - ‘downloaded_file.zip’ saved [1264688649/1264688649]\n",
            "\n",
            "--2025-01-24 18:58:54--  https://drive.usercontent.google.com/download?id=1qRibZL3kr7MQJQRgDFRquHMQlIGCN4XP&export=download&authuser=0&confirm=t&uuid=631d60dd-569c-4bb1-a9e8-d681f0ed3d43&at=APvzH3r4me8x_LwP3n8O7lgPo8oK%3A1733988188000\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.24.132, 2404:6800:4003:c03::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.24.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14246564 (14M) [application/octet-stream]\n",
            "Saving to: ‘vesicle_cloud__syn_interface__mitochondria_annotation.zip’\n",
            "\n",
            "vesicle_cloud__syn_ 100%[===================>]  13.59M  40.3MB/s    in 0.3s    \n",
            "\n",
            "2025-01-24 18:59:24 (40.3 MB/s) - ‘vesicle_cloud__syn_interface__mitochondria_annotation.zip’ saved [14246564/14246564]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install transformers scikit-learn matplotlib seaborn torch torchvision umap-learn git+https://github.com/funkelab/funlib.learn.torch.git\n",
        "!pip install openpyxl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZwyJMBQTXV3",
        "outputId": "40402215-8226-40c7-aeeb-464a22b7ba65"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/funkelab/funlib.learn.torch.git\n",
            "  Cloning https://github.com/funkelab/funlib.learn.torch.git to /tmp/pip-req-build-ao780g90\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/funkelab/funlib.learn.torch.git /tmp/pip-req-build-ao780g90\n",
            "  Resolved https://github.com/funkelab/funlib.learn.torch.git to commit 049729151c7a2c0320a446dc9d3244ac830f7ea8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu121)\n",
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: funlib.learn.torch\n",
            "  Building wheel for funlib.learn.torch (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funlib.learn.torch: filename=funlib.learn.torch-0.1.0-py3-none-any.whl size=13995 sha256=c9750311f6e3a38f37476702f22a8e521d0fe8cd0a60884da43a4904de8adc20\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mutattki/wheels/ae/7f/7b/ecbd355ccdfbd2bb0ab4f76ea45f60a02a572c88c1f4761e8d\n",
            "Successfully built funlib.learn.torch\n",
            "Installing collected packages: pynndescent, umap-learn, funlib.learn.torch\n",
            "Successfully installed funlib.learn.torch-0.1.0 pynndescent-0.5.13 umap-learn-0.5.7\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import glob\n",
        "import imageio.v2 as iio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch, Rectangle\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ViTImageProcessor, ViTModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "from umap import UMAP\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "HRS_MN6YTekw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG Example\n"
      ],
      "metadata": {
        "id": "fyxp7ZMWQgfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1Data"
      ],
      "metadata": {
        "id": "09dpZLkdZLwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just Uncomment either 1.1 or 1.2 for data loading"
      ],
      "metadata": {
        "id": "hRxENzoKXrZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Dark background (sectoins that have not segmentaions be 0)"
      ],
      "metadata": {
        "id": "lf_jtchGXcJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import io\n",
        "import argparse\n",
        "import multiprocessing\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio.v3 as iio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "class SimpleVideoProcessor:\n",
        "    def __init__(self, size=(80, 80), mean=(0.485,),  # Use a single channel for grayscale\n",
        "                 std=(0.229,)):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(size),\n",
        "            transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std),\n",
        "        ])\n",
        "\n",
        "    def __call__(self, frames, return_tensors=None):\n",
        "        processed_frames = [self.transform(frame) for frame in frames]\n",
        "        pixel_values = torch.stack(processed_frames)\n",
        "        if return_tensors == \"pt\":\n",
        "            return {\"pixel_values\": pixel_values}\n",
        "        else:\n",
        "            return pixel_values\n",
        "\n",
        "\n",
        "def load_volumes(bbox_name: str, raw_base_dir: str, seg_base_dir: str, add_mask_base_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "    seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "\n",
        "    if bbox_name.startswith(\"bbox\"):\n",
        "        bbox_num = bbox_name.replace(\"bbox\", \"\")\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, f\"bbox_{bbox_num}\")\n",
        "    else:\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, bbox_name)\n",
        "\n",
        "    raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "    seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "    add_mask_tif_files = sorted(glob.glob(os.path.join(add_mask_dir, 'slice_*.tif')))\n",
        "\n",
        "    if not (len(raw_tif_files) == len(seg_tif_files) == len(add_mask_tif_files)):\n",
        "        return None, None, None\n",
        "\n",
        "    try:\n",
        "        raw_vol = np.stack([iio.imread(f) for f in raw_tif_files], axis=0)\n",
        "        seg_vol = np.stack([iio.imread(f).astype(np.uint32) for f in seg_tif_files], axis=0)\n",
        "        add_mask_vol = np.stack([iio.imread(f).astype(np.uint32) for f in add_mask_tif_files], axis=0)\n",
        "        return raw_vol, seg_vol, add_mask_vol\n",
        "    except Exception as e:\n",
        "        return None, None, None\n",
        "\n",
        "def create_segmented_cube(\n",
        "    raw_vol: np.ndarray,\n",
        "    seg_vol: np.ndarray,\n",
        "    add_mask_vol: np.ndarray,\n",
        "    central_coord: Tuple[int, int, int],\n",
        "    side1_coord: Tuple[int, int, int],\n",
        "    side2_coord: Tuple[int, int, int],\n",
        "    segmentation_type: int,\n",
        "    subvolume_size: int = 80,\n",
        "    alpha: float = 0.3,\n",
        "    input_mask: bool = False  # New parameter\n",
        ") -> np.ndarray:\n",
        "    def create_segment_masks(segmentation_volume, s1_coord, s2_coord):\n",
        "        x1, y1, z1 = s1_coord\n",
        "        x2, y2, z2 = s2_coord\n",
        "        seg_id_1 = segmentation_volume[z1, y1, x1]\n",
        "        seg_id_2 = segmentation_volume[z2, y2, x2]\n",
        "\n",
        "        mask_1 = (segmentation_volume == seg_id_1) if seg_id_1 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        mask_2 = (segmentation_volume == seg_id_2) if seg_id_2 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        return mask_1, mask_2\n",
        "\n",
        "    mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "    mask_3_full = (add_mask_vol > 0)\n",
        "\n",
        "    half_size = subvolume_size // 2\n",
        "    cx, cy, cz = central_coord\n",
        "    x_start, x_end = max(cx - half_size, 0), min(cx + half_size, raw_vol.shape[2])\n",
        "    y_start, y_end = max(cy - half_size, 0), min(cy + half_size, raw_vol.shape[1])\n",
        "    z_start, z_end = max(cz - half_size, 0), min(cz + half_size, raw_vol.shape[0])\n",
        "\n",
        "    sub_raw = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_1 = mask_1_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_2 = mask_2_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_3 = mask_3_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "    pad_z = subvolume_size - sub_raw.shape[0]\n",
        "    pad_y = subvolume_size - sub_raw.shape[1]\n",
        "    pad_x = subvolume_size - sub_raw.shape[2]\n",
        "\n",
        "    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n",
        "        sub_raw = np.pad(sub_raw, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=0)\n",
        "        sub_mask_1 = np.pad(sub_mask_1, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "        sub_mask_2 = np.pad(sub_mask_2, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "        sub_mask_3 = np.pad(sub_mask_3, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "\n",
        "    sub_raw = sub_raw[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_1 = sub_mask_1[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_2 = sub_mask_2[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_3 = sub_mask_3[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "\n",
        "    overlaid_cube = np.zeros((subvolume_size, subvolume_size, 3, subvolume_size), dtype=np.uint8)\n",
        "\n",
        "    for z in range(subvolume_size):\n",
        "        raw_slice = sub_raw[z].astype(np.float32)\n",
        "        mn, mx = raw_slice.min(), raw_slice.max()\n",
        "        if mx > mn:\n",
        "            raw_slice = (raw_slice - mn) / (mx - mn)\n",
        "        else:\n",
        "            raw_slice = raw_slice - mn\n",
        "\n",
        "        if input_mask:  # New masking logic\n",
        "            combined_mask = np.zeros_like(raw_slice, dtype=np.float32)\n",
        "            if segmentation_type in [1, 3, 5]:\n",
        "                combined_mask = np.logical_or(combined_mask, sub_mask_1[z])\n",
        "            if segmentation_type in [2, 3, 5]:\n",
        "                combined_mask = np.logical_or(combined_mask, sub_mask_2[z])\n",
        "            if segmentation_type in [4, 5]:\n",
        "                combined_mask = np.logical_or(combined_mask, sub_mask_3[z])\n",
        "\n",
        "            masked_raw = raw_slice * combined_mask\n",
        "            masked_rgb = np.stack([masked_raw]*3, axis=-1)\n",
        "            overlaid_image = (masked_rgb * 255).astype(np.uint8)\n",
        "        else:  # Original overlay logic\n",
        "            raw_rgb = np.stack([raw_slice]*3, axis=-1)\n",
        "            mask1_rgb = np.zeros_like(raw_rgb)\n",
        "            mask2_rgb = np.zeros_like(raw_rgb)\n",
        "            mask3_rgb = np.zeros_like(raw_rgb)\n",
        "\n",
        "            if segmentation_type in [1, 3, 5]:\n",
        "                mask1_rgb[sub_mask_1[z]] = [1, 0, 0]\n",
        "            if segmentation_type in [2, 3, 5]:\n",
        "                mask2_rgb[sub_mask_2[z]] = [0, 0, 1]\n",
        "            if segmentation_type in [4, 5]:\n",
        "                mask3_rgb[sub_mask_3[z]] = [0, 1, 0]\n",
        "\n",
        "            combined_masks = mask1_rgb + mask2_rgb + mask3_rgb\n",
        "            combined_masks = np.clip(combined_masks, 0, 1)\n",
        "            overlaid_image = (1 - alpha) * raw_rgb + alpha * combined_masks\n",
        "            overlaid_image = (np.clip(overlaid_image, 0, 1) * 255).astype(np.uint8)\n",
        "\n",
        "        overlaid_cube[:, :, :, z] = overlaid_image\n",
        "\n",
        "    return overlaid_cube\n",
        "\n",
        "class VideoMAEDataset(Dataset):\n",
        "    def __init__(self, vol_data_dict: dict, synapse_df: pd.DataFrame, processor,\n",
        "                 segmentation_type: int, subvol_size: int = 80, num_frames: int = 16,\n",
        "                 alpha: float = 0.3, input_mask: bool = False):  # Added input_mask\n",
        "        self.vol_data_dict = vol_data_dict\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.segmentation_type = segmentation_type\n",
        "        self.subvol_size = subvol_size\n",
        "        self.num_frames = num_frames\n",
        "        self.alpha = alpha\n",
        "        self.input_mask = input_mask  # Store input_mask flag\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        syn_info = self.synapse_df.iloc[idx]\n",
        "        bbox_name = syn_info['bbox_name']\n",
        "        raw_vol, seg_vol, add_mask_vol = self.vol_data_dict.get(bbox_name, (None, None, None))\n",
        "\n",
        "        if raw_vol is None:\n",
        "            return torch.zeros((self.num_frames, 3, self.subvol_size, self.subvol_size), dtype=torch.float32), syn_info, bbox_name\n",
        "\n",
        "        central_coord = (int(syn_info['central_coord_1']), int(syn_info['central_coord_2']), int(syn_info['central_coord_3']))\n",
        "        side1_coord = (int(syn_info['side_1_coord_1']), int(syn_info['side_1_coord_2']), int(syn_info['side_1_coord_3']))\n",
        "        side2_coord = (int(syn_info['side_2_coord_1']), int(syn_info['side_2_coord_2']), int(syn_info['side_2_coord_3']))\n",
        "\n",
        "        overlaid_cube = create_segmented_cube(\n",
        "            raw_vol=raw_vol,\n",
        "            seg_vol=seg_vol,\n",
        "            add_mask_vol=add_mask_vol,\n",
        "            central_coord=central_coord,\n",
        "            side1_coord=side1_coord,\n",
        "            side2_coord=side2_coord,\n",
        "            segmentation_type=self.segmentation_type,\n",
        "            subvolume_size=self.subvol_size,\n",
        "            alpha=self.alpha,\n",
        "            input_mask=self.input_mask  # Pass the flag\n",
        "        )\n",
        "\n",
        "        frames = [overlaid_cube[..., z] for z in range(overlaid_cube.shape[3])]\n",
        "        if len(frames) < self.num_frames:\n",
        "            frames += [frames[-1]] * (self.num_frames - len(frames))\n",
        "        elif len(frames) > self.num_frames:\n",
        "            indices = np.linspace(0, len(frames)-1, self.num_frames, dtype=int)\n",
        "            frames = [frames[i] for i in indices]\n",
        "\n",
        "        inputs = self.processor(frames, return_tensors=\"pt\")\n",
        "        return inputs[\"pixel_values\"].squeeze(0).float(), syn_info, bbox_name\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"VideoMAE Pre-training Script with Segmented Videos and Additional Masks\")\n",
        "    parser.add_argument('--raw_base_dir', type=str, default='raw')\n",
        "    parser.add_argument('--seg_base_dir', type=str, default='seg')\n",
        "    parser.add_argument('--add_mask_base_dir', type=str, default='')\n",
        "    parser.add_argument('--bbox_name', type=str, default=['bbox1','bbox2','bbox3','bbox4','bbox5','bbox6', ], nargs='+')\n",
        "    parser.add_argument('--excel_file', type=str, default='')\n",
        "    parser.add_argument('--csv_output_dir', type=str, default='csv_outputs')\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints')\n",
        "    parser.add_argument('--log_dir', type=str, default='logs')\n",
        "    parser.add_argument('--size', type=tuple, default=(80,80))\n",
        "    parser.add_argument('--batch_size', type=int, default=2)\n",
        "    parser.add_argument('--num_epochs', type=int, default=5)\n",
        "    parser.add_argument('--learning_rate', type=float, default=1e-4)\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-2)\n",
        "    parser.add_argument('--subvol_size', type=int, default=80)\n",
        "    parser.add_argument('--num_frames', type=int, default=80)\n",
        "    parser.add_argument('--mask_ratio', type=float, default=0.75)\n",
        "    parser.add_argument('--patience', type=int, default=3)\n",
        "    parser.add_argument('--resume_checkpoint', type=str, default=None)\n",
        "    parser.add_argument('--save_gifs_dir', type=str, default='gifs')\n",
        "    parser.add_argument('--num_gifs', type=int, default=10)\n",
        "    parser.add_argument('--alpha', type=float, default=0.3)\n",
        "    parser.add_argument('--segmentation_type', type=int, default=0, choices=range(0, 6))\n",
        "    parser.add_argument('--input_mask', action='store_true', # Added argument\n",
        "                       help='Mask input image using segmentation_type regions')\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "def main(args):\n",
        "    processor = SimpleVideoProcessor(size=(80, 80))\n",
        "    vol_data_dict = {}\n",
        "\n",
        "    for bbox_name in args.bbox_name:\n",
        "        raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "            bbox_name=bbox_name,\n",
        "            raw_base_dir=args.raw_base_dir,\n",
        "            seg_base_dir=args.seg_base_dir,\n",
        "            add_mask_base_dir=args.add_mask_base_dir\n",
        "        )\n",
        "        if raw_vol is not None:\n",
        "            vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "    synapse_dfs = []\n",
        "    for bbox_name in args.bbox_name:\n",
        "        excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "        if os.path.exists(excel_path):\n",
        "            df = pd.read_excel(excel_path)\n",
        "            df['bbox_name'] = bbox_name\n",
        "            synapse_dfs.append(df)\n",
        "\n",
        "    syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "    dataset = VideoMAEDataset(\n",
        "        vol_data_dict=vol_data_dict,\n",
        "        synapse_df=syn_df,\n",
        "        processor=processor,\n",
        "        segmentation_type=args.segmentation_type,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=args.alpha,\n",
        "        input_mask=args.input_mask  # Pass the flag\n",
        "    )\n",
        "\n",
        "    cubes = []\n",
        "    syn_info_list = []\n",
        "    for idx in range(len(dataset)):\n",
        "        pixel_values, syn_info, _ = dataset[idx]\n",
        "        cubes.append(pixel_values)\n",
        "        syn_info_list.append(syn_info)\n",
        "\n",
        "    print(f\"Processed {len(cubes)} cubes successfully.\")\n",
        "    return cubes, pd.DataFrame(syn_info_list)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     args = parse_args()\n",
        "#     cubes, sys_inf = main(args)\n",
        "#     print(f\"Final output: {len(cubes)} cubes\")\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import imageio\n",
        "\n",
        "# cube = cubes[0]\n",
        "# mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "# std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "# denormalized_cube = cube * std + mean\n",
        "\n",
        "# denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "# frames = denormalized_cube.permute(0, 2, 3, 1).numpy()  # Change to (T, H, W, C)\n",
        "# frames = (frames * 255).astype(np.uint8)  # Convert to 0-255\n",
        "\n",
        "# imageio.mimsave('synapse_cube.gif', frames, fps=10)\n",
        "\n",
        "# print(\"GIF saved successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LOPaOUse7KEd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Test Average (sections that have not seg. be avg pix)"
      ],
      "metadata": {
        "id": "GpyUIuC7Wfwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import io\n",
        "import argparse\n",
        "import multiprocessing\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio.v3 as iio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "class SimpleVideoProcessor:\n",
        "    def __init__(self, size=(80, 80), mean=(0.485,),  # Use a single channel for grayscale\n",
        "                 std=(0.229,)):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(size),\n",
        "            transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std),\n",
        "        ])\n",
        "\n",
        "    def __call__(self, frames, return_tensors=None):\n",
        "        processed_frames = [self.transform(frame) for frame in frames]\n",
        "        pixel_values = torch.stack(processed_frames)\n",
        "        if return_tensors == \"pt\":\n",
        "            return {\"pixel_values\": pixel_values}\n",
        "        else:\n",
        "            return pixel_values\n",
        "\n",
        "\n",
        "def load_volumes(bbox_name: str, raw_base_dir: str, seg_base_dir: str, add_mask_base_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "    seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "\n",
        "    if bbox_name.startswith(\"bbox\"):\n",
        "        bbox_num = bbox_name.replace(\"bbox\", \"\")\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, f\"bbox_{bbox_num}\")\n",
        "    else:\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, bbox_name)\n",
        "\n",
        "    raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "    seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "    add_mask_tif_files = sorted(glob.glob(os.path.join(add_mask_dir, 'slice_*.tif')))\n",
        "\n",
        "    if not (len(raw_tif_files) == len(seg_tif_files) == len(add_mask_tif_files)):\n",
        "        return None, None, None\n",
        "\n",
        "    try:\n",
        "        raw_vol = np.stack([iio.imread(f) for f in raw_tif_files], axis=0)\n",
        "        seg_vol = np.stack([iio.imread(f).astype(np.uint32) for f in seg_tif_files], axis=0)\n",
        "        add_mask_vol = np.stack([iio.imread(f).astype(np.uint32) for f in add_mask_tif_files], axis=0)\n",
        "        return raw_vol, seg_vol, add_mask_vol\n",
        "    except Exception as e:\n",
        "        return None, None, None\n",
        "def create_segmented_cube(\n",
        "    raw_vol: np.ndarray,\n",
        "    seg_vol: np.ndarray,\n",
        "    add_mask_vol: np.ndarray,\n",
        "    central_coord: Tuple[int, int, int],\n",
        "    side1_coord: Tuple[int, int, int],\n",
        "    side2_coord: Tuple[int, int, int],\n",
        "    segmentation_type: int,\n",
        "    subvolume_size: int = 80,\n",
        "    alpha: float = 0.3,\n",
        "    input_mask: bool = False  # New parameter\n",
        ") -> np.ndarray:\n",
        "    def create_segment_masks(segmentation_volume, s1_coord, s2_coord):\n",
        "        x1, y1, z1 = s1_coord\n",
        "        x2, y2, z2 = s2_coord\n",
        "        seg_id_1 = segmentation_volume[z1, y1, x1]\n",
        "        seg_id_2 = segmentation_volume[z2, y2, x2]\n",
        "\n",
        "        mask_1 = (segmentation_volume == seg_id_1) if seg_id_1 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        mask_2 = (segmentation_volume == seg_id_2) if seg_id_2 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        return mask_1, mask_2\n",
        "\n",
        "    mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "    mask_3_full = (add_mask_vol > 0)\n",
        "\n",
        "    half_size = subvolume_size // 2\n",
        "    cx, cy, cz = central_coord\n",
        "    x_start, x_end = max(cx - half_size, 0), min(cx + half_size, raw_vol.shape[2])\n",
        "    y_start, y_end = max(cy - half_size, 0), min(cy + half_size, raw_vol.shape[1])\n",
        "    z_start, z_end = max(cz - half_size, 0), min(cz + half_size, raw_vol.shape[0])\n",
        "\n",
        "    sub_raw = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_1 = mask_1_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_2 = mask_2_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_3 = mask_3_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "    pad_z = subvolume_size - sub_raw.shape[0]\n",
        "    pad_y = subvolume_size - sub_raw.shape[1]\n",
        "    pad_x = subvolume_size - sub_raw.shape[2]\n",
        "\n",
        "    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n",
        "        sub_raw = np.pad(sub_raw, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=0)\n",
        "        sub_mask_1 = np.pad(sub_mask_1, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "        sub_mask_2 = np.pad(sub_mask_2, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "        sub_mask_3 = np.pad(sub_mask_3, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "\n",
        "    sub_raw = sub_raw[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_1 = sub_mask_1[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_2 = sub_mask_2[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_3 = sub_mask_3[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "\n",
        "    overlaid_cube = np.zeros((subvolume_size, subvolume_size, 3, subvolume_size), dtype=np.uint8)\n",
        "\n",
        "    for z in range(subvolume_size):\n",
        "        raw_slice = sub_raw[z].astype(np.float32)\n",
        "        mn, mx = raw_slice.min(), raw_slice.max()\n",
        "        if mx > mn:\n",
        "            raw_slice = (raw_slice - mn) / (mx - mn)\n",
        "        else:\n",
        "            raw_slice = raw_slice - mn\n",
        "\n",
        "        if input_mask:  # New masking logic with average pixel filling\n",
        "            combined_mask = np.zeros_like(raw_slice, dtype=np.float32)\n",
        "            if segmentation_type in [1, 3, 5]:\n",
        "                combined_mask = np.logical_or(combined_mask, sub_mask_1[z])\n",
        "            if segmentation_type in [2, 3, 5]:\n",
        "                combined_mask = np.logical_or(combined_mask, sub_mask_2[z])\n",
        "            if segmentation_type in [4, 5]:\n",
        "                combined_mask = np.logical_or(combined_mask, sub_mask_3[z])\n",
        "\n",
        "            # Fill the non-segmented regions with the average of the segmented pixels\n",
        "            if np.any(~combined_mask):  # Non-masked areas\n",
        "                avg_value = np.mean(raw_slice[combined_mask == 0])  # Average of segmented pixels\n",
        "                raw_slice[combined_mask == 0] = avg_value  # Replace non-segmented areas with the average\n",
        "\n",
        "            # Convert to RGB (average pixel value in all channels)\n",
        "            masked_rgb = np.stack([raw_slice] * 3, axis=-1)\n",
        "            overlaid_image = (masked_rgb * 255).astype(np.uint8)\n",
        "        else:  # Original overlay logic\n",
        "            raw_rgb = np.stack([raw_slice] * 3, axis=-1)\n",
        "            mask1_rgb = np.zeros_like(raw_rgb)\n",
        "            mask2_rgb = np.zeros_like(raw_rgb)\n",
        "            mask3_rgb = np.zeros_like(raw_rgb)\n",
        "\n",
        "            if segmentation_type in [1, 3, 5]:\n",
        "                mask1_rgb[sub_mask_1[z]] = [1, 0, 0]\n",
        "            if segmentation_type in [2, 3, 5]:\n",
        "                mask2_rgb[sub_mask_2[z]] = [0, 0, 1]\n",
        "            if segmentation_type in [4, 5]:\n",
        "                mask3_rgb[sub_mask_3[z]] = [0, 1, 0]\n",
        "\n",
        "            combined_masks = mask1_rgb + mask2_rgb + mask3_rgb\n",
        "            combined_masks = np.clip(combined_masks, 0, 1)\n",
        "            overlaid_image = (1 - alpha) * raw_rgb + alpha * combined_masks\n",
        "            overlaid_image = (np.clip(overlaid_image, 0, 1) * 255).astype(np.uint8)\n",
        "\n",
        "        overlaid_cube[:, :, :, z] = overlaid_image\n",
        "\n",
        "    return overlaid_cube\n",
        "\n",
        "\n",
        "class VideoMAEDataset(Dataset):\n",
        "    def __init__(self, vol_data_dict: dict, synapse_df: pd.DataFrame, processor,\n",
        "                 segmentation_type: int, subvol_size: int = 80, num_frames: int = 16,\n",
        "                 alpha: float = 0.3, input_mask: bool = False):  # Added input_mask\n",
        "        self.vol_data_dict = vol_data_dict\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.segmentation_type = segmentation_type\n",
        "        self.subvol_size = subvol_size\n",
        "        self.num_frames = num_frames\n",
        "        self.alpha = alpha\n",
        "        self.input_mask = input_mask  # Store input_mask flag\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        syn_info = self.synapse_df.iloc[idx]\n",
        "        bbox_name = syn_info['bbox_name']\n",
        "        raw_vol, seg_vol, add_mask_vol = self.vol_data_dict.get(bbox_name, (None, None, None))\n",
        "\n",
        "        if raw_vol is None:\n",
        "            return torch.zeros((self.num_frames, 3, self.subvol_size, self.subvol_size), dtype=torch.float32), syn_info, bbox_name\n",
        "\n",
        "        central_coord = (int(syn_info['central_coord_1']), int(syn_info['central_coord_2']), int(syn_info['central_coord_3']))\n",
        "        side1_coord = (int(syn_info['side_1_coord_1']), int(syn_info['side_1_coord_2']), int(syn_info['side_1_coord_3']))\n",
        "        side2_coord = (int(syn_info['side_2_coord_1']), int(syn_info['side_2_coord_2']), int(syn_info['side_2_coord_3']))\n",
        "\n",
        "        overlaid_cube = create_segmented_cube(\n",
        "            raw_vol=raw_vol,\n",
        "            seg_vol=seg_vol,\n",
        "            add_mask_vol=add_mask_vol,\n",
        "            central_coord=central_coord,\n",
        "            side1_coord=side1_coord,\n",
        "            side2_coord=side2_coord,\n",
        "            segmentation_type=self.segmentation_type,\n",
        "            subvolume_size=self.subvol_size,\n",
        "            alpha=self.alpha,\n",
        "            input_mask=self.input_mask  # Pass the flag\n",
        "        )\n",
        "\n",
        "        frames = [overlaid_cube[..., z] for z in range(overlaid_cube.shape[3])]\n",
        "        if len(frames) < self.num_frames:\n",
        "            frames += [frames[-1]] * (self.num_frames - len(frames))\n",
        "        elif len(frames) > self.num_frames:\n",
        "            indices = np.linspace(0, len(frames)-1, self.num_frames, dtype=int)\n",
        "            frames = [frames[i] for i in indices]\n",
        "\n",
        "        inputs = self.processor(frames, return_tensors=\"pt\")\n",
        "        return inputs[\"pixel_values\"].squeeze(0).float(), syn_info, bbox_name\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"VideoMAE Pre-training Script with Segmented Videos and Additional Masks\")\n",
        "    parser.add_argument('--raw_base_dir', type=str, default='raw')\n",
        "    parser.add_argument('--seg_base_dir', type=str, default='seg')\n",
        "    parser.add_argument('--add_mask_base_dir', type=str, default='')\n",
        "    parser.add_argument('--bbox_name', type=str, default=['bbox1','bbox2','bbox3','bbox4','bbox5','bbox6', ], nargs='+')\n",
        "    parser.add_argument('--excel_file', type=str, default='')\n",
        "    parser.add_argument('--csv_output_dir', type=str, default='csv_outputs')\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints')\n",
        "    parser.add_argument('--log_dir', type=str, default='logs')\n",
        "    parser.add_argument('--size', type=tuple, default=(80,80))\n",
        "    parser.add_argument('--batch_size', type=int, default=2)\n",
        "    parser.add_argument('--num_epochs', type=int, default=5)\n",
        "    parser.add_argument('--learning_rate', type=float, default=1e-4)\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-2)\n",
        "    parser.add_argument('--subvol_size', type=int, default=80)\n",
        "    parser.add_argument('--num_frames', type=int, default=80)\n",
        "    parser.add_argument('--mask_ratio', type=float, default=0.75)\n",
        "    parser.add_argument('--patience', type=int, default=3)\n",
        "    parser.add_argument('--resume_checkpoint', type=str, default=None)\n",
        "    parser.add_argument('--save_gifs_dir', type=str, default='gifs')\n",
        "    parser.add_argument('--num_gifs', type=int, default=10)\n",
        "    parser.add_argument('--alpha', type=float, default=0.3)\n",
        "    parser.add_argument('--segmentation_type', type=int, default=5, choices=range(0, 6))\n",
        "    parser.add_argument('--input_mask', action='store_true', # Added argument\n",
        "                       help='Mask input image using segmentation_type regions')\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "def main(args):\n",
        "    processor = SimpleVideoProcessor(size=(80, 80))\n",
        "    vol_data_dict = {}\n",
        "\n",
        "    for bbox_name in args.bbox_name:\n",
        "        raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "            bbox_name=bbox_name,\n",
        "            raw_base_dir=args.raw_base_dir,\n",
        "            seg_base_dir=args.seg_base_dir,\n",
        "            add_mask_base_dir=args.add_mask_base_dir\n",
        "        )\n",
        "        if raw_vol is not None:\n",
        "            vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "    synapse_dfs = []\n",
        "    for bbox_name in args.bbox_name:\n",
        "        excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "        if os.path.exists(excel_path):\n",
        "            df = pd.read_excel(excel_path)\n",
        "            df['bbox_name'] = bbox_name\n",
        "            synapse_dfs.append(df)\n",
        "\n",
        "    syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "    dataset = VideoMAEDataset(\n",
        "        vol_data_dict=vol_data_dict,\n",
        "        synapse_df=syn_df,\n",
        "        processor=processor,\n",
        "        segmentation_type=args.segmentation_type,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=args.alpha,\n",
        "        input_mask=args.input_mask  # Pass the flag\n",
        "    )\n",
        "\n",
        "    cubes = []\n",
        "    syn_info_list = []\n",
        "    for idx in range(len(dataset)):\n",
        "        pixel_values, syn_info, _ = dataset[idx]\n",
        "        cubes.append(pixel_values)\n",
        "        syn_info_list.append(syn_info)\n",
        "\n",
        "    print(f\"Processed {len(cubes)} cubes successfully.\")\n",
        "    return cubes, pd.DataFrame(syn_info_list)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     args = parse_args()\n",
        "#     args.input_mask=True\n",
        "#     cubes, sys_inf = main(args)\n",
        "#     print(f\"Final output: {len(cubes)} cubes\")\n",
        "\n",
        "# # Visualizeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import imageio\n",
        "\n",
        "# cube = cubes[0]\n",
        "# mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "# std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "# denormalized_cube = cube * std + mean\n",
        "\n",
        "# denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "# frames = denormalized_cube.permute(0, 2, 3, 1).numpy()  # Change to (T, H, W, C)\n",
        "# frames = (frames * 255).astype(np.uint8)  # Convert to 0-255\n",
        "\n",
        "# imageio.mimsave('synapse_cube.gif', frames, fps=10)\n",
        "\n",
        "# print(\"GIF saved successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pyZu4WqtWhCe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG Model and Feature Extracture"
      ],
      "metadata": {
        "id": "RfOZInSDZYXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "class Vgg3D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=(80, 80, 80),\n",
        "        fmaps=24,\n",
        "        downsample_factors=[(2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)],\n",
        "        fmap_inc=(2, 2, 2, 2),\n",
        "        n_convolutions=(4, 2, 2, 2),\n",
        "        output_classes=7,\n",
        "        input_fmaps=3,\n",
        "    ):\n",
        "        super(Vgg3D, self).__init__()\n",
        "\n",
        "        # Validate input parameters\n",
        "        if len(downsample_factors) != len(fmap_inc):\n",
        "            raise ValueError(\"fmap_inc needs to have same length as downsample factors\")\n",
        "        if len(n_convolutions) != len(fmap_inc):\n",
        "            raise ValueError(\"n_convolutions needs to have the same length as downsample factors\")\n",
        "        if np.any(np.array(n_convolutions) < 1):\n",
        "            raise ValueError(\"Each layer must have at least one convolution\")\n",
        "\n",
        "        current_fmaps = input_fmaps\n",
        "        current_size = np.array(input_size)\n",
        "\n",
        "        # Feature extraction layers\n",
        "        layers = []\n",
        "        for i, (df, nc) in enumerate(zip(downsample_factors, n_convolutions)):\n",
        "            # Convolution block\n",
        "            layers += [\n",
        "                nn.Conv3d(current_fmaps, fmaps, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm3d(fmaps),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "\n",
        "            # Additional convolutions\n",
        "            for _ in range(nc - 1):\n",
        "                layers += [\n",
        "                    nn.Conv3d(fmaps, fmaps, kernel_size=3, padding=1),\n",
        "                    nn.BatchNorm3d(fmaps),\n",
        "                    nn.ReLU(inplace=True)\n",
        "                ]\n",
        "\n",
        "            # Downsampling\n",
        "            layers.append(nn.MaxPool3d(df))\n",
        "\n",
        "            # Update feature map size\n",
        "            current_fmaps = fmaps\n",
        "            fmaps *= fmap_inc[i]\n",
        "\n",
        "            # Update spatial dimensions\n",
        "            current_size = np.floor(current_size / np.array(df))\n",
        "            # logger.info(f\"Block {i+1}: features {current_fmaps}, size {current_size}\")\n",
        "\n",
        "        self.features = nn.Sequential(*layers)\n",
        "\n",
        "        # Classifier (not used for feature extraction)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(current_size)) * current_fmaps, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, output_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        x = self.features(x)\n",
        "        if return_features:\n",
        "            return x  # Return raw features before flattening\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "def extract_features(model, dataset, args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device).eval()\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=2,\n",
        "        collate_fn=lambda b: (\n",
        "            torch.stack([item[0] for item in b]),  # Pixel values\n",
        "            [item[1] for item in b],               # Synapse info\n",
        "            [item[2] for item in b]                # Bbox names\n",
        "        )\n",
        "    )\n",
        "\n",
        "    features = []\n",
        "    metadata = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            pixels, info, names = batch\n",
        "            inputs = pixels.permute(0, 2, 1, 3, 4).to(device)  # Reshape for 3D convolution\n",
        "\n",
        "            batch_features = model.features(inputs)\n",
        "            pooled_features = nn.AdaptiveAvgPool3d((1, 1, 1))(batch_features)\n",
        "\n",
        "            # Flatten and convert to numpy immediately\n",
        "            features.append(pooled_features.cpu().numpy().squeeze())\n",
        "            metadata.extend(zip(names, info))\n",
        "\n",
        "    # Combine all batch features\n",
        "    features = np.concatenate(features, axis=0)\n",
        "\n",
        "    # Create metadata DataFrame\n",
        "    metadata_df = pd.DataFrame([\n",
        "        {\"bbox\": name, **info.to_dict()}\n",
        "        for name, info in metadata\n",
        "    ])\n",
        "\n",
        "    # Create feature columns\n",
        "    feature_columns = [f'feat_{i+1}' for i in range(features.shape[1])]\n",
        "    features_df = pd.DataFrame(features, columns=feature_columns)\n",
        "\n",
        "    # Combine with metadata\n",
        "    combined_df = pd.concat([metadata_df, features_df], axis=1)\n",
        "\n",
        "    return combined_df\n",
        "# # Initialize the model\n",
        "# model = Vgg3D(input_size=(80, 80, 80), fmaps=24, downsample_factors=[(2, 2, 2), (2, 2, 2)],\n",
        "#               fmap_inc=(2, 2), n_convolutions=(4, 2), output_classes=7, input_fmaps=1)\n",
        "\n",
        "# Load model from checkpoint\n",
        "def load_model_from_checkpoint(model, checkpoint_path):\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    # Load the state_dict into the model\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # If the checkpoint includes the optimizer state, you can load that as well\n",
        "    # model.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  # if needed\n",
        "\n",
        "    # Set the model to evaluation mode (disable dropout, batch norm updates, etc.)\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Example: Load your model with the checkpoint\n",
        "model = Vgg3D(input_size=(80, 80, 80), fmaps=24,output_classes=7, input_fmaps=1)\n",
        "\n",
        "# Paths and directories\n",
        "checkpoint_url = \"https://dl.dropboxusercontent.com/scl/fo/mfejaomhu43aa6oqs6zsf/AKMAAgT7OrUtruR0AQXZBy0/hemibrain_production.checkpoint.20220225?rlkey=6cmwxdvehy4ylztvsbgkfnrfc&dl=0\"\n",
        "checkpoint_path = 'hemibrain_production.checkpoint'\n",
        "\n",
        "# Download the checkpoint if it doesn't exist\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    os.system(f\"wget -O {checkpoint_path} '{checkpoint_url}'\")\n",
        "    print(\"Downloaded VGG3D checkpoint.\")\n",
        "else:\n",
        "    print(\"VGG3D checkpoint already exists.\")\n",
        "\n",
        "\n",
        "checkpoint_path = 'hemibrain_production.checkpoint'  # Replace with the actual path to your checkpoint\n",
        "model = load_model_from_checkpoint(model, checkpoint_path)\n"
      ],
      "metadata": {
        "id": "1QB2QlZYZbp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ad2a61-6233-498d-d63a-f2d6f88e933d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded VGG3D checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-16434c063c55>:136: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "brZg-Z5GdI5K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir Result\n"
      ],
      "metadata": {
        "id": "tbS9B5TmgiCJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#raw"
      ],
      "metadata": {
        "id": "ZIKEZAyJjn9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "args = parse_args()\n",
        "args.input_mask=False\n",
        "args.segmentation_type=0\n",
        "# Initialize the arguments (or use your existing args)\n",
        "# Use the main function to load data and prepare the dataset\n",
        "# cubes, sys_inf = main(args)\n",
        "\n",
        "# Define the processor (if not already done)\n",
        "processor = SimpleVideoProcessor(size=(80, 80))\n",
        "\n",
        "# Prepare the dataset\n",
        "vol_data_dict = {}\n",
        "for bbox_name in args.bbox_name:\n",
        "    raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "        bbox_name=bbox_name,\n",
        "        raw_base_dir=args.raw_base_dir,\n",
        "        seg_base_dir=args.seg_base_dir,\n",
        "        add_mask_base_dir=args.add_mask_base_dir\n",
        "    )\n",
        "    if raw_vol is not None:\n",
        "        vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "synapse_dfs = []\n",
        "for bbox_name in args.bbox_name:\n",
        "    excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "    if os.path.exists(excel_path):\n",
        "        df = pd.read_excel(excel_path)\n",
        "        df['bbox_name'] = bbox_name\n",
        "        synapse_dfs.append(df)\n",
        "\n",
        "syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "\n",
        "# Prepare the dataset\n",
        "dataset = VideoMAEDataset(\n",
        "    vol_data_dict=vol_data_dict,\n",
        "    synapse_df=syn_df,\n",
        "    processor=processor,\n",
        "    segmentation_type=args.segmentation_type,\n",
        "    subvol_size=args.subvol_size,\n",
        "    num_frames=args.num_frames,\n",
        "    alpha=args.alpha,\n",
        "    input_mask=False\n",
        ")\n",
        "\n",
        "# Now extract features from the model\n",
        "features_df = extract_features(model, dataset, args)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "# print(features_df.head())\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "# Extract the data tensor (cube) from the tuple returned by dataset[0]\n",
        "cube, syn_info, bbox_name = dataset[0]\n",
        "\n",
        "# Define the mean and std values for denormalization\n",
        "mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)  # 1x3x1x1 for RGB\n",
        "std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)  # 1x3x1x1 for RGB\n",
        "\n",
        "# Denormalize the cube (inverting the normalization step)\n",
        "denormalized_cube = cube * std + mean\n",
        "\n",
        "# Clamp the values between 0 and 1\n",
        "denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "\n",
        "# Convert to numpy and adjust the shape (T, H, W, C) for gif creation\n",
        "frames = denormalized_cube.permute(0, 2, 3, 1).numpy()  # Change to (T, H, W, C)\n",
        "\n",
        "# Convert frames to 0-255 range for gif\n",
        "frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_True\"\n",
        "output_gif_path = os.path.join(\"Result\", f\"{Gif_Name}.gif\")\n",
        "# Save the frames as a gif using imageio\n",
        "imageio.mimsave(output_gif_path, frames, fps=10)\n",
        "\n",
        "print(\"GIF saved successfully!\")\n",
        "\n",
        "segmentation_type_name = f\"VGG_CSV_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# segmentation_type_name = f\"VGG_CSV_segmentation_type_{args.segmentation_type}_input_mask_True\"\n",
        "output_csv_path = os.path.join(\"Result\", f\"{segmentation_type_name}_features.csv\")\n",
        "\n",
        "# Save the features_df DataFrame as a CSV file\n",
        "features_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"Features saved successfully as {output_csv_path}\")\n"
      ],
      "metadata": {
        "id": "WY_cBQIMdE6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "436da708-7cff-496a-8e44-baf499a18d6f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF saved successfully!\n",
            "Features saved successfully as Result/VGG_CSV_segmentation_typeavg_0_input_mask_False_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seg 5 False"
      ],
      "metadata": {
        "id": "iDUX9v4ij1lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# checkpoint_path = 'hemibrain_production.checkpoint'  # Replace with the actual path to your checkpoint\n",
        "# model = load_model_from_checkpoint(model, checkpoint_path)\n",
        "\n",
        "# Initialize the arguments (or use your existing args)\n",
        "args = parse_args()\n",
        "\n",
        "args.input_mask=False\n",
        "args.segmentation_type=5\n",
        "# Use the main function to load data and prepare the dataset\n",
        "cubes, sys_inf = main(args)\n",
        "\n",
        "# Define the processor (if not already done)\n",
        "processor = SimpleVideoProcessor(size=(80, 80))\n",
        "\n",
        "# Prepare the dataset\n",
        "vol_data_dict = {}\n",
        "for bbox_name in args.bbox_name:\n",
        "    raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "        bbox_name=bbox_name,\n",
        "        raw_base_dir=args.raw_base_dir,\n",
        "        seg_base_dir=args.seg_base_dir,\n",
        "        add_mask_base_dir=args.add_mask_base_dir\n",
        "    )\n",
        "    if raw_vol is not None:\n",
        "        vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "synapse_dfs = []\n",
        "for bbox_name in args.bbox_name:\n",
        "    excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "    if os.path.exists(excel_path):\n",
        "        df = pd.read_excel(excel_path)\n",
        "        df['bbox_name'] = bbox_name\n",
        "        synapse_dfs.append(df)\n",
        "\n",
        "syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "\n",
        "# Prepare the dataset\n",
        "dataset = VideoMAEDataset(\n",
        "    vol_data_dict=vol_data_dict,\n",
        "    synapse_df=syn_df,\n",
        "    processor=processor,\n",
        "    segmentation_type=5,\n",
        "    subvol_size=args.subvol_size,\n",
        "    num_frames=args.num_frames,\n",
        "    alpha=args.alpha,\n",
        "    input_mask=False\n",
        ")\n",
        "\n",
        "# Now extract features from the model\n",
        "features_df = extract_features(model, dataset, args)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "# print(features_df.head())\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "# Extract the data tensor (cube) from the tuple returned by dataset[0]\n",
        "cube, syn_info, bbox_name = dataset[0]\n",
        "\n",
        "# Define the mean and std values for denormalization\n",
        "mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)  # 1x3x1x1 for RGB\n",
        "std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)  # 1x3x1x1 for RGB\n",
        "\n",
        "# Denormalize the cube (inverting the normalization step)\n",
        "denormalized_cube = cube * std + mean\n",
        "\n",
        "# Clamp the values between 0 and 1\n",
        "denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "\n",
        "# Convert to numpy and adjust the shape (T, H, W, C) for gif creation\n",
        "frames = denormalized_cube.permute(0, 2, 3, 1).numpy()  # Change to (T, H, W, C)\n",
        "\n",
        "# Convert frames to 0-255 range for gif\n",
        "frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# Gif_Name = \"VGG_Gif_segmentation_type_5_input_mask_False\"\n",
        "output_gif_path = os.path.join(\"Result\", f\"{Gif_Name}.gif\")\n",
        "# Save the frames as a gif using imageio\n",
        "imageio.mimsave(output_gif_path, frames, fps=10)\n",
        "\n",
        "print(\"GIF saved successfully!\")\n",
        "\n",
        "segmentation_type_name = f\"VGG_CSV_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# segmentation_type_name = \"VGG_CSV_segmentation_type_5_input_mask_False\"\n",
        "output_csv_path = os.path.join(\"Result\", f\"{segmentation_type_name}_features.csv\")\n",
        "\n",
        "# Save the features_df DataFrame as a CSV file\n",
        "features_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"Features saved successfully as {output_csv_path}\")\n"
      ],
      "metadata": {
        "id": "T-JqloVlesAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d7df7ce-5eac-4445-d4e5-5f2d817b593e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 444 cubes successfully.\n",
            "GIF saved successfully!\n",
            "Features saved successfully as Result/VGG_CSV_segmentation_type_5_input_mask_False_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seg5 True Black"
      ],
      "metadata": {
        "id": "NFaP91LVj9JS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = parse_args()\n",
        "\n",
        "args.input_mask=True\n",
        "args.segmentation_type=5\n",
        "# Initialize the arguments (or use your existing args)\n",
        "\n",
        "# Use the main function to load data and prepare the dataset\n",
        "# cubes, sys_inf = main(args)\n",
        "\n",
        "# Define the processor (if not already done)\n",
        "processor = SimpleVideoProcessor(size=(80, 80))\n",
        "\n",
        "# Prepare the dataset\n",
        "vol_data_dict = {}\n",
        "for bbox_name in args.bbox_name:\n",
        "    raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "        bbox_name=bbox_name,\n",
        "        raw_base_dir=args.raw_base_dir,\n",
        "        seg_base_dir=args.seg_base_dir,\n",
        "        add_mask_base_dir=args.add_mask_base_dir\n",
        "    )\n",
        "    if raw_vol is not None:\n",
        "        vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "synapse_dfs = []\n",
        "for bbox_name in args.bbox_name:\n",
        "    excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "    if os.path.exists(excel_path):\n",
        "        df = pd.read_excel(excel_path)\n",
        "        df['bbox_name'] = bbox_name\n",
        "        synapse_dfs.append(df)\n",
        "\n",
        "syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "\n",
        "# Prepare the dataset\n",
        "dataset = VideoMAEDataset(\n",
        "    vol_data_dict=vol_data_dict,\n",
        "    synapse_df=syn_df,\n",
        "    processor=processor,\n",
        "    segmentation_type=5,\n",
        "    subvol_size=args.subvol_size,\n",
        "    num_frames=args.num_frames,\n",
        "    alpha=args.alpha,\n",
        "    input_mask=True\n",
        ")\n",
        "\n",
        "# Now extract features from the model\n",
        "features_df = extract_features(model, dataset, args)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "# print(features_df.head())\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "# Extract the data tensor (cube) from the tuple returned by dataset[0]\n",
        "cube, syn_info, bbox_name = dataset[0]\n",
        "\n",
        "# Define the mean and std values for denormalization\n",
        "mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)  # 1x3x1x1 for RGB\n",
        "std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)  # 1x3x1x1 for RGB\n",
        "\n",
        "# Denormalize the cube (inverting the normalization step)\n",
        "denormalized_cube = cube * std + mean\n",
        "\n",
        "# Clamp the values between 0 and 1\n",
        "denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "\n",
        "# Convert to numpy and adjust the shape (T, H, W, C) for gif creation\n",
        "frames = denormalized_cube.permute(0, 2, 3, 1).numpy()  # Change to (T, H, W, C)\n",
        "\n",
        "# Convert frames to 0-255 range for gif\n",
        "frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "# Gif_Name = \"VGG_Gif_segmentation_type_5_input_mask_True\"\n",
        "output_gif_path = os.path.join(\"Result\", f\"{Gif_Name}.gif\")\n",
        "# Save the frames as a gif using imageio\n",
        "imageio.mimsave(output_gif_path, frames, fps=10)\n",
        "\n",
        "print(\"GIF saved successfully!\")\n",
        "\n",
        "segmentation_type_name = f\"VGG_CSV_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}_Black\"\n",
        "# segmentation_type_name = \"VGG_CSV_segmentation_type_5_input_mask_True\"\n",
        "output_csv_path = os.path.join(\"Result\", f\"{segmentation_type_name}_Black_features.csv\")\n",
        "\n",
        "# Save the features_df DataFrame as a CSV file\n",
        "features_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"Features saved successfully as {output_csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43L_QB4Oek8H",
        "outputId": "ab687c09-e8ce-4462-9d7b-9f5896c39ce4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 444 cubes successfully.\n",
            "GIF saved successfully!\n",
            "Features saved successfully as Result/VGG_CSV_segmentation_type_5_input_mask_True_Black_Black_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-htkZtBYpDBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# seg5 True Avg"
      ],
      "metadata": {
        "id": "vrsHDN9QgsSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before run this run 1.2"
      ],
      "metadata": {
        "id": "ZLYkGGVdklX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# checkpoint_path = 'hemibrain_production.checkpoint'  # Replace with the actual path to your checkpoint\n",
        "# model = load_model_from_checkpoint(model, checkpoint_path)\n",
        "\n",
        "# Initialize the arguments (or use your existing args)\n",
        "args = parse_args()\n",
        "\n",
        "args.input_mask=True\n",
        "args.segmentation_type=5\n",
        "# Use the main function to load data and prepare the dataset\n",
        "# cubes, sys_inf = main(args)\n",
        "\n",
        "# Define the processor (if not already done)\n",
        "processor = SimpleVideoProcessor(size=(80, 80))\n",
        "\n",
        "# Prepare the dataset\n",
        "vol_data_dict = {}\n",
        "for bbox_name in args.bbox_name:\n",
        "    raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "        bbox_name=bbox_name,\n",
        "        raw_base_dir=args.raw_base_dir,\n",
        "        seg_base_dir=args.seg_base_dir,\n",
        "        add_mask_base_dir=args.add_mask_base_dir\n",
        "    )\n",
        "    if raw_vol is not None:\n",
        "        vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "\n",
        "synapse_dfs = []\n",
        "for bbox_name in args.bbox_name:\n",
        "    excel_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "    if os.path.exists(excel_path):\n",
        "        df = pd.read_excel(excel_path)\n",
        "        df['bbox_name'] = bbox_name\n",
        "        synapse_dfs.append(df)\n",
        "\n",
        "syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "\n",
        "# Prepare the dataset\n",
        "dataset = VideoMAEDataset(\n",
        "    vol_data_dict=vol_data_dict,\n",
        "    synapse_df=syn_df,\n",
        "    processor=processor,\n",
        "    segmentation_type=5,\n",
        "    subvol_size=args.subvol_size,\n",
        "    num_frames=args.num_frames,\n",
        "    alpha=args.alpha,\n",
        "    input_mask=True\n",
        ")\n",
        "\n",
        "# Now extract features from the model\n",
        "features_df = extract_features(model, dataset, args)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "# print(features_df.head())\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "# Extract the data tensor (cube) from the tuple returned by dataset[0]\n",
        "cube, syn_info, bbox_name = dataset[0]\n",
        "\n",
        "# Define the mean and std values for denormalization\n",
        "mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)  # 1x3x1x1 for RGB\n",
        "std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)  # 1x3x1x1 for RGB\n",
        "\n",
        "# Denormalize the cube (inverting the normalization step)\n",
        "denormalized_cube = cube * std + mean\n",
        "\n",
        "# Clamp the values between 0 and 1\n",
        "denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "\n",
        "# Convert to numpy and adjust the shape (T, H, W, C) for gif creation\n",
        "frames = denormalized_cube.permute(0, 2, 3, 1).numpy()  # Change to (T, H, W, C)\n",
        "\n",
        "# Convert frames to 0-255 range for gif\n",
        "frames = (frames * 255).astype(np.uint8)\n",
        "\n",
        "# Gif_Name = f\"VGG_Gif_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "Gif_Name = \"VGG_Gif_segmentation_type_5_input_mask_True_AVG\"\n",
        "output_gif_path = os.path.join(\"Result\", f\"{Gif_Name}.gif\")\n",
        "# Save the frames as a gif using imageio\n",
        "imageio.mimsave(output_gif_path, frames, fps=10)\n",
        "\n",
        "print(\"GIF saved successfully!\")\n",
        "\n",
        "# segmentation_type_name = f\"VGG_CSV_segmentation_type_{args.segmentation_type}_input_mask_{args.input_mask}\"\n",
        "segmentation_type_name = \"VGG_CSV_segmentation_type_5_input_mask_True_AVG\"\n",
        "output_csv_path = os.path.join(\"Result\", f\"{segmentation_type_name}_features.csv\")\n",
        "\n",
        "# Save the features_df DataFrame as a CSV file\n",
        "features_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"Features saved successfully as {output_csv_path}\")\n"
      ],
      "metadata": {
        "id": "F2kY7_HbguWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fd8329c-ef03-4512-d596-61fb1e294781"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF saved successfully!\n",
            "Features saved successfully as Result/VGG_CSV_segmentation_type_5_input_mask_True_AVG_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Replace 'folder_name' with the name of your folder\n",
        "shutil.make_archive('/content/Result', 'zip', '/content/Result')\n",
        "from google.colab import files\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/Result.zip')\n"
      ],
      "metadata": {
        "id": "6ak79mJzfmN_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "978fc852-6fe1-491f-8548-1bff4b1518d8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_41d81438-08a3-44cc-aa07-302bccaad58f\", \"Result.zip\", 2141480)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "metadata": {
        "id": "0SpY9s7liHtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5439238-0959-4f86-eab1-06d10b7d9ace"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bbox_1\t    bbox_7\n",
            "bbox1.xlsx  bbox7.xlsx\n",
            "bbox_2\t    downloaded_file.zip\n",
            "bbox2.xlsx  hemibrain_production.checkpoint\n",
            "bbox_3\t    __MACOSX\n",
            "bbox3.xlsx  raw\n",
            "bbox_4\t    Result\n",
            "bbox4.xlsx  Result.zip\n",
            "bbox_5\t    sample_data\n",
            "bbox5.xlsx  seg\n",
            "bbox_6\t    vesicle_cloud__syn_interface__mitochondria_annotation.zip\n",
            "bbox6.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fuck that"
      ],
      "metadata": {
        "id": "isgY9Mygc3Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import glob\n",
        "# import io\n",
        "# import argparse\n",
        "# import multiprocessing\n",
        "# from typing import List, Tuple\n",
        "\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import imageio.v3 as iio\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from torchvision import transforms\n",
        "\n",
        "# class SimpleVideoProcessor:\n",
        "#     def __init__(self, size=(80, 80),\n",
        "#                  mean=(0.485,),  # Single channel default\n",
        "#                  std=(0.229,),\n",
        "#                  grayscale=True):\n",
        "#         self.grayscale = grayscale\n",
        "#         transform_chain = [\n",
        "#             transforms.ToPILImage(),\n",
        "#             transforms.Resize(size),\n",
        "#         ]\n",
        "\n",
        "#         if grayscale:\n",
        "#             transform_chain.append(transforms.Grayscale(num_output_channels=1))\n",
        "\n",
        "#         transform_chain += [\n",
        "#             transforms.ToTensor(),\n",
        "#             transforms.Normalize(mean=mean, std=std),\n",
        "#         ]\n",
        "\n",
        "#         self.transform = transforms.Compose(transform_chain)\n",
        "\n",
        "#     def __call__(self, frames, return_tensors=None):\n",
        "#         processed_frames = [self.transform(frame) for frame in frames]\n",
        "#         pixel_values = torch.stack(processed_frames)\n",
        "#         return {\"pixel_values\": pixel_values} if return_tensors == \"pt\" else pixel_values\n",
        "\n",
        "\n",
        "# def load_volumes(bbox_name: str, raw_base_dir: str, seg_base_dir: str, add_mask_base_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "#     raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "#     seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "#     # print(raw_dir)\n",
        "#     if bbox_name.startswith(\"bbox\"):\n",
        "#         bbox_num = bbox_name.replace(\"bbox\", \"\")\n",
        "#         add_mask_dir = os.path.join(add_mask_base_dir, f\"bbox_{bbox_num}\")\n",
        "#     else:\n",
        "#         add_mask_dir = os.path.join(add_mask_base_dir, bbox_name)\n",
        "\n",
        "#     raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "#     seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "#     add_mask_tif_files = sorted(glob.glob(os.path.join(add_mask_dir, 'slice_*.tif')))\n",
        "\n",
        "#     if not (len(raw_tif_files) == len(seg_tif_files) == len(add_mask_tif_files)):\n",
        "#         print(len(raw_tif_files) , len(seg_tif_files) , len(add_mask_tif_files))\n",
        "#         return None, None, None\n",
        "\n",
        "#     try:\n",
        "#         raw_vol = np.stack([iio.imread(f) for f in raw_tif_files], axis=0)\n",
        "#         seg_vol = np.stack([iio.imread(f).astype(np.uint32) for f in seg_tif_files], axis=0)\n",
        "#         add_mask_vol = np.stack([iio.imread(f).astype(np.uint32) for f in add_mask_tif_files], axis=0)\n",
        "#         return raw_vol, seg_vol, add_mask_vol\n",
        "#     except Exception as e:\n",
        "#         print (67)\n",
        "#         return None, None, None\n",
        "\n",
        "# def create_segmented_cube(\n",
        "#     raw_vol: np.ndarray,\n",
        "#     seg_vol: np.ndarray,\n",
        "#     add_mask_vol: np.ndarray,\n",
        "#     central_coord: Tuple[int, int, int],\n",
        "#     side1_coord: Tuple[int, int, int],\n",
        "#     side2_coord: Tuple[int, int, int],\n",
        "#     segmentation_type: int,\n",
        "#     subvolume_size: int = 80,\n",
        "#     alpha: float = 0.3,\n",
        "#     input_mask: bool = False  # New parameter\n",
        "# ) -> np.ndarray:\n",
        "#     def create_segment_masks(segmentation_volume, s1_coord, s2_coord):\n",
        "#         x1, y1, z1 = s1_coord\n",
        "#         x2, y2, z2 = s2_coord\n",
        "#         seg_id_1 = segmentation_volume[z1, y1, x1]\n",
        "#         seg_id_2 = segmentation_volume[z2, y2, x2]\n",
        "\n",
        "#         mask_1 = (segmentation_volume == seg_id_1) if seg_id_1 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "#         mask_2 = (segmentation_volume == seg_id_2) if seg_id_2 != 0 else np.zeros_like(segmentation_volume, dtype=bool)\n",
        "#         return mask_1, mask_2\n",
        "\n",
        "#     mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "#     mask_3_full = (add_mask_vol > 0)\n",
        "\n",
        "#     half_size = subvolume_size // 2\n",
        "#     cx, cy, cz = central_coord\n",
        "#     x_start, x_end = max(cx - half_size, 0), min(cx + half_size, raw_vol.shape[2])\n",
        "#     y_start, y_end = max(cy - half_size, 0), min(cy + half_size, raw_vol.shape[1])\n",
        "#     z_start, z_end = max(cz - half_size, 0), min(cz + half_size, raw_vol.shape[0])\n",
        "\n",
        "#     sub_raw = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "#     sub_mask_1 = mask_1_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "#     sub_mask_2 = mask_2_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "#     sub_mask_3 = mask_3_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "#     pad_z = subvolume_size - sub_raw.shape[0]\n",
        "#     pad_y = subvolume_size - sub_raw.shape[1]\n",
        "#     pad_x = subvolume_size - sub_raw.shape[2]\n",
        "\n",
        "#     if pad_z > 0 or pad_y > 0 or pad_x > 0:\n",
        "#         sub_raw = np.pad(sub_raw, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=0)\n",
        "#         sub_mask_1 = np.pad(sub_mask_1, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "#         sub_mask_2 = np.pad(sub_mask_2, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "#         sub_mask_3 = np.pad(sub_mask_3, ((0, pad_z), (0, pad_y), (0, pad_x)), mode='constant', constant_values=False)\n",
        "\n",
        "#     sub_raw = sub_raw[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "#     sub_mask_1 = sub_mask_1[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "#     sub_mask_2 = sub_mask_2[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "#     sub_mask_3 = sub_mask_3[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "\n",
        "#     overlaid_cube = np.zeros((subvolume_size, subvolume_size, 3, subvolume_size), dtype=np.uint8)\n",
        "\n",
        "#     for z in range(subvolume_size):\n",
        "#         raw_slice = sub_raw[z].astype(np.float32)\n",
        "#         mn, mx = raw_slice.min(), raw_slice.max()\n",
        "#         if mx > mn:\n",
        "#             raw_slice = (raw_slice - mn) / (mx - mn)\n",
        "#         else:\n",
        "#             raw_slice = raw_slice - mn\n",
        "\n",
        "#         if input_mask:  # New masking logic\n",
        "#             combined_mask = np.zeros_like(raw_slice, dtype=np.float32)\n",
        "#             if segmentation_type in [1, 3, 5]:\n",
        "#                 combined_mask = np.logical_or(combined_mask, sub_mask_1[z])\n",
        "#             if segmentation_type in [2, 3, 5]:\n",
        "#                 combined_mask = np.logical_or(combined_mask, sub_mask_2[z])\n",
        "#             if segmentation_type in [4, 5]:\n",
        "#                 combined_mask = np.logical_or(combined_mask, sub_mask_3[z])\n",
        "\n",
        "#             masked_raw = raw_slice * combined_mask\n",
        "#             masked_rgb = np.stack([masked_raw]*3, axis=-1)\n",
        "#             overlaid_image = (masked_rgb * 255).astype(np.uint8)\n",
        "#         else:  # Original overlay logic\n",
        "#             raw_rgb = np.stack([raw_slice]*3, axis=-1)\n",
        "#             mask1_rgb = np.zeros_like(raw_rgb)\n",
        "#             mask2_rgb = np.zeros_like(raw_rgb)\n",
        "#             mask3_rgb = np.zeros_like(raw_rgb)\n",
        "\n",
        "#             if segmentation_type in [1, 3, 5]:\n",
        "#                 mask1_rgb[sub_mask_1[z]] = [1, 0, 0]\n",
        "#             if segmentation_type in [2, 3, 5]:\n",
        "#                 mask2_rgb[sub_mask_2[z]] = [0, 0, 1]\n",
        "#             if segmentation_type in [4, 5]:\n",
        "#                 mask3_rgb[sub_mask_3[z]] = [0, 1, 0]\n",
        "\n",
        "#             combined_masks = mask1_rgb + mask2_rgb + mask3_rgb\n",
        "#             combined_masks = np.clip(combined_masks, 0, 1)\n",
        "#             overlaid_image = (1 - alpha) * raw_rgb + alpha * combined_masks\n",
        "#             overlaid_image = (np.clip(overlaid_image, 0, 1) * 255).astype(np.uint8)\n",
        "\n",
        "#     # Add grayscale conversion option\n",
        "#     if input_mask:  # If using masks, convert to grayscale\n",
        "#         overlaid_cube = np.dot(overlaid_cube[..., :3], [0.2989, 0.5870, 0.1140])\n",
        "#         overlaid_cube = np.expand_dims(overlaid_cube, 2)  # Add channel dimension\n",
        "\n",
        "#         overlaid_cube[:, :, :, z] = overlaid_image\n",
        "\n",
        "#     return overlaid_cube\n",
        "\n",
        "# class VideoMAEDataset(Dataset):\n",
        "#     def __init__(self, vol_data_dict: dict, synapse_df: pd.DataFrame, processor,\n",
        "#                  segmentation_type: int, subvol_size: int = 80, num_frames: int = 16,\n",
        "#                  alpha: float = 0.3, input_mask: bool = False, output_channels: int = 1):\n",
        "#         self.vol_data_dict = vol_data_dict\n",
        "#         self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "#         self.processor = processor\n",
        "#         self.segmentation_type = segmentation_type\n",
        "#         self.subvol_size = subvol_size\n",
        "#         self.num_frames = num_frames\n",
        "#         self.alpha = alpha\n",
        "#         self.input_mask = input_mask\n",
        "#         self.output_channels = output_channels\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.synapse_df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         syn_info = self.synapse_df.iloc[idx]\n",
        "#         bbox_name = syn_info['bbox_name']\n",
        "#         raw_vol, seg_vol, add_mask_vol = self.vol_data_dict.get(bbox_name, (None, None, None))\n",
        "\n",
        "#         if raw_vol is None:\n",
        "#             return torch.zeros((self.num_frames, self.output_channels,\n",
        "#                               self.subvol_size, self.subvol_size),\n",
        "#                              dtype=torch.float32), syn_info, bbox_name\n",
        "\n",
        "#         central_coord = (int(syn_info['central_coord_1']), int(syn_info['central_coord_2']),\n",
        "#                         int(syn_info['central_coord_3']))\n",
        "#         side1_coord = (int(syn_info['side_1_coord_1']), int(syn_info['side_1_coord_2']),\n",
        "#                       int(syn_info['side_1_coord_3']))\n",
        "#         side2_coord = (int(syn_info['side_2_coord_1']), int(syn_info['side_2_coord_2']),\n",
        "#                       int(syn_info['side_2_coord_3']))\n",
        "\n",
        "#         overlaid_cube = create_segmented_cube(\n",
        "#             raw_vol=raw_vol,\n",
        "#             seg_vol=seg_vol,\n",
        "#             add_mask_vol=add_mask_vol,\n",
        "#             central_coord=central_coord,\n",
        "#             side1_coord=side1_coord,\n",
        "#             side2_coord=side2_coord,\n",
        "#             segmentation_type=self.segmentation_type,\n",
        "#             subvolume_size=self.subvol_size,\n",
        "#             alpha=self.alpha,\n",
        "#             input_mask=self.input_mask\n",
        "#         )\n",
        "\n",
        "#         # Convert to grayscale if needed\n",
        "#         if self.output_channels == 1:\n",
        "#             overlaid_cube = np.dot(overlaid_cube[..., :3], [0.2989, 0.5870, 0.1140])\n",
        "#             overlaid_cube = np.expand_dims(overlaid_cube, 2)  # Add channel dimension\n",
        "\n",
        "#         frames = [overlaid_cube[..., z] for z in range(overlaid_cube.shape[3])]\n",
        "#         if len(frames) < self.num_frames:\n",
        "#             frames += [frames[-1]] * (self.num_frames - len(frames))\n",
        "#         elif len(frames) > self.num_frames:\n",
        "#             indices = np.linspace(0, len(frames)-1, self.num_frames, dtype=int)\n",
        "#             frames = [frames[i] for i in indices]\n",
        "\n",
        "#         inputs = self.processor(frames, return_tensors=\"pt\")\n",
        "#         return inputs[\"pixel_values\"].squeeze(0).float(), syn_info, bbox_name\n",
        "\n",
        "# def parse_args():\n",
        "#     parser = argparse.ArgumentParser(description=\"VideoMAE Pre-training Script with Segmented Videos and Additional Masks\")\n",
        "#     parser.add_argument('--raw_base_dir', type=str, default='raw')\n",
        "#     parser.add_argument('--seg_base_dir', type=str, default='seg')\n",
        "#     parser.add_argument('--add_mask_base_dir', type=str, default='')\n",
        "#     parser.add_argument('--bbox_name', type=str, default=['bbox1'], nargs='+')\n",
        "#     parser.add_argument('--excel_file', type=str, default='')\n",
        "#     parser.add_argument('--csv_output_dir', type=str, default='csv_outputs')\n",
        "#     parser.add_argument('--checkpoint_dir', type=str, default='checkpoints')\n",
        "#     parser.add_argument('--log_dir', type=str, default='logs')\n",
        "#     parser.add_argument('--size', type=tuple, default=(80,80))\n",
        "#     parser.add_argument('--batch_size', type=int, default=2)\n",
        "#     parser.add_argument('--num_epochs', type=int, default=5)\n",
        "#     parser.add_argument('--learning_rate', type=float, default=1e-4)\n",
        "#     parser.add_argument('--weight_decay', type=float, default=1e-2)\n",
        "#     parser.add_argument('--subvol_size', type=int, default=80)\n",
        "#     parser.add_argument('--num_frames', type=int, default=80)\n",
        "#     parser.add_argument('--mask_ratio', type=float, default=0.75)\n",
        "#     parser.add_argument('--patience', type=int, default=3)\n",
        "#     parser.add_argument('--resume_checkpoint', type=str, default=None)\n",
        "#     parser.add_argument('--save_gifs_dir', type=str, default='gifs')\n",
        "#     parser.add_argument('--num_gifs', type=int, default=10)\n",
        "#     parser.add_argument('--alpha', type=float, default=0.3)\n",
        "#     parser.add_argument('--segmentation_type', type=int, default=5, choices=range(0, 6))\n",
        "#     parser.add_argument('--input_mask', action='store_true', # Added argument\n",
        "#                        help='Mask input image using segmentation_type regions')\n",
        "#     parser.add_argument('--input_channels', type=int, default=1,\n",
        "#                        help='Number of input channels (1 for grayscale)')\n",
        "#     parser.add_argument('--grayscale', action='store_true',\n",
        "#                        help='Convert input to grayscale')\n",
        "#     parser.add_argument(\"--extract_features\", action=\"store_true\")\n",
        "\n",
        "#     args, _ = parser.parse_known_args()\n",
        "\n",
        "#     return args\n",
        "\n",
        "\n",
        "\n",
        "# class Args:\n",
        "#     def __init__(self):\n",
        "#         # Data directories\n",
        "#         self.raw_base_dir = 'raw'\n",
        "#         self.seg_base_dir = 'seg'\n",
        "#         self.add_mask_base_dir = ''\n",
        "#         self.bbox_name = ['bbox1']\n",
        "#         self.excel_dir = ''\n",
        "#         self.checkpoint='hemibrain_production.checkpoint'\n",
        "#         # Output directories\n",
        "#         self.csv_output_dir = 'csv_outputs'\n",
        "#         # self.checkpoint_dir = 'hemibrain_production.checkpoint'\n",
        "#         self.log_dir = 'logs'\n",
        "#         self.num_classes=7\n",
        "#         # Training parameters\n",
        "#         self.size = (80, 80)\n",
        "#         self.batch_size = 2\n",
        "#         self.num_epochs = 5\n",
        "#         self.learning_rate = 1e-4\n",
        "#         self.weight_decay = 1e-2\n",
        "#         self.subvol_size = 80\n",
        "#         self.num_frames = 80\n",
        "#         self.mask_ratio = 0.75\n",
        "#         self.patience = 3\n",
        "#         self.resume_checkpoint = None\n",
        "\n",
        "#         # GIF parameters\n",
        "#         self.save_gifs_dir = 'gifs'\n",
        "#         self.num_gifs = 10\n",
        "#         self.alpha = 0.3\n",
        "#         self.segmentation_type = 5\n",
        "#         self.input_mask = True  # Set to True if you want masking\n",
        "#         self.extract_features=True\n",
        "#         self.num_workers=2\n",
        "#         self.output_dir='output'\n",
        "#         self.grayscale=True\n",
        "#         self.input_channels=1\n",
        "\n",
        "# # In your notebook cell:\n",
        "# args = Args()  # Create config object\n",
        "# args.input_mask = False  # Optionally override parameters\n",
        "# args.bbox_name = ['bbox1']  # Example override\n",
        "\n",
        "# # def main(args):\n",
        "#     # Load data\n",
        "# vol_data_dict = {}\n",
        "# for bbox in args.bbox_name:\n",
        "#     raw, seg, mask = load_volumes(\n",
        "#         bbox, args.raw_base_dir,\n",
        "#         args.seg_base_dir, args.add_mask_base_dir\n",
        "#     )\n",
        "#     if raw is not None:\n",
        "#         vol_data_dict[bbox] = (raw, seg, mask)\n",
        "\n",
        "# # Load annotations\n",
        "# synapse_dfs = []\n",
        "# for bbox in args.bbox_name:\n",
        "#     excel_path = os.path.join(args.excel_dir, f\"{bbox}.xlsx\")\n",
        "#     if os.path.exists(excel_path):\n",
        "#         df = pd.read_excel(excel_path)\n",
        "#         df[\"bbox_name\"] = bbox\n",
        "#         synapse_dfs.append(df)\n",
        "# syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "\n",
        "# # Create dataset\n",
        "# processor = SimpleVideoProcessor(size=(args.subvol_size, args.subvol_size))\n",
        "# dataset = VideoMAEDataset(\n",
        "#     vol_data_dict=vol_data_dict,\n",
        "#     synapse_df=syn_df,\n",
        "#     processor=processor,\n",
        "#     segmentation_type=args.segmentation_type,\n",
        "#     subvol_size=args.subvol_size,\n",
        "#     num_frames=args.num_frames,\n",
        "#     alpha=args.alpha,\n",
        "#     input_mask=args.input_mask\n",
        "# )"
      ],
      "metadata": {
        "id": "Ub6VvCtpl0xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKQ9VN7AYDIs",
        "outputId": "3ccdc666-1aa9-42dc-e722-60c8ecc8aaa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GIF saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import glob\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio.v3 as iio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import glob\n",
        "import io\n",
        "import argparse\n",
        "import multiprocessing\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio.v3 as iio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def main(args):\n",
        "    # Load data\n",
        "    vol_data_dict = {}\n",
        "    for bbox in args.bbox_name:\n",
        "        raw, seg, mask = load_volumes(\n",
        "            bbox, args.raw_base_dir,\n",
        "            args.seg_base_dir, args.add_mask_base_dir\n",
        "        )\n",
        "        if raw is not None:\n",
        "            vol_data_dict[bbox] = (raw, seg, mask)\n",
        "\n",
        "    # Load annotations\n",
        "    synapse_dfs = []\n",
        "    for bbox in args.bbox_name:\n",
        "        excel_path = os.path.join(args.excel_dir, f\"{bbox}.xlsx\")\n",
        "        if os.path.exists(excel_path):\n",
        "            df = pd.read_excel(excel_path)\n",
        "            df[\"bbox_name\"] = bbox\n",
        "            synapse_dfs.append(df)\n",
        "    syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "\n",
        "    # Create dataset\n",
        "    processor = SimpleVideoProcessor(size=(args.subvol_size, args.subvol_size))\n",
        "    dataset = VideoMAEDataset(\n",
        "        vol_data_dict=vol_data_dict,\n",
        "        synapse_df=syn_df,\n",
        "        processor=processor,\n",
        "        segmentation_type=args.segmentation_type,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=args.alpha,\n",
        "        input_mask=args.input_mask\n",
        "    )\n",
        "\n",
        "#     # Initialize model\n",
        "#     model = Vgg3D(\n",
        "#         input_size=(args.num_frames, args.subvol_size, args.subvol_size),\n",
        "#         input_fmaps=1,\n",
        "#         output_classes=args.num_classes\n",
        "#     )\n",
        "\n",
        "#     # Load checkpoint\n",
        "#     if args.checkpoint:\n",
        "#         state_dict = torch.load(args.checkpoint)[\"model_state_dict\"]\n",
        "#         model.load_state_dict(state_dict)\n",
        "#         logger.info(f\"Loaded weights from {args.checkpoint}\")\n",
        "\n",
        "#     if args.extract_features:\n",
        "#         # Get combined dataframe with features\n",
        "#         combined_df = extract_features(model, dataset, args)\n",
        "\n",
        "#         # Save results\n",
        "#         os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "#         # Save full dataset with features\n",
        "#         output_path = os.path.join(args.output_dir, \"metadata_with_features.csv\")\n",
        "#         combined_df.to_csv(output_path, index=False)\n",
        "\n",
        "#         logger.info(f\"Saved combined features and metadata to {output_path}\")\n",
        "#         logger.info(f\"Dataset shape: {combined_df.shape}\")\n",
        "#         logger.info(f\"Feature columns: {[c for c in combined_df.columns if c.startswith('feat_')]}\")\n",
        "\n",
        "# # Paths and directories\n",
        "# checkpoint_url = \"https://dl.dropboxusercontent.com/scl/fo/mfejaomhu43aa6oqs6zsf/AKMAAgT7OrUtruR0AQXZBy0/hemibrain_production.checkpoint.20220225?rlkey=6cmwxdvehy4ylztvsbgkfnrfc&dl=0\"\n",
        "# checkpoint_path = 'hemibrain_production.checkpoint'\n",
        "\n",
        "# # Download the checkpoint if it doesn't exist\n",
        "# if not os.path.exists(checkpoint_path):\n",
        "#     os.system(f\"wget -O {checkpoint_path} '{checkpoint_url}'\")\n",
        "#     print(\"Downloaded VGG3D checkpoint.\")\n",
        "# else:\n",
        "#     print(\"VGG3D checkpoint already exists.\")\n",
        "\n",
        "\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        # Data directories\n",
        "        self.raw_base_dir = 'raw'\n",
        "        self.seg_base_dir = 'seg'\n",
        "        self.add_mask_base_dir = ''\n",
        "        self.bbox_name = ['bbox1']\n",
        "        self.excel_dir = ''\n",
        "        self.checkpoint='hemibrain_production.checkpoint'\n",
        "        # Output directories\n",
        "        self.csv_output_dir = 'csv_outputs'\n",
        "        # self.checkpoint_dir = 'hemibrain_production.checkpoint'\n",
        "        self.log_dir = 'logs'\n",
        "        self.num_classes=7\n",
        "        # Training parameters\n",
        "        self.size = (80, 80)\n",
        "        self.batch_size = 2\n",
        "        self.num_epochs = 5\n",
        "        self.learning_rate = 1e-4\n",
        "        self.weight_decay = 1e-2\n",
        "        self.subvol_size = 80\n",
        "        self.num_frames = 80\n",
        "        self.mask_ratio = 0.75\n",
        "        self.patience = 3\n",
        "        self.resume_checkpoint = None\n",
        "\n",
        "        # GIF parameters\n",
        "        self.save_gifs_dir = 'gifs'\n",
        "        self.num_gifs = 10\n",
        "        self.alpha = 0.3\n",
        "        self.segmentation_type = 5\n",
        "        self.input_mask = True  # Set to True if you want masking\n",
        "        self.extract_features=True\n",
        "        self.num_workers=2\n",
        "        self.output_dir='output'\n",
        "        self.grayscale=True\n",
        "        self.input_channels=1\n",
        "# In your notebook cell:\n",
        "args = Args()  # Create config object\n",
        "args.input_mask = True  # Optionally override parameters\n",
        "args.bbox_name = ['bbox1','bbox2','bbox3','bbox4','bbox5','bbox6']  # Example override\n",
        "\n",
        "# Run main program\n",
        "main(args)\n",
        "# print(f\"Processed {len(cubes)} cubes successfully.\")"
      ],
      "metadata": {
        "id": "ifrIMslw5Vyx",
        "outputId": "5cb7b62f-36f8-4915-90b5-3a59c31ba504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded VGG3D checkpoint.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8ebce88baa15>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;31m# Run main program\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;31m# print(f\"Processed {len(cubes)} cubes successfully.\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-8ebce88baa15>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mvol_data_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         raw, seg, mask = load_volumes(\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_base_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseg_base_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_mask_base_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-34c37d55f922>\u001b[0m in \u001b[0;36mload_volumes\u001b[0;34m(bbox_name, raw_base_dir, seg_base_dir, add_mask_base_dir)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mraw_vol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_tif_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mseg_vol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseg_tif_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0madd_mask_vol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madd_mask_tif_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-34c37d55f922>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mraw_vol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_tif_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mseg_vol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseg_tif_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0madd_mask_vol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madd_mask_tif_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imageio/v3.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(uri, index, plugin, extension, format_hint, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mimopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imageio/plugins/tifffile_v3.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, index, page, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mEllipsis\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mndimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mndimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(self, key, series, level, squeeze, out, maxworkers, buffersize)\u001b[0m\n\u001b[1;32m   4546\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpage0\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4547\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'page is None'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4548\u001b[0;31m             result = page0.asarray(\n\u001b[0m\u001b[1;32m   4549\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffersize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4550\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(self, out, squeeze, lock, maxworkers, buffersize)\u001b[0m\n\u001b[1;32m   8884\u001b[0m                 \u001b[0;31m#     pass  # corrupted file, for example, with too many strips\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8886\u001b[0;31m             for _ in self.segments(\n\u001b[0m\u001b[1;32m   8887\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8888\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36msegments\u001b[0;34m(self, lock, maxworkers, func, sort, buffersize, _fullsize)\u001b[0m\n\u001b[1;32m   8684\u001b[0m                 \u001b[0mflat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8685\u001b[0m             ):\n\u001b[0;32m-> 8686\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8688\u001b[0m             \u001b[0;31m# reduce memory overhead by processing chunks of up to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(args, decodeargs, decode)\u001b[0m\n\u001b[1;32m   8671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8672\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecodeargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecodeargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeyframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8673\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecodeargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxworkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmaxworkers\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36mdecode_other\u001b[0;34m(data, index, jpegtables, jpegheader, _fullsize)\u001b[0m\n\u001b[1;32m   8596\u001b[0m                 \u001b[0;31m# TODO: calculate correct size for packed integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8597\u001b[0m                 \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8598\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8599\u001b[0m             \u001b[0mdata_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8600\u001b[0m             \u001b[0;31m# del data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tifffile/_imagecodecs.py\u001b[0m in \u001b[0;36mpackbits_decode\u001b[0;34m(encoded, out)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m129\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# literal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0mout_extend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Show"
      ],
      "metadata": {
        "id": "cQ4IsHMsm28c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import glob\n",
        "import io\n",
        "import argparse\n",
        "import multiprocessing\n",
        "from typing import List, Tuple\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio.v3 as iio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "\n",
        "# import wandb  # Uncomment if using Weights & Biases for logging\n",
        "\n",
        "\n",
        "class SimpleVideoProcessor:\n",
        "    def __init__(self, size=(80, 80), mean=(0.485, 0.456, 0.406),\n",
        "                 std=(0.229, 0.224, 0.225)):\n",
        "        \"\"\"\n",
        "        Initializes the processor with resizing and normalization transforms.\n",
        "\n",
        "        Args:\n",
        "            size (tuple): Desired output size (height, width).\n",
        "            mean (tuple): Mean for normalization.\n",
        "            std (tuple): Standard deviation for normalization.\n",
        "        \"\"\"\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),          # Convert NumPy array to PIL Image\n",
        "            transforms.Resize(size),          # Resize to desired size\n",
        "            transforms.ToTensor(),            # Convert PIL Image to Tensor\n",
        "            transforms.Normalize(mean=mean, std=std),  # Normalize\n",
        "        ])\n",
        "\n",
        "    def __call__(self, frames, return_tensors=None):\n",
        "        \"\"\"\n",
        "        Processes a list of frames.\n",
        "\n",
        "        Args:\n",
        "            frames (List[np.ndarray]): List of frames as NumPy arrays.\n",
        "            return_tensors (str, optional): Type of tensors to return. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            dict or torch.Tensor: Dictionary containing processed pixel values or tensor.\n",
        "        \"\"\"\n",
        "        # Apply transformations to each frame\n",
        "        processed_frames = [self.transform(frame) for frame in frames]\n",
        "\n",
        "        # Stack frames to create a tensor of shape (num_frames, 3, H, W)\n",
        "        pixel_values = torch.stack(processed_frames)\n",
        "\n",
        "        if return_tensors == \"pt\":\n",
        "            return {\"pixel_values\": pixel_values}\n",
        "        else:\n",
        "            return pixel_values\n",
        "\n",
        "\n",
        "def load_volumes(bbox_name: str, raw_base_dir: str, seg_base_dir: str, add_mask_base_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load raw volume, segmentation volume, and additional mask volume for a bounding box.\n",
        "\n",
        "    Args:\n",
        "        bbox_name (str): Name of the bounding box directory (e.g., 'bbox1').\n",
        "        raw_base_dir (str): Base directory for raw data.\n",
        "        seg_base_dir (str): Base directory for segmentation data.\n",
        "        add_mask_base_dir (str): Base directory for additional masks.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (raw_vol, seg_vol, add_mask_vol) each as np.ndarray\n",
        "    \"\"\"\n",
        "    raw_dir = os.path.join(raw_base_dir, bbox_name)\n",
        "    seg_dir = os.path.join(seg_base_dir, bbox_name)\n",
        "\n",
        "    # Transform 'bbox1' to 'bbox_1' for additional masks\n",
        "    if bbox_name.startswith(\"bbox\"):\n",
        "        bbox_num = bbox_name.replace(\"bbox\", \"\")\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, f\"bbox_{bbox_num}\")\n",
        "    else:\n",
        "        add_mask_dir = os.path.join(add_mask_base_dir, bbox_name)\n",
        "\n",
        "    raw_tif_files = sorted(glob.glob(os.path.join(raw_dir, 'slice_*.tif')))\n",
        "    seg_tif_files = sorted(glob.glob(os.path.join(seg_dir, 'slice_*.tif')))\n",
        "    add_mask_tif_files = sorted(glob.glob(os.path.join(add_mask_dir, 'slice_*.tif')))\n",
        "\n",
        "    if len(raw_tif_files) == 0:\n",
        "        print(f\"No raw files found for {bbox_name} in {raw_dir}\")\n",
        "        return None, None, None\n",
        "\n",
        "    if len(seg_tif_files) == 0:\n",
        "        print(f\"No segmentation files found for {bbox_name} in {seg_dir}\")\n",
        "        return None, None, None\n",
        "\n",
        "    if len(add_mask_tif_files) == 0:\n",
        "        print(f\"No additional mask files found for {bbox_name} in {add_mask_dir}\")\n",
        "        return None, None, None\n",
        "\n",
        "    if not (len(raw_tif_files) == len(seg_tif_files) == len(add_mask_tif_files)):\n",
        "        print(f\"Mismatch in number of raw, seg, and additional mask slices for {bbox_name}. Skipping.\")\n",
        "        return None, None, None\n",
        "\n",
        "    try:\n",
        "        raw_vol = np.stack([iio.imread(f) for f in raw_tif_files], axis=0)  # shape: (Z, Y, X)\n",
        "        seg_vol = np.stack([iio.imread(f).astype(np.uint32) for f in seg_tif_files], axis=0)\n",
        "        add_mask_vol = np.stack([iio.imread(f).astype(np.uint32) for f in add_mask_tif_files], axis=0)\n",
        "        return raw_vol, seg_vol, add_mask_vol\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading volumes for {bbox_name}: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "\n",
        "def create_segmented_cube(\n",
        "    raw_vol: np.ndarray,\n",
        "    seg_vol: np.ndarray,\n",
        "    add_mask_vol: np.ndarray,\n",
        "    central_coord: Tuple[int, int, int],\n",
        "    side1_coord: Tuple[int, int, int],\n",
        "    side2_coord: Tuple[int, int, int],\n",
        "    segmentation_type: int,\n",
        "    subvolume_size: int = 80,\n",
        "    alpha: float = 0.3\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Constructs an 80x80x80 segmented 3D cube around the specified synapse coordinates\n",
        "    and overlays selected segmentation masks on the raw data with specified transparency for each slice.\n",
        "\n",
        "    Args:\n",
        "        raw_vol (np.ndarray): Raw volumetric data.\n",
        "        seg_vol (np.ndarray): Segmentation volumetric data.\n",
        "        add_mask_vol (np.ndarray): Additional mask volumetric data.\n",
        "        central_coord (tuple): Central coordinate (x, y, z).\n",
        "        side1_coord (tuple): Side 1 coordinate (x, y, z).\n",
        "        side2_coord (tuple): Side 2 coordinate (x, y, z).\n",
        "        segmentation_type (int): Type of segmentation overlay (0-5).\n",
        "        subvolume_size (int, optional): Size of the subvolume. Defaults to 80.\n",
        "        alpha (float, optional): Transparency factor. Defaults to 0.3.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Overlaid cube of shape (height, width, 3, depth).\n",
        "    \"\"\"\n",
        "\n",
        "    def create_segment_masks(segmentation_volume, s1_coord, s2_coord):\n",
        "        x1, y1, z1 = s1_coord\n",
        "        x2, y2, z2 = s2_coord\n",
        "        # Validate within volume\n",
        "        if not (0 <= z1 < segmentation_volume.shape[0] and\n",
        "                0 <= y1 < segmentation_volume.shape[1] and\n",
        "                0 <= x1 < segmentation_volume.shape[2]):\n",
        "            raise ValueError(\"Side1 coordinates are out of bounds.\")\n",
        "\n",
        "        if not (0 <= z2 < segmentation_volume.shape[0] and\n",
        "                0 <= y2 < segmentation_volume.shape[1] and\n",
        "                0 <= x2 < segmentation_volume.shape[2]):\n",
        "            raise ValueError(\"Side2 coordinates are out of bounds.\")\n",
        "\n",
        "        seg_id_1 = segmentation_volume[z1, y1, x1]\n",
        "        seg_id_2 = segmentation_volume[z2, y2, x2]\n",
        "\n",
        "        # If seg_id == 0, it means no segment at that voxel\n",
        "        if seg_id_1 == 0:\n",
        "            mask_1 = np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        else:\n",
        "            mask_1 = (segmentation_volume == seg_id_1)\n",
        "\n",
        "        if seg_id_2 == 0:\n",
        "            mask_2 = np.zeros_like(segmentation_volume, dtype=bool)\n",
        "        else:\n",
        "            mask_2 = (segmentation_volume == seg_id_2)\n",
        "\n",
        "        return mask_1, mask_2\n",
        "\n",
        "    # Build masks\n",
        "    mask_1_full, mask_2_full = create_segment_masks(seg_vol, side1_coord, side2_coord)\n",
        "    mask_3_full = (add_mask_vol > 0)  # Assuming binary masks; adjust if necessary\n",
        "\n",
        "    # Define subvolume bounds\n",
        "    half_size = subvolume_size // 2\n",
        "    cx, cy, cz = central_coord\n",
        "\n",
        "    x_start, x_end = max(cx - half_size, 0), min(cx + half_size, raw_vol.shape[2])\n",
        "    y_start, y_end = max(cy - half_size, 0), min(cy + half_size, raw_vol.shape[1])\n",
        "    z_start, z_end = max(cz - half_size, 0), min(cz + half_size, raw_vol.shape[0])\n",
        "\n",
        "    # Extract subvolumes\n",
        "    sub_raw = raw_vol[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_1 = mask_1_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_2 = mask_2_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "    sub_mask_3 = mask_3_full[z_start:z_end, y_start:y_end, x_start:x_end]\n",
        "\n",
        "    # Pad if smaller than subvolume_size\n",
        "    pad_z = subvolume_size - sub_raw.shape[0]\n",
        "    pad_y = subvolume_size - sub_raw.shape[1]\n",
        "    pad_x = subvolume_size - sub_raw.shape[2]\n",
        "\n",
        "    if pad_z > 0 or pad_y > 0 or pad_x > 0:\n",
        "        sub_raw = np.pad(sub_raw, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                         mode='constant', constant_values=0)\n",
        "        sub_mask_1 = np.pad(sub_mask_1, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                            mode='constant', constant_values=False)\n",
        "        sub_mask_2 = np.pad(sub_mask_2, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                            mode='constant', constant_values=False)\n",
        "        sub_mask_3 = np.pad(sub_mask_3, ((0, pad_z), (0, pad_y), (0, pad_x)),\n",
        "                            mode='constant', constant_values=False)\n",
        "\n",
        "    # Slice to exact shape\n",
        "    sub_raw = sub_raw[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_1 = sub_mask_1[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_2 = sub_mask_2[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "    sub_mask_3 = sub_mask_3[:subvolume_size, :subvolume_size, :subvolume_size]\n",
        "\n",
        "    # We'll build an overlaid cube: shape => (H, W, 3, D)\n",
        "    overlaid_cube = np.zeros((subvolume_size, subvolume_size, 3, subvolume_size), dtype=np.uint8)\n",
        "\n",
        "    # Define colors\n",
        "    side1_color = np.array([1, 0, 0], dtype=np.float32)           # Red\n",
        "    side2_color = np.array([0, 0, 1], dtype=np.float32)           # Blue\n",
        "    vesicles_color = np.array([0, 1, 0], dtype=np.float32)        # Green\n",
        "\n",
        "    for z in range(subvolume_size):\n",
        "        # Normalize raw slice to [0, 1]\n",
        "        raw_slice = sub_raw[z].astype(np.float32)\n",
        "        mn, mx = raw_slice.min(), raw_slice.max()\n",
        "        if mx > mn:\n",
        "            raw_slice = (raw_slice - mn) / (mx - mn)\n",
        "        else:\n",
        "            raw_slice = raw_slice - mn  # all zeros if mn=mx\n",
        "\n",
        "        raw_rgb = np.stack([raw_slice]*3, axis=-1)  # shape (H, W, 3)\n",
        "\n",
        "        # Initialize colored masks\n",
        "        mask1_rgb = np.zeros_like(raw_rgb)\n",
        "        mask2_rgb = np.zeros_like(raw_rgb)\n",
        "        mask3_rgb = np.zeros_like(raw_rgb)\n",
        "\n",
        "        # Overlay masks based on segmentation_type\n",
        "        if segmentation_type in [1, 3, 5]:\n",
        "            mask1_rgb[sub_mask_1[z]] = side1_color\n",
        "        if segmentation_type in [2, 3, 5]:\n",
        "            mask2_rgb[sub_mask_2[z]] = side2_color\n",
        "        if segmentation_type in [4, 5]:\n",
        "            mask3_rgb[sub_mask_3[z]] = vesicles_color\n",
        "\n",
        "        # Combine masks\n",
        "        combined_masks = mask1_rgb + mask2_rgb + mask3_rgb\n",
        "        # Ensure that combined masks do not exceed 1\n",
        "        combined_masks = np.clip(combined_masks, 0, 1)\n",
        "\n",
        "        # Blend raw image with masks\n",
        "        overlaid_image = (1 - alpha) * raw_rgb + alpha * combined_masks\n",
        "        overlaid_image = np.clip(overlaid_image, 0, 1)\n",
        "\n",
        "        # Convert to uint8\n",
        "        overlaid_image = (overlaid_image * 255).astype(np.uint8)\n",
        "        overlaid_cube[:, :, :, z] = overlaid_image\n",
        "\n",
        "    return overlaid_cube\n",
        "\n",
        "class VideoMAEDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class that uses segmented volumes (side1 & side2) and additional masks for VideoMAE pre-training.\n",
        "    \"\"\"\n",
        "    def __init__(self, vol_data_list: List[Tuple[np.ndarray, np.ndarray, np.ndarray]],\n",
        "                 synapse_df: pd.DataFrame,\n",
        "                 processor,\n",
        "                 segmentation_type: int,\n",
        "                 subvol_size: int = 80,\n",
        "                 num_frames: int = 16,\n",
        "                 alpha: float = 0.3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vol_data_list (List[Tuple[np.ndarray, np.ndarray, np.ndarray]]): List of (raw_vol, seg_vol, add_mask_vol).\n",
        "            synapse_df (pd.DataFrame): DataFrame with synapse coordinates (central, side1, side2).\n",
        "            processor: Processor for VideoMAE.\n",
        "            segmentation_type (int): Type of segmentation overlay (0-5).\n",
        "            subvol_size (int): Size of the sub-volume to extract.\n",
        "            num_frames (int): Number of frames for the model.\n",
        "            alpha (float): Blending alpha for segmentation.\n",
        "        \"\"\"\n",
        "        self.vol_data_list = vol_data_list\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.segmentation_type = segmentation_type\n",
        "        self.subvol_size = subvol_size\n",
        "        self.num_frames = num_frames\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "class VideoMAEDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class that uses segmented volumes (side1 & side2) and additional masks for VideoMAE pre-training.\n",
        "    \"\"\"\n",
        "    def __init__(self, vol_data_dict: dict,\n",
        "                 synapse_df: pd.DataFrame,\n",
        "                 processor,\n",
        "                 segmentation_type: int,\n",
        "                 subvol_size: int = 80,\n",
        "                 num_frames: int = 16,\n",
        "                 alpha: float = 0.3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vol_data_dict (dict): Dictionary with keys as bbox_name and values as tuples (raw_vol, seg_vol, add_mask_vol).\n",
        "            synapse_df (pd.DataFrame): DataFrame with synapse coordinates (central, side1, side2).\n",
        "            processor: Processor for VideoMAE.\n",
        "            segmentation_type (int): Type of segmentation overlay (0-5).\n",
        "            subvol_size (int): Size of the sub-volume to extract.\n",
        "            num_frames (int): Number of frames for the model.\n",
        "            alpha (float): Blending alpha for segmentation.\n",
        "        \"\"\"\n",
        "        self.vol_data_dict = vol_data_dict  # Changed to dictionary\n",
        "        self.synapse_df = synapse_df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.segmentation_type = segmentation_type\n",
        "        self.subvol_size = subvol_size\n",
        "        self.num_frames = num_frames\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synapse_df)\n",
        "    def __getitem__(self, idx):\n",
        "        syn_info = self.synapse_df.iloc[idx]\n",
        "        bbox_name = syn_info['bbox_name']  # Use bbox_name instead of bbox_index\n",
        "\n",
        "        # Unpack the volumes using bbox_name instead of bbox_index\n",
        "        raw_vol, seg_vol, add_mask_vol = self.vol_data_dict.get(bbox_name, (None, None, None))\n",
        "\n",
        "        if raw_vol is None or seg_vol is None or add_mask_vol is None:\n",
        "            # Return dummy data if volumes not found\n",
        "            pixel_values = torch.zeros((self.num_frames, 3, self.subvol_size, self.subvol_size), dtype=torch.float32)\n",
        "            return pixel_values, syn_info, bbox_name\n",
        "\n",
        "        # Coordinates\n",
        "        central_coord = (\n",
        "            int(syn_info['central_coord_1']),\n",
        "            int(syn_info['central_coord_2']),\n",
        "            int(syn_info['central_coord_3'])\n",
        "        )\n",
        "        side1_coord = (\n",
        "            int(syn_info['side_1_coord_1']),\n",
        "            int(syn_info['side_1_coord_2']),\n",
        "            int(syn_info['side_1_coord_3'])\n",
        "        )\n",
        "        side2_coord = (\n",
        "            int(syn_info['side_2_coord_1']),\n",
        "            int(syn_info['side_2_coord_2']),\n",
        "            int(syn_info['side_2_coord_3'])\n",
        "        )\n",
        "\n",
        "        # Create the overlaid segmented cube with the additional mask based on segmentation_type\n",
        "        overlaid_cube = create_segmented_cube(\n",
        "            raw_vol=raw_vol,\n",
        "            seg_vol=seg_vol,\n",
        "            add_mask_vol=add_mask_vol,\n",
        "            central_coord=central_coord,\n",
        "            side1_coord=side1_coord,\n",
        "            side2_coord=side2_coord,\n",
        "            segmentation_type=self.segmentation_type,\n",
        "            subvolume_size=self.subvol_size,\n",
        "            alpha=self.alpha\n",
        "        )  # shape: (80, 80, 3, 80)\n",
        "\n",
        "        # We interpret the last dimension (depth) as frames\n",
        "        frames = []\n",
        "        for z in range(overlaid_cube.shape[3]):  # 80 slices\n",
        "            frame_rgb = overlaid_cube[..., z]  # (80, 80, 3)\n",
        "            frames.append(frame_rgb)\n",
        "\n",
        "        # Now reduce or expand to self.num_frames\n",
        "        total_slices = len(frames)  # 80\n",
        "        if total_slices < self.num_frames:\n",
        "            while len(frames) < self.num_frames:\n",
        "                frames.append(frames[-1])\n",
        "        elif total_slices > self.num_frames:\n",
        "            indices = np.linspace(0, total_slices - 1, self.num_frames, dtype=int)\n",
        "            frames = [frames[i] for i in indices]\n",
        "\n",
        "        # Process using the VideoMAEImageProcessor\n",
        "        inputs = self.processor(frames, return_tensors=\"pt\")\n",
        "        pixel_values = inputs[\"pixel_values\"].squeeze(0)  # (num_frames, 3, H, W)\n",
        "        pixel_values = pixel_values.float()\n",
        "\n",
        "        # Return pixel values, the corresponding DataFrame row, and the bbox name\n",
        "        return pixel_values, syn_info, bbox_name  # Return the pixel values, DataFrame row, and bbox_name\n",
        "\n",
        "def parse_args():\n",
        "    \"\"\"\n",
        "    Parse command-line arguments for configurable paths and training parameters.\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"VideoMAE Pre-training Script with Segmented Videos and Additional Masks\")\n",
        "\n",
        "    # Data directories\n",
        "    parser.add_argument('--raw_base_dir', type=str, default='raw', help='Path to raw data directory')\n",
        "    parser.add_argument('--seg_base_dir', type=str, default='seg', help='Path to segmentation data directory')\n",
        "    parser.add_argument('--add_mask_base_dir', type=str, default='', help='Path to additional masks directory')\n",
        "    parser.add_argument('--bbox_name', type=str, default=['bbox1'], help='Name of the bounding box directory')\n",
        "    parser.add_argument('--excel_file', type=str, default='', help='Excel file with synapse coordinates')\n",
        "    # Output and logging directories\n",
        "    parser.add_argument('--csv_output_dir', type=str, default='csv_outputs', help='Directory to save CSV outputs')\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints', help='Directory to save model checkpoints')\n",
        "    parser.add_argument('--log_dir', type=str, default='logs', help='Directory for TensorBoard logs')\n",
        "    parser.add_argument('--size',type=tuple,default=(80,80),help='Size of the image')\n",
        "    # Training parameters\n",
        "    parser.add_argument('--batch_size', type=int, default=2, help='Batch size for training')\n",
        "    parser.add_argument('--num_epochs', type=int, default=5, help='Number of training epochs')\n",
        "    parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate for optimizer')\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-2, help='Weight decay for optimizer')\n",
        "    parser.add_argument('--subvol_size', type=int, default=80, help='Size of the sub-volume to extract')\n",
        "    parser.add_argument('--num_frames', type=int, default=80, help='Number of frames per video clip')\n",
        "    parser.add_argument('--mask_ratio', type=float, default=0.75, help='Mask ratio for VideoMAE')\n",
        "    parser.add_argument('--patience', type=int, default=3, help='Patience for early stopping')\n",
        "    parser.add_argument('--resume_checkpoint', type=str, default=None, help='Path to resume checkpoint')\n",
        "\n",
        "    # GIF saving parameters\n",
        "    parser.add_argument('--save_gifs_dir', type=str, default='gifs', help='Directory to save sample GIFs')\n",
        "    parser.add_argument('--num_gifs', type=int, default=10, help='Number of sample GIFs to save')\n",
        "    parser.add_argument('--alpha', type=float, default=0.3, help='Transparency factor for segmentation overlay')\n",
        "    # New argument for segmentation type\n",
        "    parser.add_argument('--segmentation_type', type=int, default=0, choices=range(0, 6),\n",
        "                        help='Type of segmentation overlay:\\n'\n",
        "                             '0 = Raw image\\n'\n",
        "                             '1 = Raw + Side1\\n'\n",
        "                             '2 = Raw + Side2\\n'\n",
        "                             '3 = Raw + Side1 + Side2\\n'\n",
        "                             '4 = Raw + Vesicles\\n'\n",
        "                             '5 = Raw + Side1 + Side2 + Vesicles')\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        # Data directories\n",
        "        self.raw_base_dir = 'raw'\n",
        "        self.seg_base_dir = 'seg'\n",
        "        self.add_mask_base_dir = ''\n",
        "        self.bbox_name = ['bbox1']\n",
        "        self.excel_file = ''\n",
        "        self.checkpoint='hemibrain_production.checkpoint'\n",
        "        # Output directories\n",
        "        self.csv_output_dir = 'csv_outputs'\n",
        "        # self.checkpoint_dir = 'hemibrain_production.checkpoint'\n",
        "        self.log_dir = 'logs'\n",
        "        self.num_classes=7\n",
        "        # Training parameters\n",
        "        self.size = (80, 80)\n",
        "        self.batch_size = 2\n",
        "        self.num_epochs = 5\n",
        "        self.learning_rate = 1e-4\n",
        "        self.weight_decay = 1e-2\n",
        "        self.subvol_size = 80\n",
        "        self.num_frames = 80\n",
        "        self.mask_ratio = 0.75\n",
        "        self.patience = 3\n",
        "        self.resume_checkpoint = None\n",
        "\n",
        "        # GIF parameters\n",
        "        self.save_gifs_dir = 'gifs'\n",
        "        self.num_gifs = 10\n",
        "        self.alpha = 0.3\n",
        "        self.segmentation_type = 0\n",
        "        self.input_mask = True  # Set to True if you want masking\n",
        "        self.extract_features=True\n",
        "        self.num_workers=2\n",
        "        self.output_dir='output_seg0_grayscale'\n",
        "        self.grayscale=True\n",
        "        self.input_channels=1\n",
        "def main(args):\n",
        "    # Initialize processor\n",
        "    args=Args()\n",
        "    processor = SimpleVideoProcessor(size=(80, 80))\n",
        "\n",
        "    # List of all bboxes\n",
        "    # bboxes = [f\"bbox{i}\" for i in range(1, 8)]  # bbox1 to bbox7\n",
        "    bboxes =args.bbox_name # bbox1 to bbox7\n",
        "\n",
        "    # Load volumes for all bboxes\n",
        "    vol_data_dict = {}\n",
        "    for bbox_name in args.bbox_name:\n",
        "        print(bbox_name)\n",
        "        raw_vol, seg_vol, add_mask_vol = load_volumes(\n",
        "            bbox_name=bbox_name,\n",
        "            raw_base_dir=args.raw_base_dir,\n",
        "            seg_base_dir=args.seg_base_dir,\n",
        "            add_mask_base_dir=args.add_mask_base_dir\n",
        "        )\n",
        "        # print(raw_vol)\n",
        "        if raw_vol is not None and seg_vol is not None and add_mask_vol is not None:\n",
        "            vol_data_dict[bbox_name] = (raw_vol, seg_vol, add_mask_vol)\n",
        "        else:\n",
        "            print(f\"Skipping {bbox_name} due to missing volumes.\")\n",
        "\n",
        "    # Load synapse data for all bboxes\n",
        "    synapse_dfs = []\n",
        "    for bbox_name in bboxes:\n",
        "        excel_file_path = os.path.join(args.excel_file, f\"{bbox_name}.xlsx\")\n",
        "        if os.path.exists(excel_file_path):\n",
        "            df = pd.read_excel(excel_file_path)\n",
        "            df['bbox_name'] = bbox_name  # Add bbox_name column\n",
        "            synapse_dfs.append(df)\n",
        "        else:\n",
        "            print(f\"Excel file not found for {bbox_name}. Skipping.\")\n",
        "    syn_df = pd.concat(synapse_dfs, ignore_index=True)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = VideoMAEDataset(\n",
        "        vol_data_dict=vol_data_dict,\n",
        "        synapse_df=syn_df,\n",
        "        processor=processor,\n",
        "        segmentation_type=args.segmentation_type,\n",
        "        subvol_size=args.subvol_size,\n",
        "        num_frames=args.num_frames,\n",
        "        alpha=args.alpha\n",
        "    )\n",
        "\n",
        "    # Process cubes and collect synapse data\n",
        "    # cubes = []\n",
        "    # syn_info_list = []  # List to collect synapse information\n",
        "\n",
        "    # for idx in range(len(dataset)):\n",
        "    #     pixel_values, syn_info,bbox_name = dataset[idx]\n",
        "    #     cubes.append(pixel_values)\n",
        "\n",
        "    #     # Collect synapse info\n",
        "    #     syn_info_list.append(syn_info)\n",
        "\n",
        "    # # Merge all synapse info into a single DataFrame\n",
        "    # merged_syn_info = pd.DataFrame(syn_info_list)\n",
        "\n",
        "    # print(f\"Processed {len(cubes)} cubes successfully.\")\n",
        "    # # return cubes, merged_syn_info\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     args = parse_args()\n",
        "#     main(args)\n",
        "\n",
        "    # Initialize model\n",
        "    model = Vgg3D(\n",
        "        input_size=(args.num_frames, args.subvol_size, args.subvol_size),\n",
        "        input_fmaps=1,\n",
        "        output_classes=args.num_classes\n",
        "    )\n",
        "\n",
        "    # Load checkpoint\n",
        "    # if args.checkpoint:\n",
        "    state_dict = torch.load(\"/content/hemibrain_production.checkpoint\")\n",
        "    model.load_state_dict(state_dict)\n",
        "    logger.info(f\"Loaded weights from {args.checkpoint}\")\n",
        "\n",
        "    if args.extract_features:\n",
        "        # Get combined dataframe with features\n",
        "        combined_df = extract_features(model, dataset, args)\n",
        "\n",
        "        # Save results\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "        # Save full dataset with features\n",
        "        output_path = os.path.join(args.output_dir, \"metadata_with_features.csv\")\n",
        "        combined_df.to_csv(output_path, index=False)\n",
        "\n",
        "        logger.info(f\"Saved combined features and metadata to {output_path}\")\n",
        "        logger.info(f\"Dataset shape: {combined_df.shape}\")\n",
        "        logger.info(f\"Feature columns: {[c for c in combined_df.columns if c.startswith('feat_')]}\")\n",
        "\n",
        "    # Paths and directories\n",
        "    checkpoint_url = \"https://dl.dropboxusercontent.com/scl/fo/mfejaomhu43aa6oqs6zsf/AKMAAgT7OrUtruR0AQXZBy0/hemibrain_production.checkpoint.20220225?rlkey=6cmwxdvehy4ylztvsbgkfnrfc&dl=0\"\n",
        "    checkpoint_path = 'hemibrain_production.checkpoint'\n",
        "\n",
        "    # Download the checkpoint if it doesn't exist\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        os.system(f\"wget -O {checkpoint_path} '{checkpoint_url}'\")\n",
        "        print(\"Downloaded VGG3D checkpoint.\")\n",
        "    else:\n",
        "        print(\"VGG3D checkpoint already exists.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    main(args)\n",
        "\n",
        "\n",
        "# args = parse_args()\n",
        "# cubes , sys_inf= main(args)\n",
        "# print(f\"Processed {len(cubes)} cubes successfully.\")\n",
        "\n",
        "# bbox1\n",
        "# Processed 58 cubes successfully.\n",
        "# Processed 58 cubes successfully.\n",
        "\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import imageio\n",
        "\n",
        "# # After running main() and getting the cubes list\n",
        "# cube = cubes[0]  # Select the first cube (adjust index as needed)\n",
        "\n",
        "# # Define normalization parameters (must match what's used in SimpleVideoProcessor)\n",
        "# mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "# std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "# # Denormalize the tensor\n",
        "# denormalized_cube = cube * std + mean\n",
        "\n",
        "# # Clamp values to valid [0,1] range and convert to numpy\n",
        "# denormalized_cube = torch.clamp(denormalized_cube, 0, 1)\n",
        "# frames = denormalized_cube.permute(0, 2, 3, 1).numpy()  # Change to (T, H, W, C)\n",
        "# frames = (frames * 255).astype(np.uint8)  # Convert to 0-255\n",
        "\n",
        "# imageio.mimsave('synapse_cube.gif', frames, fps=10)\n",
        "\n",
        "# print(\"GIF saved successfully!\")\n",
        "\n",
        "# GIF saved successfully!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "QEtsOOJfm1oB",
        "outputId": "3be5d051-9aa4-4304-bfbc-e8af5d6df125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bbox1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-64b3fea4619f>:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\"/content/hemibrain_production.checkpoint\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-64b3fea4619f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-64b3fea4619f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0;31m# Load checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;31m# if args.checkpoint:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/hemibrain_production.checkpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded weights from {args.checkpoint}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m             \u001b[0moverall_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     warnings.warn(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SEg Mode =0 (raw)"
      ],
      "metadata": {
        "id": "QUvFkFyxmEMB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b1SFQ8K-8ZFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        # Data directories\n",
        "        self.raw_base_dir = 'raw'\n",
        "        self.seg_base_dir = 'seg'\n",
        "        self.add_mask_base_dir = ''\n",
        "        self.bbox_name = ['bbox1']\n",
        "        self.excel_dir = ''\n",
        "        self.checkpoint='hemibrain_production.checkpoint'\n",
        "        # Output directories\n",
        "        self.csv_output_dir = 'csv_outputs'\n",
        "        # self.checkpoint_dir = 'hemibrain_production.checkpoint'\n",
        "        self.log_dir = 'logs'\n",
        "        self.num_classes=7\n",
        "        # Training parameters\n",
        "        self.size = (80, 80)\n",
        "        self.batch_size = 2\n",
        "        self.num_epochs = 5\n",
        "        self.learning_rate = 1e-4\n",
        "        self.weight_decay = 1e-2\n",
        "        self.subvol_size = 80\n",
        "        self.num_frames = 80\n",
        "        self.mask_ratio = 0.75\n",
        "        self.patience = 3\n",
        "        self.resume_checkpoint = None\n",
        "\n",
        "        # GIF parameters\n",
        "        self.save_gifs_dir = 'gifs'\n",
        "        self.num_gifs = 10\n",
        "        self.alpha = 0.3\n",
        "        self.segmentation_type = 0\n",
        "        self.input_mask = True  # Set to True if you want masking\n",
        "        self.extract_features=True\n",
        "        self.num_workers=2\n",
        "        self.output_dir='output_seg0_grayscale'\n",
        "        self.grayscale=True\n",
        "        self.input_channels=1\n",
        "# In your notebook cell:\n",
        "args = Args()  # Create config object\n",
        "args.input_mask = True  # Optionally override parameters\n",
        "args.bbox_name = ['bbox1','bbox2','bbox3','bbox4','bbox5','bbox6']  # Example override\n",
        "\n",
        "# Run main program\n",
        "main(args)\n",
        "# print(f\"Processed {len(cubes)} cubes successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOulj7LKk7XX",
        "outputId": "cc89f7d3-35af-4a17-8644-bf6826e1d4c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-8ebce88baa15>:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.checkpoint)[\"model_state_dict\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        # Data directories\n",
        "        self.raw_base_dir = 'raw'\n",
        "        self.seg_base_dir = 'seg'\n",
        "        self.add_mask_base_dir = ''\n",
        "        self.bbox_name = ['bbox1']\n",
        "        self.excel_dir = ''\n",
        "        self.checkpoint='hemibrain_production.checkpoint'\n",
        "        # Output directories\n",
        "        self.csv_output_dir = 'csv_outputs'\n",
        "        # self.checkpoint_dir = 'hemibrain_production.checkpoint'\n",
        "        self.log_dir = 'logs'\n",
        "        self.num_classes=7\n",
        "        # Training parameters\n",
        "        self.size = (80, 80)\n",
        "        self.batch_size = 2\n",
        "        self.num_epochs = 5\n",
        "        self.learning_rate = 1e-4\n",
        "        self.weight_decay = 1e-2\n",
        "        self.subvol_size = 80\n",
        "        self.num_frames = 80\n",
        "        self.mask_ratio = 0.75\n",
        "        self.patience = 3\n",
        "        self.resume_checkpoint = None\n",
        "\n",
        "        # GIF parameters\n",
        "        self.save_gifs_dir = 'gifs'\n",
        "        self.num_gifs = 10\n",
        "        self.alpha = 0.3\n",
        "        self.segmentation_type = 0\n",
        "        self.input_mask = True  # Set to True if you want masking\n",
        "        self.extract_features=True\n",
        "        self.num_workers=2\n",
        "        self.output_dir='output_seg0_grayscale'\n",
        "        self.grayscale=True\n",
        "        self.input_channels=1\n",
        "# In your notebook cell:\n",
        "args = Args()  # Create config object\n",
        "args.input_mask = True  # Optionally override parameters\n",
        "args.bbox_name = ['bbox1','bbox2','bbox3','bbox4','bbox5','bbox6']  # Example override\n",
        "\n",
        "# Run main program\n",
        "main(args)\n",
        "# print(f\"Processed {len(cubes)} cubes successfully.\")"
      ],
      "metadata": {
        "id": "va_whStLmiQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FisID29Jmhq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dim Red and Visualize"
      ],
      "metadata": {
        "id": "HAeO_QEKZcF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a9ad72e-cc93-4b86-df4e-551365fb105d",
        "id": "CkPqaAsRZcGW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bbox1.xlsx  bbox6.xlsx\t\t\t     __MACOSX\t      vit_umap_x_vs_y.html\n",
            "bbox2.xlsx  bbox7.xlsx\t\t\t     raw\t      vit_umap_x_vs_z.html\n",
            "bbox3.xlsx  csv_outputs\t\t\t     sample_data      vit_umap_y_vs_z.html\n",
            "bbox4.xlsx  downloaded_file.zip\t\t     seg\n",
            "bbox5.xlsx  hemibrain_production.checkpoint  vit_umap3d.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import umap.umap_ as umap\n",
        "import plotly.express as px\n",
        "\n",
        "merged_csv = '/content/output/metadata_with_features.csv'\n",
        "df = pd.read_csv(merged_csv)\n",
        "\n",
        "feat_cols = [c for c in df.columns if c.startswith('feat_')]\n",
        "X = df[feat_cols].values  # shape: [N, hidden_size] (e.g. [N, 768])\n",
        "\n",
        "pca = PCA(n_components=7, random_state=42)\n",
        "X_pca = pca.fit_transform(X)  # shape => [N, 50]\n",
        "\n",
        "# 4) UMAP from 50 -> 3 dims (for 3D visualization)\n",
        "umap_3d = umap.UMAP(\n",
        "    n_components=3,\n",
        "    n_neighbors=15,     # can tune\n",
        "    min_dist=0.1,       # can tune\n",
        "    random_state=42\n",
        ")\n",
        "X_umap3 = umap_3d.fit_transform(X_pca)  # shape => [N, 3]\n",
        "\n",
        "# 5) Add UMAP coordinates back to the DataFrame\n",
        "df['umap_x'] = X_umap3[:,0]\n",
        "df['umap_y'] = X_umap3[:,1]\n",
        "df['umap_z'] = X_umap3[:,2]\n",
        "\n",
        "fig = px.scatter_3d(\n",
        "    df,\n",
        "    x='umap_x',\n",
        "    y='umap_y',\n",
        "    z='umap_z',\n",
        "    color='bbox_name',\n",
        "    hover_data=['central_coord_1', 'central_coord_2', 'central_coord_3']\n",
        ")\n",
        "fig.update_traces(marker=dict(size=3))\n",
        "fig.update_layout(width=800, height=600)\n",
        "fig.write_html(\"VGG_umap3d.html\")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "b590d590-84b7-4150-c173-97784ed4563f",
        "id": "RhywJBfCZcGa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
            "\n",
            "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "\n",
            "/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning:\n",
            "\n",
            "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"ea539072-177f-4406-9a28-fed1d2d3ec0c\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ea539072-177f-4406-9a28-fed1d2d3ec0c\")) {                    Plotly.newPlot(                        \"ea539072-177f-4406-9a28-fed1d2d3ec0c\",                        [{\"customdata\":[[171,260,350],[223,113,425],[280,102,377],[455,131,162],[138,121,302],[122,113,325],[175,257,297],[300,383,404],[274,158,188],[467,348,242],[467,390,244],[441,362,255],[340,188,269],[344,376,359],[357,329,309],[467,425,318],[138,307,185],[235,303,300],[248,286,293],[113,253,403],[165,367,392],[156,287,360],[130,247,389],[252,360,356],[453,386,324],[396,105,261],[394,284,259],[397,318,361],[247,225,192],[412,338,290],[440,288,256],[169,336,146],[252,377,268],[468,424,292],[460,396,339],[459,414,350],[198,300,322],[155,353,165],[150,366,158],[156,330,163],[132,293,179],[181,373,180],[161,353,195],[195,409,195],[211,437,195],[189,241,235],[418,219,199],[352,337,341],[203,347,359],[264,444,469],[177,266,345],[269,466,454],[169,322,354],[367,354,369],[339,370,395],[278,385,395],[252,388,395],[253,343,337]],\"hovertemplate\":\"bbox_name=bbox1\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eumap_z=%{z}\\u003cbr\\u003ecentral_coord_1=%{customdata[0]}\\u003cbr\\u003ecentral_coord_2=%{customdata[1]}\\u003cbr\\u003ecentral_coord_3=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox1\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"bbox1\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[6.320848,6.868704,6.527309,5.7201967,5.504691,6.368925,5.584473,6.5815415,6.0709996,5.547961,6.8613815,5.539868,7.9374447,5.977072,7.0375266,5.73709,7.845846,5.671688,5.7251077,7.393874,5.9098163,7.603018,7.804602,6.8634253,7.083628,6.8541102,5.6302857,7.454782,6.653799,6.8165684,7.8831396,7.42407,6.6293106,7.413919,5.8100405,5.655298,5.9776444,6.7837343,5.727114,7.1478252,7.5097566,5.7703094,6.0439177,7.275848,7.187259,5.7678304,5.625099,6.057722,7.1064067,7.548423,7.4722323,7.786518,6.8941984,6.349089,5.96265,7.2368374,7.354787,6.1669087],\"y\":[1.269926,1.4927359,1.1532238,0.86782265,1.3223397,1.5089643,1.3659642,1.9268119,0.69472766,1.1263334,0.99016833,1.4593079,1.703558,1.3703259,1.7580043,0.7027388,2.069424,1.2774208,1.2710545,1.0835494,0.6930185,1.2790532,2.5208964,1.1153023,1.4296037,0.6846901,1.0070198,1.4984587,1.6841856,0.80536443,1.3456845,1.1058097,1.8387506,2.3963609,1.0435407,1.220936,0.84920436,1.3811061,1.417144,1.444629,1.5773708,0.85543925,1.0315205,1.7811003,0.9843891,1.0211095,1.2627047,0.86874527,0.96847725,1.5128411,1.5924027,1.953415,0.7599589,0.62437075,0.72866863,1.4485724,1.5667044,0.79082656],\"z\":[5.035003,5.2424083,5.1918364,5.529476,5.664883,5.062537,5.614609,5.0859056,5.7297487,5.7426186,5.5077996,5.5896807,5.9728303,5.3389797,5.332006,6.032172,5.9234996,5.6477523,5.633202,5.906338,5.875859,6.910996,6.5052147,5.406056,5.4760785,6.129239,6.1408753,5.7478337,5.1371937,5.8785195,6.480586,6.408336,5.3317175,5.677474,5.7257857,5.7948713,6.1833944,5.6608124,5.7642407,5.9678907,6.055814,5.77364,6.066858,5.790117,6.3045444,5.923527,5.973696,5.4473243,5.9144163,6.250254,6.79778,6.771094,6.63371,6.4336576,6.395868,6.3105893,7.336922,6.59581],\"type\":\"scatter3d\"},{\"customdata\":[[325,193,399],[376,381,205],[355,400,190],[392,381,190],[345,121,172],[345,140,164],[453,193,491],[201,193,424],[185,106,386],[167,187,299],[474,109,332],[215,331,375],[285,354,304],[196,359,243],[168,471,253],[171,445,292],[134,439,122],[101,221,196],[470,227,383],[256,266,339],[261,225,283],[208,460,163],[169,443,213],[473,303,277],[435,307,228],[248,294,225],[178,260,228],[123,124,201],[143,130,259],[188,160,327],[231,223,344],[211,210,353],[202,263,212],[198,295,203],[187,116,334],[230,201,329],[142,288,346],[205,205,292],[253,239,263],[281,255,313],[315,191,332],[286,109,142],[457,242,189],[418,206,196],[359,174,333],[343,141,327],[242,211,341],[421,224,215],[240,276,303],[127,241,191],[457,302,370],[459,243,386],[235,277,409],[111,275,149],[167,230,326],[140,192,105],[278,122,163],[107,212,160],[148,430,169],[183,433,172],[185,455,138],[188,447,190],[156,425,190],[205,112,306],[230,112,334],[230,106,308],[466,304,304],[438,248,121],[457,225,212],[426,308,253],[218,239,327],[380,188,315],[353,188,316],[297,257,340],[457,243,329],[169,277,297],[152,281,306],[152,258,317],[310,260,340],[408,235,343],[433,221,345],[442,202,345],[349,216,235],[437,290,213],[256,335,276],[160,379,384],[172,201,318],[316,170,334],[254,249,341],[246,253,352],[197,167,338],[228,248,279],[137,115,273],[468,290,295],[472,300,277],[135,127,250],[190,241,190],[360,238,253],[221,233,306],[234,207,245]],\"hovertemplate\":\"bbox_name=bbox2\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eumap_z=%{z}\\u003cbr\\u003ecentral_coord_1=%{customdata[0]}\\u003cbr\\u003ecentral_coord_2=%{customdata[1]}\\u003cbr\\u003ecentral_coord_3=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox2\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"bbox2\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[6.471332,7.5181804,6.8196044,7.1974874,6.0142126,6.4876976,7.136677,6.6587753,5.6834297,7.0344157,6.272738,7.0256357,6.322197,6.745193,6.142396,7.042722,6.3555856,6.531251,7.272558,6.827308,5.869167,6.0442777,6.3084316,6.022226,6.66891,6.550416,5.9476347,7.1514087,6.5631356,6.698946,5.925877,6.4896374,7.283149,6.410516,5.743928,7.3338947,6.6301923,6.5739636,7.2799606,6.2305193,6.6339893,6.408582,5.8733983,6.067681,6.0072026,7.0327215,5.886594,6.6982126,6.2971234,6.7953167,7.000367,7.0031757,7.177908,6.6612144,6.705218,6.577227,6.811989,5.8185253,6.991147,6.618037,6.862479,6.3358097,5.721546,6.3934493,6.0909424,5.8245454,6.564363,6.8570747,6.3224053,6.2827277,6.881747,6.092355,6.065134,6.3030496,5.9195414,5.5396104,5.898544,6.642313,6.7227163,6.6150155,6.28568,6.9710894,6.74594,6.098579,6.474288,6.0101795,6.029689,5.860342,6.8189664,6.6419573,5.9279537,6.186878,6.659691,5.9242744,5.96739,6.434012,6.252948,6.0649095,6.628577,6.2640266],\"y\":[0.7944705,2.1648886,1.1718825,1.2835804,0.8799315,0.93951553,2.7011032,1.0864222,1.0699046,2.1004853,1.0581764,2.0663826,0.862092,1.0576993,1.0402467,2.747193,1.2094586,2.048154,1.7590544,1.5597826,1.0201838,1.0595052,1.093412,1.143118,1.3962892,1.1781464,2.3989115,2.326682,1.6728816,1.9631786,1.3093699,1.2654364,2.1666765,1.2230918,1.2943376,2.1740785,2.0223427,2.539319,2.0341284,1.6906875,1.5493609,1.9134668,1.7847602,1.4358768,1.471908,1.7919289,1.5929948,2.2551508,2.0927644,2.2863226,2.4061253,1.8521435,2.1170557,1.4517274,2.5414474,2.5423586,2.2169292,1.3557911,1.449908,1.9698429,2.6501026,1.6459545,1.964762,2.529689,1.7304962,1.6374708,2.2935712,2.4429016,2.3806498,2.0209966,2.2303169,1.6711258,2.366551,2.2373788,2.1091962,1.7713856,2.183078,1.8980925,2.1625881,2.082995,2.418133,1.6663271,1.8762665,2.0284035,2.205503,2.4090016,2.2241273,1.8926041,1.7279217,1.8698736,2.1953573,2.565536,2.0111895,2.1620839,1.901595,1.5773185,1.8034174,1.7004234,2.0881076,1.8560284],\"z\":[6.326063,6.9599447,6.8664436,6.945028,6.2499523,6.4762917,6.1579237,7.055593,6.2574477,7.479023,6.272149,7.2713804,5.8799777,6.352491,5.8582554,6.685192,7.1431885,7.3728633,6.838344,6.791372,6.556036,6.458795,6.7473373,6.383667,7.0697064,6.8196683,6.6358175,6.6040907,7.1285863,7.2127585,6.744717,6.838003,5.912319,6.7256813,6.4736905,6.7729535,6.8277216,7.028124,6.5431223,7.0973277,6.6809044,6.967142,6.8304167,6.5903654,6.4931145,6.6040554,6.847252,7.1217,6.951928,6.794158,6.2832108,5.985091,6.1200767,6.6113234,5.9854255,6.683618,6.4851513,6.4572477,6.406686,6.428871,6.331547,6.594099,6.685112,6.2028675,7.022855,6.81492,6.1181498,5.8165913,6.6808424,6.0758014,5.9262595,6.5408187,6.5603914,6.4436407,6.4843416,6.3322062,6.726312,6.3398266,5.576062,6.155062,6.149462,6.2588873,5.9452724,6.1830163,5.865547,5.9110947,6.184998,6.4239445,6.021743,5.99811,6.2235336,6.0570884,5.802744,6.058542,6.462475,6.397371,6.3199425,6.225245,6.437115,5.9102387],\"type\":\"scatter3d\"},{\"customdata\":[[351,290,458],[291,226,455],[153,190,413],[235,141,244],[196,212,251],[453,467,203],[206,84,206],[404,219,195],[253,467,187],[368,422,436],[398,469,398],[416,344,328],[273,205,278],[403,126,213],[473,203,193],[464,200,192],[478,208,193],[183,401,374],[160,357,445],[227,409,415],[223,388,446],[135,350,449],[234,372,442],[403,401,365],[333,310,280],[347,281,264],[264,202,224],[232,104,366],[310,194,409],[339,160,350],[193,180,345],[224,163,273],[286,145,229],[300,188,271],[291,233,243],[290,476,309],[278,478,341],[431,440,417],[194,388,426],[214,425,389],[133,233,417],[132,268,417],[136,144,438],[253,237,468],[266,248,468],[255,211,277],[331,337,300],[366,248,256],[282,143,258],[269,134,252],[253,223,249],[185,189,259],[172,207,258],[110,349,293],[325,371,279],[320,322,283],[287,322,181],[426,221,182],[461,323,170],[384,179,101],[404,190,101],[360,109,91]],\"hovertemplate\":\"bbox_name=bbox3\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eumap_z=%{z}\\u003cbr\\u003ecentral_coord_1=%{customdata[0]}\\u003cbr\\u003ecentral_coord_2=%{customdata[1]}\\u003cbr\\u003ecentral_coord_3=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox3\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"bbox3\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[6.883459,6.6475296,5.8160086,6.370837,5.8936477,6.402843,6.0717006,5.956161,6.043161,6.5802774,6.90029,6.5803432,6.7610893,6.714982,5.965941,6.169317,6.5512986,6.6295857,6.604784,6.7101054,6.615584,6.0637126,6.599324,6.1229477,6.408752,6.659329,6.453432,5.884198,6.7596426,6.4507174,6.731519,6.311013,6.2546487,6.6682267,6.824485,6.013682,6.178463,6.419103,6.633221,6.112539,6.253562,6.630824,6.7976365,6.2445617,6.8099837,6.364212,6.350959,6.4317384,6.0656176,6.2572565,6.7914867,6.5521812,6.12482,6.3774133,6.3220057,6.449023,6.213191,6.207129,6.29685,5.97172,6.6198497,6.520426],\"y\":[1.8457015,1.686475,1.8737981,2.2299533,1.8289009,0.6388161,1.3299502,1.7313812,1.7509884,1.5141133,1.3258698,1.4978921,0.8064798,1.1638875,1.7254692,1.5907732,0.88944143,0.8567526,1.531599,1.1478052,0.8017769,0.6001686,1.4490206,1.5194066,1.8443745,0.8697635,1.3961837,1.4920034,1.6265417,1.2887223,1.3745407,1.2895213,0.9368249,1.6376885,0.99691415,1.0752714,0.6379779,1.1098622,1.0169189,1.5988492,1.2196866,0.76476985,1.2046639,1.3135325,1.7057936,0.50954324,0.5841611,1.1370591,1.1295274,0.6477655,1.1357856,1.2876352,1.0257604,0.99883974,1.0728536,0.9430559,0.68055946,1.6398295,1.1454122,0.72018844,1.3482691,1.3218886],\"z\":[5.7837157,6.2756457,6.3682375,5.7420506,5.977252,6.057221,6.36302,6.4396796,6.634494,6.1962075,5.943683,6.375511,5.6305757,6.0236764,6.213812,6.099872,6.304055,6.1396356,5.904754,5.770087,5.0830474,6.100108,6.142657,6.1587424,5.8253403,5.686188,6.1782293,6.180106,5.6642966,5.822932,5.7539964,6.209315,5.7952657,5.7016587,5.909398,6.0613933,5.675005,6.0899954,5.928738,5.7213287,6.0380397,5.387316,5.38361,4.8597703,5.4195833,5.351725,5.2805867,5.6707196,5.76454,5.3003936,5.774347,5.056572,5.486849,5.1733623,5.5742745,5.3863373,5.4927235,5.8335533,5.151239,5.4143324,5.1814756,5.274105],\"type\":\"scatter3d\"},{\"customdata\":[[342,215,386],[268,223,306],[349,190,186],[401,184,119],[327,248,200],[330,352,397],[358,189,381],[164,298,320],[223,226,403],[233,337,329],[234,306,390],[226,267,373],[239,246,373],[264,237,327],[294,218,314],[358,324,328],[368,337,328],[460,425,324],[212,388,316],[247,350,316],[166,298,336],[184,318,141],[142,175,101],[162,148,111],[147,192,152],[126,210,152],[132,194,155],[140,173,155],[82,217,131],[179,162,289],[111,208,124],[315,120,193],[301,450,294],[318,474,299],[303,355,324],[327,361,320],[350,342,354],[346,416,290],[241,282,234],[193,148,266]],\"hovertemplate\":\"bbox_name=bbox4\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eumap_z=%{z}\\u003cbr\\u003ecentral_coord_1=%{customdata[0]}\\u003cbr\\u003ecentral_coord_2=%{customdata[1]}\\u003cbr\\u003ecentral_coord_3=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox4\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"bbox4\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[6.249248,6.1131454,6.0224733,6.428279,6.4086185,6.1801815,5.8169684,6.378914,6.325536,5.993609,6.46431,6.096682,5.9950237,6.1605315,5.918073,6.086242,6.3906775,6.4350557,6.190655,6.3768654,5.4771075,6.2515664,6.1293797,6.1048007,6.2216773,6.013161,6.0574145,6.0381093,5.8084116,6.061588,6.267484,5.9901223,5.8095207,5.8964863,5.875873,5.7806754,5.874742,6.0463257,5.655362,5.648787],\"y\":[0.8739907,1.1295762,0.9395399,2.0209148,1.8136376,0.9903561,2.1734476,1.4791936,1.5725201,2.1885688,1.8121874,1.0148607,1.2110313,1.0932102,2.124795,2.0910962,1.6431801,1.4424651,1.2634959,1.6156607,2.1258335,1.8718227,1.6466628,2.256527,1.9614286,2.0752647,1.7271669,1.6209544,1.3928841,1.7652967,1.975083,1.8527673,1.7297674,1.9683955,1.9491774,2.1201525,1.3256266,1.7156023,2.2603695,1.7769926],\"z\":[5.3843117,5.9491315,5.5892134,5.7009206,5.270047,5.554914,5.809518,5.392431,5.5581436,5.083759,5.4952717,5.0590787,5.2338133,5.0396967,5.763001,5.535059,4.9971585,5.301975,5.437281,5.360623,5.9235463,5.0552354,5.358806,5.396616,5.027365,5.312774,5.858646,5.46526,5.275495,5.741008,5.181814,5.2399077,5.867944,5.191258,5.5590477,5.8755336,5.2332296,5.514091,5.4478464,5.7356915],\"type\":\"scatter3d\"},{\"customdata\":[[436,469,413],[170,441,334],[459,359,387],[401,423,411],[88,420,395],[297,396,199],[129,172,104],[109,205,389],[422,286,156],[405,404,266],[399,296,161],[383,232,173],[428,300,116],[457,288,116],[386,455,333],[155,214,455],[264,185,403],[222,317,252],[389,418,266],[288,302,221],[225,289,242],[130,352,224],[130,312,230],[173,253,220],[155,210,199],[362,179,149],[121,254,140],[157,234,136],[204,217,136],[190,211,156],[138,212,160],[219,215,158],[210,219,193],[275,272,204],[138,410,203],[176,401,226],[312,372,207],[254,243,187],[406,333,354],[129,337,256],[313,131,432],[388,123,391],[356,168,398],[397,203,452],[248,295,431],[163,433,371],[113,231,415],[127,421,304],[287,103,330],[269,107,340],[447,129,185],[334,127,210],[328,128,256],[309,158,256],[149,136,221],[348,112,221],[274,126,289],[274,153,285],[135,221,232],[272,321,119],[309,339,124],[433,234,218],[446,232,192],[445,199,146],[456,209,140],[408,171,101],[385,193,111],[361,223,105],[402,154,113],[350,178,116],[338,192,120],[395,201,117],[472,348,223],[400,293,331],[435,343,281],[399,396,309],[403,475,333],[385,474,339],[424,470,327],[442,464,274],[448,376,249],[413,400,286],[434,382,262],[423,394,257],[379,458,257],[158,430,397]],\"hovertemplate\":\"bbox_name=bbox5\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eumap_z=%{z}\\u003cbr\\u003ecentral_coord_1=%{customdata[0]}\\u003cbr\\u003ecentral_coord_2=%{customdata[1]}\\u003cbr\\u003ecentral_coord_3=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox5\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"bbox5\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[5.9738746,5.601307,5.6571994,6.103901,5.8813534,5.5591617,5.467118,5.1222806,5.443731,5.656317,5.3800054,5.7825794,5.777619,5.7291045,5.723026,5.3576407,5.656458,5.098737,5.4991775,5.5062456,5.334042,5.42541,5.428143,5.685484,5.4760995,5.297463,5.5537944,5.6257753,5.4340334,7.0386553,7.161118,5.079734,5.849782,5.19003,6.066254,5.0431232,7.295348,4.9668546,5.842884,5.4071603,5.8727407,5.337935,5.7475967,5.1678505,6.7131124,7.6985884,4.491574,4.7941628,6.4862995,5.5549474,5.346467,6.0116234,4.2672005,5.1481633,5.4135695,5.235376,5.0175576,5.0662208,4.604719,5.178563,4.9820986,5.253937,5.5445843,5.7992177,5.487877,5.2713995,4.947555,4.9786363,5.9010663,5.568516,5.1878967,4.9511094,5.612332,5.3764815,5.168915,4.808191,5.1414795,5.209918,4.944958,5.4503384,5.609389,5.354393,5.426832,4.8942533,5.1652775,5.603424],\"y\":[1.2354586,2.1958086,1.95095,1.8806335,1.6613055,1.7348299,1.8410894,1.785179,2.044951,1.6531831,1.9505576,1.5341187,1.0755742,1.8902828,1.5393391,1.9683665,1.3752367,1.7046607,1.7065334,1.4406333,1.8443403,1.4468153,1.7080082,1.2467192,1.5985398,1.4973326,1.411685,1.1946311,1.30362,2.300223,0.7707788,1.5616124,0.622007,1.3401212,0.06638969,1.6045223,1.3427358,1.6150775,1.7267298,0.47594178,1.9717605,0.62471974,2.761084,2.1802118,2.1691723,2.5683339,0.94310725,0.3068035,0.7163847,0.38312823,0.68404746,2.4408448,1.3087096,0.9991446,-0.021084063,0.5013392,0.7861882,0.56467265,1.2663199,0.275978,0.45628938,0.013077659,-0.15172225,-0.009798002,0.23991103,0.1832353,0.25381464,-0.047289312,0.77468413,0.1761033,0.3330751,1.593604,0.26586053,0.79371166,0.07373436,0.54997814,0.43319124,1.5541425,2.3159614,1.0535835,0.5651438,0.020549921,0.11775613,0.8379129,0.3904481,2.5239367],\"z\":[4.810584,5.768322,5.221465,5.1326475,5.3920774,5.48416,5.826811,5.667678,5.435934,4.97268,5.780829,4.908673,5.2530484,4.992185,5.0420613,5.3847103,5.295141,5.148041,5.6463146,4.956923,5.182929,4.980341,5.4177723,5.030325,5.3770924,5.4831767,5.389746,5.212768,5.149822,5.254217,5.248027,6.486047,4.7757826,6.7926083,5.783696,6.4009733,5.1335106,6.183709,4.5040636,5.442799,4.548631,5.4095354,5.8683534,6.2958717,8.085125,7.78085,5.336691,6.1890497,7.6991615,4.5737586,4.661208,7.9977074,5.9447904,7.378923,5.1182604,4.7024083,4.8692904,4.9819856,6.9794936,6.719738,5.0835376,5.1953464,5.8431687,6.451851,4.7876377,4.9404683,5.6743,5.8849764,7.526801,6.7478104,5.0777497,7.178527,4.9181314,7.347968,6.2802877,5.2767973,5.1534743,7.3990526,7.0260773,7.4498677,7.2074065,5.5724335,5.2833233,6.927157,5.742641,7.457143],\"type\":\"scatter3d\"},{\"customdata\":[[353,102,250],[116,308,172],[140,406,197],[357,330,268],[148,170,139],[151,198,139],[157,188,176],[175,216,172],[172,248,167],[131,256,179],[110,225,185],[157,192,191],[166,264,195],[211,262,199],[200,153,206],[205,143,206],[212,135,206],[207,192,219],[259,267,233],[286,266,226],[357,231,261],[209,305,197],[228,328,195],[207,281,213],[223,340,210],[275,336,222],[155,298,148],[292,353,212],[327,375,212],[347,348,212],[237,383,193],[232,388,170],[249,376,168],[182,366,168],[295,396,133],[314,464,151],[163,267,152],[157,235,152],[225,129,142],[148,134,141],[163,203,157],[141,177,157],[168,147,125],[164,264,147],[306,403,119],[269,411,119],[163,174,110],[286,294,230],[261,316,235],[313,351,235],[281,377,235],[232,351,233],[346,313,235],[344,302,244],[309,357,246],[275,336,248],[262,321,248],[254,265,248],[282,196,248],[351,165,264],[307,265,332],[302,362,332],[475,398,271],[285,324,253],[271,303,253],[330,297,253],[364,324,253],[218,314,257],[275,382,255],[282,184,253],[291,243,273],[247,342,283],[323,124,272],[317,270,272],[388,148,276],[354,155,285],[329,194,285],[342,205,285],[314,231,289],[280,236,290],[245,361,296],[392,241,322],[332,279,296],[462,168,303],[418,251,332],[456,137,329],[153,94,337],[356,220,347],[341,424,347],[468,125,358],[422,121,354],[398,476,410],[378,442,377],[428,162,416],[420,193,407],[392,460,427],[418,136,392],[202,266,389]],\"hovertemplate\":\"bbox_name=bbox6\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eumap_z=%{z}\\u003cbr\\u003ecentral_coord_1=%{customdata[0]}\\u003cbr\\u003ecentral_coord_2=%{customdata[1]}\\u003cbr\\u003ecentral_coord_3=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox6\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\",\"size\":3},\"mode\":\"markers\",\"name\":\"bbox6\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[5.5516405,4.7182803,4.660108,4.518363,5.7620873,6.382896,5.227513,5.5483947,5.8431516,4.930075,6.7839684,4.4657154,6.740739,5.6334734,5.5762324,4.9351544,4.833312,6.4615545,5.79223,5.2197466,5.8570433,6.443044,5.2295094,4.887509,5.7994885,7.1796155,4.7778473,5.8613987,6.53244,5.7513614,7.6550727,5.551396,4.9879117,5.3967752,5.310584,5.69732,4.4922686,7.0045495,6.9170647,6.0404882,4.733388,5.046545,5.72999,5.7402034,4.5323014,4.5541058,6.0729065,5.073003,4.8287435,6.232051,6.5645237,4.7437863,7.357915,5.0111313,4.664418,6.280439,6.8772693,4.7838674,6.0399632,4.711765,6.3515296,4.746116,6.0598435,4.7348275,4.8940277,4.9978385,4.8542843,4.83325,4.580312,5.164249,5.31132,5.096924,5.027365,4.854425,4.945194,4.8034296,6.5321317,4.9198084,5.212032,5.1563935,6.6642323,7.346506,5.240856,5.122102,5.455065,5.2771926,5.971975,5.0602703,8.08123,5.075455,5.3909984,5.800288,5.916781,5.729084,5.3790026,6.3238587,7.3313413,6.306697],\"y\":[3.227021,1.6358249,0.66898304,0.72642076,2.3974304,3.3598304,1.3008735,1.7811091,1.6631522,1.9048952,2.9439027,1.402845,3.6309,1.5611838,2.6700857,1.8447746,2.3611863,3.5685654,3.1201096,2.1629357,3.25016,2.328182,2.59974,1.3568105,2.6606338,3.3667607,0.81062895,2.809072,3.3910985,1.6149882,3.246594,1.0046537,2.1780543,0.8258995,2.866013,0.4435001,1.8626055,1.9411737,3.2940218,2.2883863,0.7778869,2.6574953,0.37146613,3.0781379,1.9038953,1.0750041,3.3617382,2.6073484,0.8229247,2.7971292,1.9524784,1.3268062,2.3604314,0.9404065,1.4119037,1.5751576,2.5936174,1.1025264,2.9585974,1.0577112,1.8039218,1.9831907,1.1003261,2.0808558,0.6611317,1.7542853,1.240304,1.3685106,1.4700996,0.7307389,0.41031393,0.84627247,0.7768766,1.7600399,1.5335617,1.0705366,1.1148101,1.5478966,1.2163827,0.82278734,0.6554125,1.120802,0.8775195,1.0931414,0.7791284,0.3260666,0.09897812,1.2741694,2.3373797,0.8476316,0.38650313,0.027244566,0.035219327,0.28895354,0.49221024,0.06965756,0.46060064,0.006996735],\"z\":[6.3445544,6.9583397,5.681394,6.139238,7.6409755,5.244455,7.07718,7.510227,7.68656,6.8033724,4.843742,6.101495,6.9824667,4.32593,4.873795,6.7886834,6.42131,6.0170546,6.958283,7.128164,5.635093,4.457616,6.9471254,6.7016373,4.7627954,5.556624,6.0622745,7.2830505,6.777718,4.3181605,6.0667667,4.42154,6.653395,4.4562583,6.543924,4.6817265,6.469272,4.521957,5.7280216,4.4136744,6.638803,5.860805,4.8853345,6.177097,5.8538537,5.524816,6.3107963,6.211399,5.4432397,4.9347,4.528188,5.5164137,4.866807,6.7566867,6.287085,4.2625403,4.986957,5.5821724,5.343608,6.379747,4.3832693,6.2734766,4.324419,6.177731,6.130877,6.5714707,6.0513854,6.143765,5.6975393,5.368249,5.552102,5.873147,6.003284,6.3846593,5.86478,5.73287,4.465147,6.024478,6.7477264,5.756844,4.6815677,4.8914404,6.691334,6.4059734,6.9376597,5.9376945,5.3152246,6.5422006,5.576097,6.487182,6.1984415,5.831464,5.9725738,6.5895286,6.195723,6.2208967,5.606689,6.0136857],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"umap_x\"}},\"yaxis\":{\"title\":{\"text\":\"umap_y\"}},\"zaxis\":{\"title\":{\"text\":\"umap_z\"}}},\"legend\":{\"title\":{\"text\":\"bbox_name\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"width\":800,\"height\":600},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ea539072-177f-4406-9a28-fed1d2d3ec0c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# 1) UMAP (x vs. y)\n",
        "fig_xy = px.scatter(\n",
        "    df,\n",
        "    x=\"umap_x\",\n",
        "    y=\"umap_y\",\n",
        "    color=\"bbox_name\",  # color by bbox_name => discrete legend\n",
        "    title=\"UMAP (x vs y)\",\n",
        "    hover_data=[\"umap_x\", \"umap_y\", \"bbox_name\", \"Var1\"]\n",
        ")\n",
        "fig_xy.write_html(\"VGG_umap_x_vs_y.html\")\n",
        "\n",
        "fig_xy.show()\n",
        "\n",
        "# 2) UMAP (x vs. z)\n",
        "fig_xz = px.scatter(\n",
        "    df,\n",
        "    x=\"umap_x\",\n",
        "    y=\"umap_z\",\n",
        "    color=\"bbox_name\",\n",
        "    title=\"UMAP (x vs z)\",\n",
        "    hover_data=[\"umap_x\", \"umap_z\", \"bbox_name\", \"Var1\"]\n",
        ")\n",
        "fig_xz.write_html(\"VGG_umap_x_vs_z.html\")\n",
        "\n",
        "fig_xz.show()\n",
        "\n",
        "# 3) UMAP (y vs. z)\n",
        "fig_yz = px.scatter(\n",
        "    df,\n",
        "    x=\"umap_y\",\n",
        "    y=\"umap_z\",\n",
        "    color=\"bbox_name\",\n",
        "    title=\"UMAP (y vs z)\",\n",
        "    hover_data=[\"umap_y\", \"umap_z\", \"bbox_name\", \"Var1\"]\n",
        ")\n",
        "fig_xz.write_html(\"VGG_umap_y_vs_z.html\")\n",
        "\n",
        "fig_yz.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be73f205-fbeb-463a-9fc7-e4026a0e7a59",
        "id": "TklvLqH8ZcGa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"a718abb7-e083-4e0c-a1a3-c25f09bd7169\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a718abb7-e083-4e0c-a1a3-c25f09bd7169\")) {                    Plotly.newPlot(                        \"a718abb7-e083-4e0c-a1a3-c25f09bd7169\",                        [{\"customdata\":[[\"bbox1\",\"non_spine_synapsed_056\"],[\"bbox1\",\"non_spine_synapse_057\"],[\"bbox1\",\"non_spine_synapse_058\"],[\"bbox1\",\"non_spine_synapse_063\"],[\"bbox1\",\"non_spine_synapse_062\"],[\"bbox1\",\"non_spine_synapse_061\"],[\"bbox1\",\"non_spine_synapse_060\"],[\"bbox1\",\"non_spine_synapse_059\"],[\"bbox1\",\"non_spine_synapse_054\"],[\"bbox1\",\"non_spine_synapse_053\"],[\"bbox1\",\"non_spine_synapse_052\"],[\"bbox1\",\"non_spine_synapse_051\"],[\"bbox1\",\"non_spine_synapse_050\"],[\"bbox1\",\"non_spine_synapse_049\"],[\"bbox1\",\"non_spine_synapse_048\"],[\"bbox1\",\"non_spine_synapse_047\"],[\"bbox1\",\"non_spine_synapse_046\"],[\"bbox1\",\"non_spine_synapse_045\"],[\"bbox1\",\"non_spine_synapse_044\"],[\"bbox1\",\"non_spine_synapse_040\"],[\"bbox1\",\"non_spine_synapse_038\"],[\"bbox1\",\"non_spine_synapse_037\"],[\"bbox1\",\"non_spine_synapse_036\"],[\"bbox1\",\"non_spine_synapse_035\"],[\"bbox1\",\"non_spine_synapse_034\"],[\"bbox1\",\"non_spine_synapse_033\"],[\"bbox1\",\"non_spine_synapse_032\"],[\"bbox1\",\"non_spine_synapse_031\"],[\"bbox1\",\"non_spine_synapse_030\"],[\"bbox1\",\"non_spine_synapse_029\"],[\"bbox1\",\"non_spine_synapse_028\"],[\"bbox1\",\"non_spine_synapse_027\"],[\"bbox1\",\"non_spine_synapse_026\"],[\"bbox1\",\"non_spine_synapse_025\"],[\"bbox1\",\"non_spine_synapse_024\"],[\"bbox1\",\"non_spine_synapse_023\"],[\"bbox1\",\"non_spine_synapse_022\"],[\"bbox1\",\"non_spine_synapse_021\"],[\"bbox1\",\"non_spine_synapse_020\"],[\"bbox1\",\"non_spine_synapse_019\"],[\"bbox1\",\"non_spine_synapse_018\"],[\"bbox1\",\"non_spine_synapse_017\"],[\"bbox1\",\"non_spine_synapse_016\"],[\"bbox1\",\"non_spine_synapse_015\"],[\"bbox1\",\"non_spine_synapse_014\"],[\"bbox1\",\"non_spine_synapse_013\"],[\"bbox1\",\"non_spine_synapse_012\"],[\"bbox1\",\"non_spine_synapse_011\"],[\"bbox1\",\"non_spine_synapsed_010\"],[\"bbox1\",\"non_spine_synapse_009\"],[\"bbox1\",\"non_spine_synapse_008\"],[\"bbox1\",\"non_spine_synapse_007\"],[\"bbox1\",\"non_spine_synapse_006\"],[\"bbox1\",\"non_spine_synapse_005\"],[\"bbox1\",\"non_spine_synapse_004\"],[\"bbox1\",\"non_spine_synapse_003\"],[\"bbox1\",\"non_spine_synapse_002\"],[\"bbox1\",\"non_spine_synapse_001\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox1\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[6.320848,6.868704,6.527309,5.7201967,5.504691,6.368925,5.584473,6.5815415,6.0709996,5.547961,6.8613815,5.539868,7.9374447,5.977072,7.0375266,5.73709,7.845846,5.671688,5.7251077,7.393874,5.9098163,7.603018,7.804602,6.8634253,7.083628,6.8541102,5.6302857,7.454782,6.653799,6.8165684,7.8831396,7.42407,6.6293106,7.413919,5.8100405,5.655298,5.9776444,6.7837343,5.727114,7.1478252,7.5097566,5.7703094,6.0439177,7.275848,7.187259,5.7678304,5.625099,6.057722,7.1064067,7.548423,7.4722323,7.786518,6.8941984,6.349089,5.96265,7.2368374,7.354787,6.1669087],\"xaxis\":\"x\",\"y\":[1.269926,1.4927359,1.1532238,0.86782265,1.3223397,1.5089643,1.3659642,1.9268119,0.69472766,1.1263334,0.99016833,1.4593079,1.703558,1.3703259,1.7580043,0.7027388,2.069424,1.2774208,1.2710545,1.0835494,0.6930185,1.2790532,2.5208964,1.1153023,1.4296037,0.6846901,1.0070198,1.4984587,1.6841856,0.80536443,1.3456845,1.1058097,1.8387506,2.3963609,1.0435407,1.220936,0.84920436,1.3811061,1.417144,1.444629,1.5773708,0.85543925,1.0315205,1.7811003,0.9843891,1.0211095,1.2627047,0.86874527,0.96847725,1.5128411,1.5924027,1.953415,0.7599589,0.62437075,0.72866863,1.4485724,1.5667044,0.79082656],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox2\",\"explorative_2024-11-07_Cora_Wolter_133\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_110\"],[\"bbox2\",\"explorative_2024-11-07_Cora_Wolter_132\"],[\"bbox2\",\"explorative_2024-11-07_Cora_Wolter_131\"],[\"bbox2\",\"explorative_2024-10-17_Valentin_Pinkau_130\"],[\"bbox2\",\"explorative_2024-10-17_Valentin_Pinkau_129\"],[\"bbox2\",\"explorative_2024-10-17_Valentin_Pinkau_128\"],[\"bbox2\",\"explorative_2024-10-17_Valentin_Pinkau_123\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_118\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_117\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_116\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_114\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_113\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_111\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_109\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_108\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_107\"],[\"bbox2\",\"explorative_2024-09-02_Ali_Karimi_106\"],[\"bbox2\",\"explorative_2024-09-02_Ali_Karimi_104\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_103\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_102\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_099\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_096\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_094\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_093\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_091\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_090\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_088\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_087\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_085\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_083\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_081\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_080\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_079\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_078\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_077\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_076\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_075\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_074\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_073\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_072\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_071\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_070\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_069\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_068\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_067\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_066\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_065\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_064\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_063\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_062\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_061\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_060\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_059\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_058\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_054\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_053\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_052\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_051\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_050\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_048\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_047\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_046\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_044\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_043\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_042\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_041\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_040\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_039\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_037\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_036\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_034\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_033\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_032\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_031\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_030\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_029\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_028\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_027\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_026\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_025\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_024\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_023\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_021\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_019\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_018 \"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_016\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_015\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_014\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_013\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_012\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_011\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_010\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_009\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_008\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_007\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_006\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_005\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_004\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_003\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox2\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox2\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[6.471332,7.5181804,6.8196044,7.1974874,6.0142126,6.4876976,7.136677,6.6587753,5.6834297,7.0344157,6.272738,7.0256357,6.322197,6.745193,6.142396,7.042722,6.3555856,6.531251,7.272558,6.827308,5.869167,6.0442777,6.3084316,6.022226,6.66891,6.550416,5.9476347,7.1514087,6.5631356,6.698946,5.925877,6.4896374,7.283149,6.410516,5.743928,7.3338947,6.6301923,6.5739636,7.2799606,6.2305193,6.6339893,6.408582,5.8733983,6.067681,6.0072026,7.0327215,5.886594,6.6982126,6.2971234,6.7953167,7.000367,7.0031757,7.177908,6.6612144,6.705218,6.577227,6.811989,5.8185253,6.991147,6.618037,6.862479,6.3358097,5.721546,6.3934493,6.0909424,5.8245454,6.564363,6.8570747,6.3224053,6.2827277,6.881747,6.092355,6.065134,6.3030496,5.9195414,5.5396104,5.898544,6.642313,6.7227163,6.6150155,6.28568,6.9710894,6.74594,6.098579,6.474288,6.0101795,6.029689,5.860342,6.8189664,6.6419573,5.9279537,6.186878,6.659691,5.9242744,5.96739,6.434012,6.252948,6.0649095,6.628577,6.2640266],\"xaxis\":\"x\",\"y\":[0.7944705,2.1648886,1.1718825,1.2835804,0.8799315,0.93951553,2.7011032,1.0864222,1.0699046,2.1004853,1.0581764,2.0663826,0.862092,1.0576993,1.0402467,2.747193,1.2094586,2.048154,1.7590544,1.5597826,1.0201838,1.0595052,1.093412,1.143118,1.3962892,1.1781464,2.3989115,2.326682,1.6728816,1.9631786,1.3093699,1.2654364,2.1666765,1.2230918,1.2943376,2.1740785,2.0223427,2.539319,2.0341284,1.6906875,1.5493609,1.9134668,1.7847602,1.4358768,1.471908,1.7919289,1.5929948,2.2551508,2.0927644,2.2863226,2.4061253,1.8521435,2.1170557,1.4517274,2.5414474,2.5423586,2.2169292,1.3557911,1.449908,1.9698429,2.6501026,1.6459545,1.964762,2.529689,1.7304962,1.6374708,2.2935712,2.4429016,2.3806498,2.0209966,2.2303169,1.6711258,2.366551,2.2373788,2.1091962,1.7713856,2.183078,1.8980925,2.1625881,2.082995,2.418133,1.6663271,1.8762665,2.0284035,2.205503,2.4090016,2.2241273,1.8926041,1.7279217,1.8698736,2.1953573,2.565536,2.0111895,2.1620839,1.901595,1.5773185,1.8034174,1.7004234,2.0881076,1.8560284],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox3\",\"non_spine_synapse_009\"],[\"bbox3\",\"non_spine_synapse_008\"],[\"bbox3\",\"non_spine_synapse_007\"],[\"bbox3\",\"non_spine_synapse_006\"],[\"bbox3\",\"non_spine_synapse_005\"],[\"bbox3\",\"non_spine_synapse_004\"],[\"bbox3\",\"non_spine_synapse_003\"],[\"bbox3\",\"non_spine_synapse_002\"],[\"bbox3\",\"non_spine_synapse_001\"],[\"bbox3\",\"non_spine_synapse_061\"],[\"bbox3\",\"non_spine_synapse_060\"],[\"bbox3\",\"non_spine_synapse_059\"],[\"bbox3\",\"non_spine_synapse_058\"],[\"bbox3\",\"non_spine_synapse_057\"],[\"bbox3\",\"non_spine_synapse_056\"],[\"bbox3\",\"non_spine_synapse_055\"],[\"bbox3\",\"non_spine_synapse_054\"],[\"bbox3\",\"non_spine_synapse_053\"],[\"bbox3\",\"non_spine_synapse_052\"],[\"bbox3\",\"non_spine_synapse_051\"],[\"bbox3\",\"non_spine_synapse_050\"],[\"bbox3\",\"non_spine_synapse_049\"],[\"bbox3\",\"non_spine_synapse_048\"],[\"bbox3\",\"non_spine_synapse_047\"],[\"bbox3\",\"non_spine_synapse_046\"],[\"bbox3\",\"non_spine_synapse_045\"],[\"bbox3\",\"non_spine_synapse_044\"],[\"bbox3\",\"non_spine_synapse_043\"],[\"bbox3\",\"non_spine_synapse_042\"],[\"bbox3\",\"non_spine_synapse_041\"],[\"bbox3\",\"non_spine_synapse_040\"],[\"bbox3\",\"non_spine_synapse_039\"],[\"bbox3\",\"non_spine_synapse_038\"],[\"bbox3\",\"non_spine_synapse_037\"],[\"bbox3\",\"non_spine_synapse_036\"],[\"bbox3\",\"non_spine_synapse_035\"],[\"bbox3\",\"non_spine_synapse_034\"],[\"bbox3\",\"non_spine_synapse_033\"],[\"bbox3\",\"non_spine_synapse_032\"],[\"bbox3\",\"non_spine_synapse_031\"],[\"bbox3\",\"non_spine_synapse_030\"],[\"bbox3\",\"non_spine_synapse_029\"],[\"bbox3\",\"non_spine_synapse_028\"],[\"bbox3\",\"non_spine_synapse_027\"],[\"bbox3\",\"non_spine_synapse_026\"],[\"bbox3\",\"non_spine_synapse_025\"],[\"bbox3\",\"non_spine_synapse_024\"],[\"bbox3\",\"non_spine_synapse_023\"],[\"bbox3\",\"non_spine_synapse_022\"],[\"bbox3\",\"non_spine_synapse_021\"],[\"bbox3\",\"non_spine_synapse_020\"],[\"bbox3\",\"non_spine_synapse_019\"],[\"bbox3\",\"non_spine_synapse_018\"],[\"bbox3\",\"non_spine_synapse_017\"],[\"bbox3\",\"non_spine_synapse_016\"],[\"bbox3\",\"non_spine_synapse_015\"],[\"bbox3\",\"non_spine_synapse_014\"],[\"bbox3\",\"non_spine_synapse_013\"],[\"bbox3\",\"spine_synapse_001\"],[\"bbox3\",\"non_spine_synapse_012\"],[\"bbox3\",\"non_spine_synapse_011\"],[\"bbox3\",\"non_spine_synapse_010\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox3\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox3\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[6.883459,6.6475296,5.8160086,6.370837,5.8936477,6.402843,6.0717006,5.956161,6.043161,6.5802774,6.90029,6.5803432,6.7610893,6.714982,5.965941,6.169317,6.5512986,6.6295857,6.604784,6.7101054,6.615584,6.0637126,6.599324,6.1229477,6.408752,6.659329,6.453432,5.884198,6.7596426,6.4507174,6.731519,6.311013,6.2546487,6.6682267,6.824485,6.013682,6.178463,6.419103,6.633221,6.112539,6.253562,6.630824,6.7976365,6.2445617,6.8099837,6.364212,6.350959,6.4317384,6.0656176,6.2572565,6.7914867,6.5521812,6.12482,6.3774133,6.3220057,6.449023,6.213191,6.207129,6.29685,5.97172,6.6198497,6.520426],\"xaxis\":\"x\",\"y\":[1.8457015,1.686475,1.8737981,2.2299533,1.8289009,0.6388161,1.3299502,1.7313812,1.7509884,1.5141133,1.3258698,1.4978921,0.8064798,1.1638875,1.7254692,1.5907732,0.88944143,0.8567526,1.531599,1.1478052,0.8017769,0.6001686,1.4490206,1.5194066,1.8443745,0.8697635,1.3961837,1.4920034,1.6265417,1.2887223,1.3745407,1.2895213,0.9368249,1.6376885,0.99691415,1.0752714,0.6379779,1.1098622,1.0169189,1.5988492,1.2196866,0.76476985,1.2046639,1.3135325,1.7057936,0.50954324,0.5841611,1.1370591,1.1295274,0.6477655,1.1357856,1.2876352,1.0257604,0.99883974,1.0728536,0.9430559,0.68055946,1.6398295,1.1454122,0.72018844,1.3482691,1.3218886],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox4\",\"explorative_2024-09-02_Ali_Karimi_040\"],[\"bbox4\",\"explorative_2024-09-02_Ali_Karimi_039\"],[\"bbox4\",\"explorative_2024-09-02_Ali_Karimi_038\"],[\"bbox4\",\"explorative_2024-09-02_Ali_Karimi_037\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_036\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_035\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_034\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_033\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_032\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_031\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_030\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_029\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_028\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_027\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_026\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_025\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_024\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_023\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_022\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_021\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_020\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_019\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_018\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_017\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_016\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_015\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_014\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_013\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_012\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_011\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_010\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_009\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_008\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_007\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_006\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_005\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_004\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_003\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_002\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_001\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox4\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox4\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[6.249248,6.1131454,6.0224733,6.428279,6.4086185,6.1801815,5.8169684,6.378914,6.325536,5.993609,6.46431,6.096682,5.9950237,6.1605315,5.918073,6.086242,6.3906775,6.4350557,6.190655,6.3768654,5.4771075,6.2515664,6.1293797,6.1048007,6.2216773,6.013161,6.0574145,6.0381093,5.8084116,6.061588,6.267484,5.9901223,5.8095207,5.8964863,5.875873,5.7806754,5.874742,6.0463257,5.655362,5.648787],\"xaxis\":\"x\",\"y\":[0.8739907,1.1295762,0.9395399,2.0209148,1.8136376,0.9903561,2.1734476,1.4791936,1.5725201,2.1885688,1.8121874,1.0148607,1.2110313,1.0932102,2.124795,2.0910962,1.6431801,1.4424651,1.2634959,1.6156607,2.1258335,1.8718227,1.6466628,2.256527,1.9614286,2.0752647,1.7271669,1.6209544,1.3928841,1.7652967,1.975083,1.8527673,1.7297674,1.9683955,1.9491774,2.1201525,1.3256266,1.7156023,2.2603695,1.7769926],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox5\",\"non_spine_synapse_005\"],[\"bbox5\",\"spine_synapse_001\"],[\"bbox5\",\"non_spine_synapse_004\"],[\"bbox5\",\"non_spine_synapse_003\"],[\"bbox5\",\"non_spine_synapse_002\"],[\"bbox5\",\"non_spine_synapse_001\"],[\"bbox5\",\"non_spine_synapse_074\"],[\"bbox5\",\"non_spine_synapse_073\"],[\"bbox5\",\"spine_synapse_012\"],[\"bbox5\",\"non_spine_synapse_072\"],[\"bbox5\",\"spine_synapse_011\"],[\"bbox5\",\"spine_synapse_010\"],[\"bbox5\",\"spine_synapse_009\"],[\"bbox5\",\"spine_synapse_008\"],[\"bbox5\",\"non_spine_synapse_071\"],[\"bbox5\",\"non_spine_synapse_070\"],[\"bbox5\",\"non_spine_synapse_069\"],[\"bbox5\",\"non_spine_synapse_068\"],[\"bbox5\",\"non_spine_synapse_067\"],[\"bbox5\",\"non_spine_synapse_066\"],[\"bbox5\",\"non_spine_synapse_065\"],[\"bbox5\",\"spine_synapse_007\"],[\"bbox5\",\"spine_synapse_006\"],[\"bbox5\",\"spine_synapse_005\"],[\"bbox5\",\"non_spine_synapse_064\"],[\"bbox5\",\"non_spine_synapse_063\"],[\"bbox5\",\"non_spine_synapse_062\"],[\"bbox5\",\"non_spine_synapse_061\"],[\"bbox5\",\"non_spine_synapse_060\"],[\"bbox5\",\"non_spine_synapse_059\"],[\"bbox5\",\"non_spine_synapse_058\"],[\"bbox5\",\"non_spine_synapse_057\"],[\"bbox5\",\"non_spine_synapse_056\"],[\"bbox5\",\"non_spine_synapse_055\"],[\"bbox5\",\"non_spine_synapse_054\"],[\"bbox5\",\"non_spine_synapse_053\"],[\"bbox5\",\"non_spine_synapse_052\"],[\"bbox5\",\"non_spine_synapse_051\"],[\"bbox5\",\"spine_synapse_004\"],[\"bbox5\",\"non_spine_synapse_050\"],[\"bbox5\",\"non_spine_synapse_049\"],[\"bbox5\",\"non_spine_synapse_048\"],[\"bbox5\",\"non_spine_synapse_047\"],[\"bbox5\",\"non_spine_synapse_046\"],[\"bbox5\",\"non_spine_synapse_045\"],[\"bbox5\",\"non_spine_synapse_044\"],[\"bbox5\",\"non_spine_synapse_043\"],[\"bbox5\",\"non_spine_synapse_042\"],[\"bbox5\",\"non_spine_synapse_041\"],[\"bbox5\",\"non_spine_synapse_040\"],[\"bbox5\",\"non_spine_synapse_039\"],[\"bbox5\",\"non_spine_synapse_038\"],[\"bbox5\",\"non_spine_synapse_037\"],[\"bbox5\",\"non_spine_synapse_036\"],[\"bbox5\",\"non_spine_synapse_035\"],[\"bbox5\",\"non_spine_synapse_034\"],[\"bbox5\",\"non_spine_synapse_033\"],[\"bbox5\",\"non_spine_synapse_032\"],[\"bbox5\",\"non_spine_synapse_031\"],[\"bbox5\",\"non_spine_synapse_030\"],[\"bbox5\",\"non_spine_synapse_029\"],[\"bbox5\",\"non_spine_synapse_028\"],[\"bbox5\",\"non_spine_synapse_027\"],[\"bbox5\",\"non_spine_synapse_026\"],[\"bbox5\",\"non_spine_synapse_025\"],[\"bbox5\",\"non_spine_synapse_024\"],[\"bbox5\",\"non_spine_synapse_023\"],[\"bbox5\",\"non_spine_synapse_022\"],[\"bbox5\",\"non_spine_synapse_021\"],[\"bbox5\",\"non_spine_synapse_020\"],[\"bbox5\",\"non_spine_synapse_019\"],[\"bbox5\",\"non_spine_synapse_018\"],[\"bbox5\",\"non_spine_synapse_017\"],[\"bbox5\",\"spine_synapse_003\"],[\"bbox5\",\"spine_synapse_002\"],[\"bbox5\",\"non_spine_synapse_016\"],[\"bbox5\",\"non_spine_synapse_015\"],[\"bbox5\",\"non_spine_synapse_014\"],[\"bbox5\",\"non_spine_synapse_013\"],[\"bbox5\",\"non_spine_synapse_012\"],[\"bbox5\",\"non_spine_synapse_011\"],[\"bbox5\",\"non_spine_synapse_010\"],[\"bbox5\",\"non_spine_synapse_009\"],[\"bbox5\",\"non_spine_synapse_008\"],[\"bbox5\",\"non_spine_synapse_007\"],[\"bbox5\",\"non_spine_synapse_006\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox5\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox5\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[5.9738746,5.601307,5.6571994,6.103901,5.8813534,5.5591617,5.467118,5.1222806,5.443731,5.656317,5.3800054,5.7825794,5.777619,5.7291045,5.723026,5.3576407,5.656458,5.098737,5.4991775,5.5062456,5.334042,5.42541,5.428143,5.685484,5.4760995,5.297463,5.5537944,5.6257753,5.4340334,7.0386553,7.161118,5.079734,5.849782,5.19003,6.066254,5.0431232,7.295348,4.9668546,5.842884,5.4071603,5.8727407,5.337935,5.7475967,5.1678505,6.7131124,7.6985884,4.491574,4.7941628,6.4862995,5.5549474,5.346467,6.0116234,4.2672005,5.1481633,5.4135695,5.235376,5.0175576,5.0662208,4.604719,5.178563,4.9820986,5.253937,5.5445843,5.7992177,5.487877,5.2713995,4.947555,4.9786363,5.9010663,5.568516,5.1878967,4.9511094,5.612332,5.3764815,5.168915,4.808191,5.1414795,5.209918,4.944958,5.4503384,5.609389,5.354393,5.426832,4.8942533,5.1652775,5.603424],\"xaxis\":\"x\",\"y\":[1.2354586,2.1958086,1.95095,1.8806335,1.6613055,1.7348299,1.8410894,1.785179,2.044951,1.6531831,1.9505576,1.5341187,1.0755742,1.8902828,1.5393391,1.9683665,1.3752367,1.7046607,1.7065334,1.4406333,1.8443403,1.4468153,1.7080082,1.2467192,1.5985398,1.4973326,1.411685,1.1946311,1.30362,2.300223,0.7707788,1.5616124,0.622007,1.3401212,0.06638969,1.6045223,1.3427358,1.6150775,1.7267298,0.47594178,1.9717605,0.62471974,2.761084,2.1802118,2.1691723,2.5683339,0.94310725,0.3068035,0.7163847,0.38312823,0.68404746,2.4408448,1.3087096,0.9991446,-0.021084063,0.5013392,0.7861882,0.56467265,1.2663199,0.275978,0.45628938,0.013077659,-0.15172225,-0.009798002,0.23991103,0.1832353,0.25381464,-0.047289312,0.77468413,0.1761033,0.3330751,1.593604,0.26586053,0.79371166,0.07373436,0.54997814,0.43319124,1.5541425,2.3159614,1.0535835,0.5651438,0.020549921,0.11775613,0.8379129,0.3904481,2.5239367],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox6\",\"spine_synapse_072\"],[\"bbox6\",\"non_spine_synapse_026\"],[\"bbox6\",\"non_spine_synapse_025\"],[\"bbox6\",\"spine_synapse_073\"],[\"bbox6\",\"spine_synapse_071\"],[\"bbox6\",\"spine_synapse_070\"],[\"bbox6\",\"spine_synapse_069\"],[\"bbox6\",\"spine_synapse_068\"],[\"bbox6\",\"spine_synapse_067\"],[\"bbox6\",\"non_spine_synapse_024\"],[\"bbox6\",\"non_spine_synapse_023\"],[\"bbox6\",\"spine_synapse_066\"],[\"bbox6\",\"spine_synapse_065\"],[\"bbox6\",\"spine_synapse_064\"],[\"bbox6\",\"spine_synapse_063\"],[\"bbox6\",\"spine_synapse_062\"],[\"bbox6\",\"spine_synapse_061\"],[\"bbox6\",\"spine_synapse_060\"],[\"bbox6\",\"spine_synapse_059\"],[\"bbox6\",\"spine_synapse_058\"],[\"bbox6\",\"spine_synapse_057\"],[\"bbox6\",\"spine_synapse_056\"],[\"bbox6\",\"spine_synapse_055\"],[\"bbox6\",\"spine_synapse_054\"],[\"bbox6\",\"spine_synapse_053\"],[\"bbox6\",\"spine_synapse_051\"],[\"bbox6\",\"spine_synapse_063\"],[\"bbox6\",\"spine_synapse_050\"],[\"bbox6\",\"spine_synapse_049\"],[\"bbox6\",\"spine_synapse_048\"],[\"bbox6\",\"spine_synapse_047   \"],[\"bbox6\",\"spine_synapse_046\"],[\"bbox6\",\"spine_synapse_045\"],[\"bbox6\",\"non_spine_synapse_022\"],[\"bbox6\",\"spine_synapse_044\"],[\"bbox6\",\"spine_synapse_043\"],[\"bbox6\",\"spine_synapse_042\"],[\"bbox6\",\"spine_synapse_041\"],[\"bbox6\",\"spine_synapse_040\"],[\"bbox6\",\"non_spine_synapse_021\"],[\"bbox6\",\"spine_synapse_039\"],[\"bbox6\",\"spine_synapse_038\"],[\"bbox6\",\"spine_synapse_037\"],[\"bbox6\",\"spine_synapse_036\"],[\"bbox6\",\"spine_synapse_035\"],[\"bbox6\",\"spine_synapse_034\"],[\"bbox6\",\"spine_synapse_033\"],[\"bbox6\",\"spine_synapse_032\"],[\"bbox6\",\"spine_synapse_031\"],[\"bbox6\",\"spine_synapse_030\"],[\"bbox6\",\"spine_synapse_029\"],[\"bbox6\",\"spine_synapse_028\"],[\"bbox6\",\"spine_synapse_027\"],[\"bbox6\",\"spine_synapse_026\"],[\"bbox6\",\"spine_synapse_025\"],[\"bbox6\",\"spine_synapse_024\"],[\"bbox6\",\"spine_synapse_023\"],[\"bbox6\",\"spine_synapse_022\"],[\"bbox6\",\"spine_synapse_021\"],[\"bbox6\",\"spine_synapse_020\"],[\"bbox6\",\"non_spine_synapse_020\"],[\"bbox6\",\"spine_synapse_019\"],[\"bbox6\",\"non_spine_synapse_019\"],[\"bbox6\",\"spine_synapse_018\"],[\"bbox6\",\"spine_synapse_017\"],[\"bbox6\",\"spine_synapse_016\"],[\"bbox6\",\"spine_synapse_015\"],[\"bbox6\",\"spine_synapse_014\"],[\"bbox6\",\"spine_synapse_013\"],[\"bbox6\",\"spine_synapse_012\"],[\"bbox6\",\"spine_synapse_011\"],[\"bbox6\",\"non_spine_synapse_018   \"],[\"bbox6\",\"spine_synapse_010\"],[\"bbox6\",\"spine_synapse_008\"],[\"bbox6\",\"spine_synapse_007\"],[\"bbox6\",\"spine_synapse_006\"],[\"bbox6\",\"spine_synapse_005\"],[\"bbox6\",\"spine_synapse_004\"],[\"bbox6\",\"spine_synapse_003\"],[\"bbox6\",\"spine_synapse_002\"],[\"bbox6\",\"non_spine_synapse_017\"],[\"bbox6\",\"non_spine_synapse_016\"],[\"bbox6\",\"spine_synapse_001\"],[\"bbox6\",\"non_spine_synapse_015\"],[\"bbox6\",\"non_spine_synapse_014\"],[\"bbox6\",\"non_spine_synapse_013\"],[\"bbox6\",\"non_spine_synapse_012\"],[\"bbox6\",\"non_spine_synapse_011\"],[\"bbox6\",\"non_spine_synapse_010\"],[\"bbox6\",\"non_spine_synapse_009\"],[\"bbox6\",\"non_spine_synapse_008\"],[\"bbox6\",\"non_spine_synapse_007\"],[\"bbox6\",\"non_spine_synapse_006\"],[\"bbox6\",\"non_spine_synapse_005\"],[\"bbox6\",\"non_spine_synapse_004\"],[\"bbox6\",\"non_spine_synapse_003\"],[\"bbox6\",\"non_spine_synapse_002\"],[\"bbox6\",\"non_spine_synapse_001\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox6\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox6\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[5.5516405,4.7182803,4.660108,4.518363,5.7620873,6.382896,5.227513,5.5483947,5.8431516,4.930075,6.7839684,4.4657154,6.740739,5.6334734,5.5762324,4.9351544,4.833312,6.4615545,5.79223,5.2197466,5.8570433,6.443044,5.2295094,4.887509,5.7994885,7.1796155,4.7778473,5.8613987,6.53244,5.7513614,7.6550727,5.551396,4.9879117,5.3967752,5.310584,5.69732,4.4922686,7.0045495,6.9170647,6.0404882,4.733388,5.046545,5.72999,5.7402034,4.5323014,4.5541058,6.0729065,5.073003,4.8287435,6.232051,6.5645237,4.7437863,7.357915,5.0111313,4.664418,6.280439,6.8772693,4.7838674,6.0399632,4.711765,6.3515296,4.746116,6.0598435,4.7348275,4.8940277,4.9978385,4.8542843,4.83325,4.580312,5.164249,5.31132,5.096924,5.027365,4.854425,4.945194,4.8034296,6.5321317,4.9198084,5.212032,5.1563935,6.6642323,7.346506,5.240856,5.122102,5.455065,5.2771926,5.971975,5.0602703,8.08123,5.075455,5.3909984,5.800288,5.916781,5.729084,5.3790026,6.3238587,7.3313413,6.306697],\"xaxis\":\"x\",\"y\":[3.227021,1.6358249,0.66898304,0.72642076,2.3974304,3.3598304,1.3008735,1.7811091,1.6631522,1.9048952,2.9439027,1.402845,3.6309,1.5611838,2.6700857,1.8447746,2.3611863,3.5685654,3.1201096,2.1629357,3.25016,2.328182,2.59974,1.3568105,2.6606338,3.3667607,0.81062895,2.809072,3.3910985,1.6149882,3.246594,1.0046537,2.1780543,0.8258995,2.866013,0.4435001,1.8626055,1.9411737,3.2940218,2.2883863,0.7778869,2.6574953,0.37146613,3.0781379,1.9038953,1.0750041,3.3617382,2.6073484,0.8229247,2.7971292,1.9524784,1.3268062,2.3604314,0.9404065,1.4119037,1.5751576,2.5936174,1.1025264,2.9585974,1.0577112,1.8039218,1.9831907,1.1003261,2.0808558,0.6611317,1.7542853,1.240304,1.3685106,1.4700996,0.7307389,0.41031393,0.84627247,0.7768766,1.7600399,1.5335617,1.0705366,1.1148101,1.5478966,1.2163827,0.82278734,0.6554125,1.120802,0.8775195,1.0931414,0.7791284,0.3260666,0.09897812,1.2741694,2.3373797,0.8476316,0.38650313,0.027244566,0.035219327,0.28895354,0.49221024,0.06965756,0.46060064,0.006996735],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"umap_x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"umap_y\"}},\"legend\":{\"title\":{\"text\":\"bbox_name\"},\"tracegroupgap\":0},\"title\":{\"text\":\"UMAP (x vs y)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a718abb7-e083-4e0c-a1a3-c25f09bd7169');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"aa7bd25e-74da-4b52-8404-ae0688058f8f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"aa7bd25e-74da-4b52-8404-ae0688058f8f\")) {                    Plotly.newPlot(                        \"aa7bd25e-74da-4b52-8404-ae0688058f8f\",                        [{\"customdata\":[[\"bbox1\",\"non_spine_synapsed_056\"],[\"bbox1\",\"non_spine_synapse_057\"],[\"bbox1\",\"non_spine_synapse_058\"],[\"bbox1\",\"non_spine_synapse_063\"],[\"bbox1\",\"non_spine_synapse_062\"],[\"bbox1\",\"non_spine_synapse_061\"],[\"bbox1\",\"non_spine_synapse_060\"],[\"bbox1\",\"non_spine_synapse_059\"],[\"bbox1\",\"non_spine_synapse_054\"],[\"bbox1\",\"non_spine_synapse_053\"],[\"bbox1\",\"non_spine_synapse_052\"],[\"bbox1\",\"non_spine_synapse_051\"],[\"bbox1\",\"non_spine_synapse_050\"],[\"bbox1\",\"non_spine_synapse_049\"],[\"bbox1\",\"non_spine_synapse_048\"],[\"bbox1\",\"non_spine_synapse_047\"],[\"bbox1\",\"non_spine_synapse_046\"],[\"bbox1\",\"non_spine_synapse_045\"],[\"bbox1\",\"non_spine_synapse_044\"],[\"bbox1\",\"non_spine_synapse_040\"],[\"bbox1\",\"non_spine_synapse_038\"],[\"bbox1\",\"non_spine_synapse_037\"],[\"bbox1\",\"non_spine_synapse_036\"],[\"bbox1\",\"non_spine_synapse_035\"],[\"bbox1\",\"non_spine_synapse_034\"],[\"bbox1\",\"non_spine_synapse_033\"],[\"bbox1\",\"non_spine_synapse_032\"],[\"bbox1\",\"non_spine_synapse_031\"],[\"bbox1\",\"non_spine_synapse_030\"],[\"bbox1\",\"non_spine_synapse_029\"],[\"bbox1\",\"non_spine_synapse_028\"],[\"bbox1\",\"non_spine_synapse_027\"],[\"bbox1\",\"non_spine_synapse_026\"],[\"bbox1\",\"non_spine_synapse_025\"],[\"bbox1\",\"non_spine_synapse_024\"],[\"bbox1\",\"non_spine_synapse_023\"],[\"bbox1\",\"non_spine_synapse_022\"],[\"bbox1\",\"non_spine_synapse_021\"],[\"bbox1\",\"non_spine_synapse_020\"],[\"bbox1\",\"non_spine_synapse_019\"],[\"bbox1\",\"non_spine_synapse_018\"],[\"bbox1\",\"non_spine_synapse_017\"],[\"bbox1\",\"non_spine_synapse_016\"],[\"bbox1\",\"non_spine_synapse_015\"],[\"bbox1\",\"non_spine_synapse_014\"],[\"bbox1\",\"non_spine_synapse_013\"],[\"bbox1\",\"non_spine_synapse_012\"],[\"bbox1\",\"non_spine_synapse_011\"],[\"bbox1\",\"non_spine_synapsed_010\"],[\"bbox1\",\"non_spine_synapse_009\"],[\"bbox1\",\"non_spine_synapse_008\"],[\"bbox1\",\"non_spine_synapse_007\"],[\"bbox1\",\"non_spine_synapse_006\"],[\"bbox1\",\"non_spine_synapse_005\"],[\"bbox1\",\"non_spine_synapse_004\"],[\"bbox1\",\"non_spine_synapse_003\"],[\"bbox1\",\"non_spine_synapse_002\"],[\"bbox1\",\"non_spine_synapse_001\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox1\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[6.320848,6.868704,6.527309,5.7201967,5.504691,6.368925,5.584473,6.5815415,6.0709996,5.547961,6.8613815,5.539868,7.9374447,5.977072,7.0375266,5.73709,7.845846,5.671688,5.7251077,7.393874,5.9098163,7.603018,7.804602,6.8634253,7.083628,6.8541102,5.6302857,7.454782,6.653799,6.8165684,7.8831396,7.42407,6.6293106,7.413919,5.8100405,5.655298,5.9776444,6.7837343,5.727114,7.1478252,7.5097566,5.7703094,6.0439177,7.275848,7.187259,5.7678304,5.625099,6.057722,7.1064067,7.548423,7.4722323,7.786518,6.8941984,6.349089,5.96265,7.2368374,7.354787,6.1669087],\"xaxis\":\"x\",\"y\":[5.035003,5.2424083,5.1918364,5.529476,5.664883,5.062537,5.614609,5.0859056,5.7297487,5.7426186,5.5077996,5.5896807,5.9728303,5.3389797,5.332006,6.032172,5.9234996,5.6477523,5.633202,5.906338,5.875859,6.910996,6.5052147,5.406056,5.4760785,6.129239,6.1408753,5.7478337,5.1371937,5.8785195,6.480586,6.408336,5.3317175,5.677474,5.7257857,5.7948713,6.1833944,5.6608124,5.7642407,5.9678907,6.055814,5.77364,6.066858,5.790117,6.3045444,5.923527,5.973696,5.4473243,5.9144163,6.250254,6.79778,6.771094,6.63371,6.4336576,6.395868,6.3105893,7.336922,6.59581],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox2\",\"explorative_2024-11-07_Cora_Wolter_133\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_110\"],[\"bbox2\",\"explorative_2024-11-07_Cora_Wolter_132\"],[\"bbox2\",\"explorative_2024-11-07_Cora_Wolter_131\"],[\"bbox2\",\"explorative_2024-10-17_Valentin_Pinkau_130\"],[\"bbox2\",\"explorative_2024-10-17_Valentin_Pinkau_129\"],[\"bbox2\",\"explorative_2024-10-17_Valentin_Pinkau_128\"],[\"bbox2\",\"explorative_2024-10-17_Valentin_Pinkau_123\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_118\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_117\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_116\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_114\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_113\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_111\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_109\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_108\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_107\"],[\"bbox2\",\"explorative_2024-09-02_Ali_Karimi_106\"],[\"bbox2\",\"explorative_2024-09-02_Ali_Karimi_104\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_103\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_102\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_099\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_096\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_094\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_093\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_091\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_090\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_088\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_087\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_085\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_083\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_081\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_080\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_079\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_078\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_077\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_076\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_075\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_074\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_073\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_072\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_071\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_070\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_069\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_068\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_067\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_066\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_065\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_064\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_063\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_062\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_061\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_060\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_059\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_058\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_054\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_053\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_052\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_051\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_050\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_048\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_047\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_046\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_044\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_043\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_042\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_041\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_040\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_039\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_037\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_036\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_034\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_033\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_032\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_031\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_030\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_029\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_028\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_027\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_026\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_025\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_024\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_023\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_021\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_019\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_018 \"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_016\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_015\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_014\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_013\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_012\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_011\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_010\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_009\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_008\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_007\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_006\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_005\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_004\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_003\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox2\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox2\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[6.471332,7.5181804,6.8196044,7.1974874,6.0142126,6.4876976,7.136677,6.6587753,5.6834297,7.0344157,6.272738,7.0256357,6.322197,6.745193,6.142396,7.042722,6.3555856,6.531251,7.272558,6.827308,5.869167,6.0442777,6.3084316,6.022226,6.66891,6.550416,5.9476347,7.1514087,6.5631356,6.698946,5.925877,6.4896374,7.283149,6.410516,5.743928,7.3338947,6.6301923,6.5739636,7.2799606,6.2305193,6.6339893,6.408582,5.8733983,6.067681,6.0072026,7.0327215,5.886594,6.6982126,6.2971234,6.7953167,7.000367,7.0031757,7.177908,6.6612144,6.705218,6.577227,6.811989,5.8185253,6.991147,6.618037,6.862479,6.3358097,5.721546,6.3934493,6.0909424,5.8245454,6.564363,6.8570747,6.3224053,6.2827277,6.881747,6.092355,6.065134,6.3030496,5.9195414,5.5396104,5.898544,6.642313,6.7227163,6.6150155,6.28568,6.9710894,6.74594,6.098579,6.474288,6.0101795,6.029689,5.860342,6.8189664,6.6419573,5.9279537,6.186878,6.659691,5.9242744,5.96739,6.434012,6.252948,6.0649095,6.628577,6.2640266],\"xaxis\":\"x\",\"y\":[6.326063,6.9599447,6.8664436,6.945028,6.2499523,6.4762917,6.1579237,7.055593,6.2574477,7.479023,6.272149,7.2713804,5.8799777,6.352491,5.8582554,6.685192,7.1431885,7.3728633,6.838344,6.791372,6.556036,6.458795,6.7473373,6.383667,7.0697064,6.8196683,6.6358175,6.6040907,7.1285863,7.2127585,6.744717,6.838003,5.912319,6.7256813,6.4736905,6.7729535,6.8277216,7.028124,6.5431223,7.0973277,6.6809044,6.967142,6.8304167,6.5903654,6.4931145,6.6040554,6.847252,7.1217,6.951928,6.794158,6.2832108,5.985091,6.1200767,6.6113234,5.9854255,6.683618,6.4851513,6.4572477,6.406686,6.428871,6.331547,6.594099,6.685112,6.2028675,7.022855,6.81492,6.1181498,5.8165913,6.6808424,6.0758014,5.9262595,6.5408187,6.5603914,6.4436407,6.4843416,6.3322062,6.726312,6.3398266,5.576062,6.155062,6.149462,6.2588873,5.9452724,6.1830163,5.865547,5.9110947,6.184998,6.4239445,6.021743,5.99811,6.2235336,6.0570884,5.802744,6.058542,6.462475,6.397371,6.3199425,6.225245,6.437115,5.9102387],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox3\",\"non_spine_synapse_009\"],[\"bbox3\",\"non_spine_synapse_008\"],[\"bbox3\",\"non_spine_synapse_007\"],[\"bbox3\",\"non_spine_synapse_006\"],[\"bbox3\",\"non_spine_synapse_005\"],[\"bbox3\",\"non_spine_synapse_004\"],[\"bbox3\",\"non_spine_synapse_003\"],[\"bbox3\",\"non_spine_synapse_002\"],[\"bbox3\",\"non_spine_synapse_001\"],[\"bbox3\",\"non_spine_synapse_061\"],[\"bbox3\",\"non_spine_synapse_060\"],[\"bbox3\",\"non_spine_synapse_059\"],[\"bbox3\",\"non_spine_synapse_058\"],[\"bbox3\",\"non_spine_synapse_057\"],[\"bbox3\",\"non_spine_synapse_056\"],[\"bbox3\",\"non_spine_synapse_055\"],[\"bbox3\",\"non_spine_synapse_054\"],[\"bbox3\",\"non_spine_synapse_053\"],[\"bbox3\",\"non_spine_synapse_052\"],[\"bbox3\",\"non_spine_synapse_051\"],[\"bbox3\",\"non_spine_synapse_050\"],[\"bbox3\",\"non_spine_synapse_049\"],[\"bbox3\",\"non_spine_synapse_048\"],[\"bbox3\",\"non_spine_synapse_047\"],[\"bbox3\",\"non_spine_synapse_046\"],[\"bbox3\",\"non_spine_synapse_045\"],[\"bbox3\",\"non_spine_synapse_044\"],[\"bbox3\",\"non_spine_synapse_043\"],[\"bbox3\",\"non_spine_synapse_042\"],[\"bbox3\",\"non_spine_synapse_041\"],[\"bbox3\",\"non_spine_synapse_040\"],[\"bbox3\",\"non_spine_synapse_039\"],[\"bbox3\",\"non_spine_synapse_038\"],[\"bbox3\",\"non_spine_synapse_037\"],[\"bbox3\",\"non_spine_synapse_036\"],[\"bbox3\",\"non_spine_synapse_035\"],[\"bbox3\",\"non_spine_synapse_034\"],[\"bbox3\",\"non_spine_synapse_033\"],[\"bbox3\",\"non_spine_synapse_032\"],[\"bbox3\",\"non_spine_synapse_031\"],[\"bbox3\",\"non_spine_synapse_030\"],[\"bbox3\",\"non_spine_synapse_029\"],[\"bbox3\",\"non_spine_synapse_028\"],[\"bbox3\",\"non_spine_synapse_027\"],[\"bbox3\",\"non_spine_synapse_026\"],[\"bbox3\",\"non_spine_synapse_025\"],[\"bbox3\",\"non_spine_synapse_024\"],[\"bbox3\",\"non_spine_synapse_023\"],[\"bbox3\",\"non_spine_synapse_022\"],[\"bbox3\",\"non_spine_synapse_021\"],[\"bbox3\",\"non_spine_synapse_020\"],[\"bbox3\",\"non_spine_synapse_019\"],[\"bbox3\",\"non_spine_synapse_018\"],[\"bbox3\",\"non_spine_synapse_017\"],[\"bbox3\",\"non_spine_synapse_016\"],[\"bbox3\",\"non_spine_synapse_015\"],[\"bbox3\",\"non_spine_synapse_014\"],[\"bbox3\",\"non_spine_synapse_013\"],[\"bbox3\",\"spine_synapse_001\"],[\"bbox3\",\"non_spine_synapse_012\"],[\"bbox3\",\"non_spine_synapse_011\"],[\"bbox3\",\"non_spine_synapse_010\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox3\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox3\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[6.883459,6.6475296,5.8160086,6.370837,5.8936477,6.402843,6.0717006,5.956161,6.043161,6.5802774,6.90029,6.5803432,6.7610893,6.714982,5.965941,6.169317,6.5512986,6.6295857,6.604784,6.7101054,6.615584,6.0637126,6.599324,6.1229477,6.408752,6.659329,6.453432,5.884198,6.7596426,6.4507174,6.731519,6.311013,6.2546487,6.6682267,6.824485,6.013682,6.178463,6.419103,6.633221,6.112539,6.253562,6.630824,6.7976365,6.2445617,6.8099837,6.364212,6.350959,6.4317384,6.0656176,6.2572565,6.7914867,6.5521812,6.12482,6.3774133,6.3220057,6.449023,6.213191,6.207129,6.29685,5.97172,6.6198497,6.520426],\"xaxis\":\"x\",\"y\":[5.7837157,6.2756457,6.3682375,5.7420506,5.977252,6.057221,6.36302,6.4396796,6.634494,6.1962075,5.943683,6.375511,5.6305757,6.0236764,6.213812,6.099872,6.304055,6.1396356,5.904754,5.770087,5.0830474,6.100108,6.142657,6.1587424,5.8253403,5.686188,6.1782293,6.180106,5.6642966,5.822932,5.7539964,6.209315,5.7952657,5.7016587,5.909398,6.0613933,5.675005,6.0899954,5.928738,5.7213287,6.0380397,5.387316,5.38361,4.8597703,5.4195833,5.351725,5.2805867,5.6707196,5.76454,5.3003936,5.774347,5.056572,5.486849,5.1733623,5.5742745,5.3863373,5.4927235,5.8335533,5.151239,5.4143324,5.1814756,5.274105],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox4\",\"explorative_2024-09-02_Ali_Karimi_040\"],[\"bbox4\",\"explorative_2024-09-02_Ali_Karimi_039\"],[\"bbox4\",\"explorative_2024-09-02_Ali_Karimi_038\"],[\"bbox4\",\"explorative_2024-09-02_Ali_Karimi_037\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_036\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_035\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_034\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_033\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_032\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_031\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_030\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_029\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_028\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_027\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_026\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_025\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_024\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_023\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_022\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_021\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_020\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_019\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_018\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_017\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_016\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_015\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_014\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_013\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_012\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_011\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_010\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_009\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_008\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_007\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_006\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_005\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_004\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_003\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_002\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_001\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox4\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox4\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[6.249248,6.1131454,6.0224733,6.428279,6.4086185,6.1801815,5.8169684,6.378914,6.325536,5.993609,6.46431,6.096682,5.9950237,6.1605315,5.918073,6.086242,6.3906775,6.4350557,6.190655,6.3768654,5.4771075,6.2515664,6.1293797,6.1048007,6.2216773,6.013161,6.0574145,6.0381093,5.8084116,6.061588,6.267484,5.9901223,5.8095207,5.8964863,5.875873,5.7806754,5.874742,6.0463257,5.655362,5.648787],\"xaxis\":\"x\",\"y\":[5.3843117,5.9491315,5.5892134,5.7009206,5.270047,5.554914,5.809518,5.392431,5.5581436,5.083759,5.4952717,5.0590787,5.2338133,5.0396967,5.763001,5.535059,4.9971585,5.301975,5.437281,5.360623,5.9235463,5.0552354,5.358806,5.396616,5.027365,5.312774,5.858646,5.46526,5.275495,5.741008,5.181814,5.2399077,5.867944,5.191258,5.5590477,5.8755336,5.2332296,5.514091,5.4478464,5.7356915],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox5\",\"non_spine_synapse_005\"],[\"bbox5\",\"spine_synapse_001\"],[\"bbox5\",\"non_spine_synapse_004\"],[\"bbox5\",\"non_spine_synapse_003\"],[\"bbox5\",\"non_spine_synapse_002\"],[\"bbox5\",\"non_spine_synapse_001\"],[\"bbox5\",\"non_spine_synapse_074\"],[\"bbox5\",\"non_spine_synapse_073\"],[\"bbox5\",\"spine_synapse_012\"],[\"bbox5\",\"non_spine_synapse_072\"],[\"bbox5\",\"spine_synapse_011\"],[\"bbox5\",\"spine_synapse_010\"],[\"bbox5\",\"spine_synapse_009\"],[\"bbox5\",\"spine_synapse_008\"],[\"bbox5\",\"non_spine_synapse_071\"],[\"bbox5\",\"non_spine_synapse_070\"],[\"bbox5\",\"non_spine_synapse_069\"],[\"bbox5\",\"non_spine_synapse_068\"],[\"bbox5\",\"non_spine_synapse_067\"],[\"bbox5\",\"non_spine_synapse_066\"],[\"bbox5\",\"non_spine_synapse_065\"],[\"bbox5\",\"spine_synapse_007\"],[\"bbox5\",\"spine_synapse_006\"],[\"bbox5\",\"spine_synapse_005\"],[\"bbox5\",\"non_spine_synapse_064\"],[\"bbox5\",\"non_spine_synapse_063\"],[\"bbox5\",\"non_spine_synapse_062\"],[\"bbox5\",\"non_spine_synapse_061\"],[\"bbox5\",\"non_spine_synapse_060\"],[\"bbox5\",\"non_spine_synapse_059\"],[\"bbox5\",\"non_spine_synapse_058\"],[\"bbox5\",\"non_spine_synapse_057\"],[\"bbox5\",\"non_spine_synapse_056\"],[\"bbox5\",\"non_spine_synapse_055\"],[\"bbox5\",\"non_spine_synapse_054\"],[\"bbox5\",\"non_spine_synapse_053\"],[\"bbox5\",\"non_spine_synapse_052\"],[\"bbox5\",\"non_spine_synapse_051\"],[\"bbox5\",\"spine_synapse_004\"],[\"bbox5\",\"non_spine_synapse_050\"],[\"bbox5\",\"non_spine_synapse_049\"],[\"bbox5\",\"non_spine_synapse_048\"],[\"bbox5\",\"non_spine_synapse_047\"],[\"bbox5\",\"non_spine_synapse_046\"],[\"bbox5\",\"non_spine_synapse_045\"],[\"bbox5\",\"non_spine_synapse_044\"],[\"bbox5\",\"non_spine_synapse_043\"],[\"bbox5\",\"non_spine_synapse_042\"],[\"bbox5\",\"non_spine_synapse_041\"],[\"bbox5\",\"non_spine_synapse_040\"],[\"bbox5\",\"non_spine_synapse_039\"],[\"bbox5\",\"non_spine_synapse_038\"],[\"bbox5\",\"non_spine_synapse_037\"],[\"bbox5\",\"non_spine_synapse_036\"],[\"bbox5\",\"non_spine_synapse_035\"],[\"bbox5\",\"non_spine_synapse_034\"],[\"bbox5\",\"non_spine_synapse_033\"],[\"bbox5\",\"non_spine_synapse_032\"],[\"bbox5\",\"non_spine_synapse_031\"],[\"bbox5\",\"non_spine_synapse_030\"],[\"bbox5\",\"non_spine_synapse_029\"],[\"bbox5\",\"non_spine_synapse_028\"],[\"bbox5\",\"non_spine_synapse_027\"],[\"bbox5\",\"non_spine_synapse_026\"],[\"bbox5\",\"non_spine_synapse_025\"],[\"bbox5\",\"non_spine_synapse_024\"],[\"bbox5\",\"non_spine_synapse_023\"],[\"bbox5\",\"non_spine_synapse_022\"],[\"bbox5\",\"non_spine_synapse_021\"],[\"bbox5\",\"non_spine_synapse_020\"],[\"bbox5\",\"non_spine_synapse_019\"],[\"bbox5\",\"non_spine_synapse_018\"],[\"bbox5\",\"non_spine_synapse_017\"],[\"bbox5\",\"spine_synapse_003\"],[\"bbox5\",\"spine_synapse_002\"],[\"bbox5\",\"non_spine_synapse_016\"],[\"bbox5\",\"non_spine_synapse_015\"],[\"bbox5\",\"non_spine_synapse_014\"],[\"bbox5\",\"non_spine_synapse_013\"],[\"bbox5\",\"non_spine_synapse_012\"],[\"bbox5\",\"non_spine_synapse_011\"],[\"bbox5\",\"non_spine_synapse_010\"],[\"bbox5\",\"non_spine_synapse_009\"],[\"bbox5\",\"non_spine_synapse_008\"],[\"bbox5\",\"non_spine_synapse_007\"],[\"bbox5\",\"non_spine_synapse_006\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox5\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox5\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[5.9738746,5.601307,5.6571994,6.103901,5.8813534,5.5591617,5.467118,5.1222806,5.443731,5.656317,5.3800054,5.7825794,5.777619,5.7291045,5.723026,5.3576407,5.656458,5.098737,5.4991775,5.5062456,5.334042,5.42541,5.428143,5.685484,5.4760995,5.297463,5.5537944,5.6257753,5.4340334,7.0386553,7.161118,5.079734,5.849782,5.19003,6.066254,5.0431232,7.295348,4.9668546,5.842884,5.4071603,5.8727407,5.337935,5.7475967,5.1678505,6.7131124,7.6985884,4.491574,4.7941628,6.4862995,5.5549474,5.346467,6.0116234,4.2672005,5.1481633,5.4135695,5.235376,5.0175576,5.0662208,4.604719,5.178563,4.9820986,5.253937,5.5445843,5.7992177,5.487877,5.2713995,4.947555,4.9786363,5.9010663,5.568516,5.1878967,4.9511094,5.612332,5.3764815,5.168915,4.808191,5.1414795,5.209918,4.944958,5.4503384,5.609389,5.354393,5.426832,4.8942533,5.1652775,5.603424],\"xaxis\":\"x\",\"y\":[4.810584,5.768322,5.221465,5.1326475,5.3920774,5.48416,5.826811,5.667678,5.435934,4.97268,5.780829,4.908673,5.2530484,4.992185,5.0420613,5.3847103,5.295141,5.148041,5.6463146,4.956923,5.182929,4.980341,5.4177723,5.030325,5.3770924,5.4831767,5.389746,5.212768,5.149822,5.254217,5.248027,6.486047,4.7757826,6.7926083,5.783696,6.4009733,5.1335106,6.183709,4.5040636,5.442799,4.548631,5.4095354,5.8683534,6.2958717,8.085125,7.78085,5.336691,6.1890497,7.6991615,4.5737586,4.661208,7.9977074,5.9447904,7.378923,5.1182604,4.7024083,4.8692904,4.9819856,6.9794936,6.719738,5.0835376,5.1953464,5.8431687,6.451851,4.7876377,4.9404683,5.6743,5.8849764,7.526801,6.7478104,5.0777497,7.178527,4.9181314,7.347968,6.2802877,5.2767973,5.1534743,7.3990526,7.0260773,7.4498677,7.2074065,5.5724335,5.2833233,6.927157,5.742641,7.457143],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox6\",\"spine_synapse_072\"],[\"bbox6\",\"non_spine_synapse_026\"],[\"bbox6\",\"non_spine_synapse_025\"],[\"bbox6\",\"spine_synapse_073\"],[\"bbox6\",\"spine_synapse_071\"],[\"bbox6\",\"spine_synapse_070\"],[\"bbox6\",\"spine_synapse_069\"],[\"bbox6\",\"spine_synapse_068\"],[\"bbox6\",\"spine_synapse_067\"],[\"bbox6\",\"non_spine_synapse_024\"],[\"bbox6\",\"non_spine_synapse_023\"],[\"bbox6\",\"spine_synapse_066\"],[\"bbox6\",\"spine_synapse_065\"],[\"bbox6\",\"spine_synapse_064\"],[\"bbox6\",\"spine_synapse_063\"],[\"bbox6\",\"spine_synapse_062\"],[\"bbox6\",\"spine_synapse_061\"],[\"bbox6\",\"spine_synapse_060\"],[\"bbox6\",\"spine_synapse_059\"],[\"bbox6\",\"spine_synapse_058\"],[\"bbox6\",\"spine_synapse_057\"],[\"bbox6\",\"spine_synapse_056\"],[\"bbox6\",\"spine_synapse_055\"],[\"bbox6\",\"spine_synapse_054\"],[\"bbox6\",\"spine_synapse_053\"],[\"bbox6\",\"spine_synapse_051\"],[\"bbox6\",\"spine_synapse_063\"],[\"bbox6\",\"spine_synapse_050\"],[\"bbox6\",\"spine_synapse_049\"],[\"bbox6\",\"spine_synapse_048\"],[\"bbox6\",\"spine_synapse_047   \"],[\"bbox6\",\"spine_synapse_046\"],[\"bbox6\",\"spine_synapse_045\"],[\"bbox6\",\"non_spine_synapse_022\"],[\"bbox6\",\"spine_synapse_044\"],[\"bbox6\",\"spine_synapse_043\"],[\"bbox6\",\"spine_synapse_042\"],[\"bbox6\",\"spine_synapse_041\"],[\"bbox6\",\"spine_synapse_040\"],[\"bbox6\",\"non_spine_synapse_021\"],[\"bbox6\",\"spine_synapse_039\"],[\"bbox6\",\"spine_synapse_038\"],[\"bbox6\",\"spine_synapse_037\"],[\"bbox6\",\"spine_synapse_036\"],[\"bbox6\",\"spine_synapse_035\"],[\"bbox6\",\"spine_synapse_034\"],[\"bbox6\",\"spine_synapse_033\"],[\"bbox6\",\"spine_synapse_032\"],[\"bbox6\",\"spine_synapse_031\"],[\"bbox6\",\"spine_synapse_030\"],[\"bbox6\",\"spine_synapse_029\"],[\"bbox6\",\"spine_synapse_028\"],[\"bbox6\",\"spine_synapse_027\"],[\"bbox6\",\"spine_synapse_026\"],[\"bbox6\",\"spine_synapse_025\"],[\"bbox6\",\"spine_synapse_024\"],[\"bbox6\",\"spine_synapse_023\"],[\"bbox6\",\"spine_synapse_022\"],[\"bbox6\",\"spine_synapse_021\"],[\"bbox6\",\"spine_synapse_020\"],[\"bbox6\",\"non_spine_synapse_020\"],[\"bbox6\",\"spine_synapse_019\"],[\"bbox6\",\"non_spine_synapse_019\"],[\"bbox6\",\"spine_synapse_018\"],[\"bbox6\",\"spine_synapse_017\"],[\"bbox6\",\"spine_synapse_016\"],[\"bbox6\",\"spine_synapse_015\"],[\"bbox6\",\"spine_synapse_014\"],[\"bbox6\",\"spine_synapse_013\"],[\"bbox6\",\"spine_synapse_012\"],[\"bbox6\",\"spine_synapse_011\"],[\"bbox6\",\"non_spine_synapse_018   \"],[\"bbox6\",\"spine_synapse_010\"],[\"bbox6\",\"spine_synapse_008\"],[\"bbox6\",\"spine_synapse_007\"],[\"bbox6\",\"spine_synapse_006\"],[\"bbox6\",\"spine_synapse_005\"],[\"bbox6\",\"spine_synapse_004\"],[\"bbox6\",\"spine_synapse_003\"],[\"bbox6\",\"spine_synapse_002\"],[\"bbox6\",\"non_spine_synapse_017\"],[\"bbox6\",\"non_spine_synapse_016\"],[\"bbox6\",\"spine_synapse_001\"],[\"bbox6\",\"non_spine_synapse_015\"],[\"bbox6\",\"non_spine_synapse_014\"],[\"bbox6\",\"non_spine_synapse_013\"],[\"bbox6\",\"non_spine_synapse_012\"],[\"bbox6\",\"non_spine_synapse_011\"],[\"bbox6\",\"non_spine_synapse_010\"],[\"bbox6\",\"non_spine_synapse_009\"],[\"bbox6\",\"non_spine_synapse_008\"],[\"bbox6\",\"non_spine_synapse_007\"],[\"bbox6\",\"non_spine_synapse_006\"],[\"bbox6\",\"non_spine_synapse_005\"],[\"bbox6\",\"non_spine_synapse_004\"],[\"bbox6\",\"non_spine_synapse_003\"],[\"bbox6\",\"non_spine_synapse_002\"],[\"bbox6\",\"non_spine_synapse_001\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox6\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox6\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[5.5516405,4.7182803,4.660108,4.518363,5.7620873,6.382896,5.227513,5.5483947,5.8431516,4.930075,6.7839684,4.4657154,6.740739,5.6334734,5.5762324,4.9351544,4.833312,6.4615545,5.79223,5.2197466,5.8570433,6.443044,5.2295094,4.887509,5.7994885,7.1796155,4.7778473,5.8613987,6.53244,5.7513614,7.6550727,5.551396,4.9879117,5.3967752,5.310584,5.69732,4.4922686,7.0045495,6.9170647,6.0404882,4.733388,5.046545,5.72999,5.7402034,4.5323014,4.5541058,6.0729065,5.073003,4.8287435,6.232051,6.5645237,4.7437863,7.357915,5.0111313,4.664418,6.280439,6.8772693,4.7838674,6.0399632,4.711765,6.3515296,4.746116,6.0598435,4.7348275,4.8940277,4.9978385,4.8542843,4.83325,4.580312,5.164249,5.31132,5.096924,5.027365,4.854425,4.945194,4.8034296,6.5321317,4.9198084,5.212032,5.1563935,6.6642323,7.346506,5.240856,5.122102,5.455065,5.2771926,5.971975,5.0602703,8.08123,5.075455,5.3909984,5.800288,5.916781,5.729084,5.3790026,6.3238587,7.3313413,6.306697],\"xaxis\":\"x\",\"y\":[6.3445544,6.9583397,5.681394,6.139238,7.6409755,5.244455,7.07718,7.510227,7.68656,6.8033724,4.843742,6.101495,6.9824667,4.32593,4.873795,6.7886834,6.42131,6.0170546,6.958283,7.128164,5.635093,4.457616,6.9471254,6.7016373,4.7627954,5.556624,6.0622745,7.2830505,6.777718,4.3181605,6.0667667,4.42154,6.653395,4.4562583,6.543924,4.6817265,6.469272,4.521957,5.7280216,4.4136744,6.638803,5.860805,4.8853345,6.177097,5.8538537,5.524816,6.3107963,6.211399,5.4432397,4.9347,4.528188,5.5164137,4.866807,6.7566867,6.287085,4.2625403,4.986957,5.5821724,5.343608,6.379747,4.3832693,6.2734766,4.324419,6.177731,6.130877,6.5714707,6.0513854,6.143765,5.6975393,5.368249,5.552102,5.873147,6.003284,6.3846593,5.86478,5.73287,4.465147,6.024478,6.7477264,5.756844,4.6815677,4.8914404,6.691334,6.4059734,6.9376597,5.9376945,5.3152246,6.5422006,5.576097,6.487182,6.1984415,5.831464,5.9725738,6.5895286,6.195723,6.2208967,5.606689,6.0136857],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"umap_x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"umap_z\"}},\"legend\":{\"title\":{\"text\":\"bbox_name\"},\"tracegroupgap\":0},\"title\":{\"text\":\"UMAP (x vs z)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('aa7bd25e-74da-4b52-8404-ae0688058f8f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"d9b02d33-300d-48fd-b7b1-a59c33e4ccd7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d9b02d33-300d-48fd-b7b1-a59c33e4ccd7\")) {                    Plotly.newPlot(                        \"d9b02d33-300d-48fd-b7b1-a59c33e4ccd7\",                        [{\"customdata\":[[\"bbox1\",\"non_spine_synapsed_056\"],[\"bbox1\",\"non_spine_synapse_057\"],[\"bbox1\",\"non_spine_synapse_058\"],[\"bbox1\",\"non_spine_synapse_063\"],[\"bbox1\",\"non_spine_synapse_062\"],[\"bbox1\",\"non_spine_synapse_061\"],[\"bbox1\",\"non_spine_synapse_060\"],[\"bbox1\",\"non_spine_synapse_059\"],[\"bbox1\",\"non_spine_synapse_054\"],[\"bbox1\",\"non_spine_synapse_053\"],[\"bbox1\",\"non_spine_synapse_052\"],[\"bbox1\",\"non_spine_synapse_051\"],[\"bbox1\",\"non_spine_synapse_050\"],[\"bbox1\",\"non_spine_synapse_049\"],[\"bbox1\",\"non_spine_synapse_048\"],[\"bbox1\",\"non_spine_synapse_047\"],[\"bbox1\",\"non_spine_synapse_046\"],[\"bbox1\",\"non_spine_synapse_045\"],[\"bbox1\",\"non_spine_synapse_044\"],[\"bbox1\",\"non_spine_synapse_040\"],[\"bbox1\",\"non_spine_synapse_038\"],[\"bbox1\",\"non_spine_synapse_037\"],[\"bbox1\",\"non_spine_synapse_036\"],[\"bbox1\",\"non_spine_synapse_035\"],[\"bbox1\",\"non_spine_synapse_034\"],[\"bbox1\",\"non_spine_synapse_033\"],[\"bbox1\",\"non_spine_synapse_032\"],[\"bbox1\",\"non_spine_synapse_031\"],[\"bbox1\",\"non_spine_synapse_030\"],[\"bbox1\",\"non_spine_synapse_029\"],[\"bbox1\",\"non_spine_synapse_028\"],[\"bbox1\",\"non_spine_synapse_027\"],[\"bbox1\",\"non_spine_synapse_026\"],[\"bbox1\",\"non_spine_synapse_025\"],[\"bbox1\",\"non_spine_synapse_024\"],[\"bbox1\",\"non_spine_synapse_023\"],[\"bbox1\",\"non_spine_synapse_022\"],[\"bbox1\",\"non_spine_synapse_021\"],[\"bbox1\",\"non_spine_synapse_020\"],[\"bbox1\",\"non_spine_synapse_019\"],[\"bbox1\",\"non_spine_synapse_018\"],[\"bbox1\",\"non_spine_synapse_017\"],[\"bbox1\",\"non_spine_synapse_016\"],[\"bbox1\",\"non_spine_synapse_015\"],[\"bbox1\",\"non_spine_synapse_014\"],[\"bbox1\",\"non_spine_synapse_013\"],[\"bbox1\",\"non_spine_synapse_012\"],[\"bbox1\",\"non_spine_synapse_011\"],[\"bbox1\",\"non_spine_synapsed_010\"],[\"bbox1\",\"non_spine_synapse_009\"],[\"bbox1\",\"non_spine_synapse_008\"],[\"bbox1\",\"non_spine_synapse_007\"],[\"bbox1\",\"non_spine_synapse_006\"],[\"bbox1\",\"non_spine_synapse_005\"],[\"bbox1\",\"non_spine_synapse_004\"],[\"bbox1\",\"non_spine_synapse_003\"],[\"bbox1\",\"non_spine_synapse_002\"],[\"bbox1\",\"non_spine_synapse_001\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_y=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox1\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.269926,1.4927359,1.1532238,0.86782265,1.3223397,1.5089643,1.3659642,1.9268119,0.69472766,1.1263334,0.99016833,1.4593079,1.703558,1.3703259,1.7580043,0.7027388,2.069424,1.2774208,1.2710545,1.0835494,0.6930185,1.2790532,2.5208964,1.1153023,1.4296037,0.6846901,1.0070198,1.4984587,1.6841856,0.80536443,1.3456845,1.1058097,1.8387506,2.3963609,1.0435407,1.220936,0.84920436,1.3811061,1.417144,1.444629,1.5773708,0.85543925,1.0315205,1.7811003,0.9843891,1.0211095,1.2627047,0.86874527,0.96847725,1.5128411,1.5924027,1.953415,0.7599589,0.62437075,0.72866863,1.4485724,1.5667044,0.79082656],\"xaxis\":\"x\",\"y\":[5.035003,5.2424083,5.1918364,5.529476,5.664883,5.062537,5.614609,5.0859056,5.7297487,5.7426186,5.5077996,5.5896807,5.9728303,5.3389797,5.332006,6.032172,5.9234996,5.6477523,5.633202,5.906338,5.875859,6.910996,6.5052147,5.406056,5.4760785,6.129239,6.1408753,5.7478337,5.1371937,5.8785195,6.480586,6.408336,5.3317175,5.677474,5.7257857,5.7948713,6.1833944,5.6608124,5.7642407,5.9678907,6.055814,5.77364,6.066858,5.790117,6.3045444,5.923527,5.973696,5.4473243,5.9144163,6.250254,6.79778,6.771094,6.63371,6.4336576,6.395868,6.3105893,7.336922,6.59581],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox2\",\"explorative_2024-11-07_Cora_Wolter_133\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_110\"],[\"bbox2\",\"explorative_2024-11-07_Cora_Wolter_132\"],[\"bbox2\",\"explorative_2024-11-07_Cora_Wolter_131\"],[\"bbox2\",\"explorative_2024-10-17_Valentin_Pinkau_130\"],[\"bbox2\",\"explorative_2024-10-17_Valentin_Pinkau_129\"],[\"bbox2\",\"explorative_2024-10-17_Valentin_Pinkau_128\"],[\"bbox2\",\"explorative_2024-10-17_Valentin_Pinkau_123\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_118\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_117\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_116\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_114\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_113\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_111\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_109\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_108\"],[\"bbox2\",\"explorative_2024-10-16_Valentin_Pinkau_107\"],[\"bbox2\",\"explorative_2024-09-02_Ali_Karimi_106\"],[\"bbox2\",\"explorative_2024-09-02_Ali_Karimi_104\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_103\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_102\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_099\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_096\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_094\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_093\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_091\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_090\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_088\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_087\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_085\"],[\"bbox2\",\"explorative_2024-08-29_Vera_Broens_083\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_081\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_080\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_079\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_078\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_077\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_076\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_075\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_074\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_073\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_072\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_071\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_070\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_069\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_068\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_067\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_066\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_065\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_064\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_063\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_062\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_061\"],[\"bbox2\",\"explorative_2024-08-29_Dominic_Evans_060\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_059\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_058\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_054\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_053\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_052\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_051\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_050\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_048\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_047\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_046\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_044\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_043\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_042\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_041\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_040\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_039\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_037\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_036\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_034\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_033\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_032\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_031\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_030\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_029\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_028\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_027\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_026\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_025\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_024\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_023\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_021\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_019\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_018 \"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_016\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_015\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_014\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_013\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_012\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_011\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_010\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_009\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_008\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_007\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_006\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_005\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_004\"],[\"bbox2\",\"explorative_2024-08-28_Cora_Wolter_003\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_y=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox2\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox2\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0.7944705,2.1648886,1.1718825,1.2835804,0.8799315,0.93951553,2.7011032,1.0864222,1.0699046,2.1004853,1.0581764,2.0663826,0.862092,1.0576993,1.0402467,2.747193,1.2094586,2.048154,1.7590544,1.5597826,1.0201838,1.0595052,1.093412,1.143118,1.3962892,1.1781464,2.3989115,2.326682,1.6728816,1.9631786,1.3093699,1.2654364,2.1666765,1.2230918,1.2943376,2.1740785,2.0223427,2.539319,2.0341284,1.6906875,1.5493609,1.9134668,1.7847602,1.4358768,1.471908,1.7919289,1.5929948,2.2551508,2.0927644,2.2863226,2.4061253,1.8521435,2.1170557,1.4517274,2.5414474,2.5423586,2.2169292,1.3557911,1.449908,1.9698429,2.6501026,1.6459545,1.964762,2.529689,1.7304962,1.6374708,2.2935712,2.4429016,2.3806498,2.0209966,2.2303169,1.6711258,2.366551,2.2373788,2.1091962,1.7713856,2.183078,1.8980925,2.1625881,2.082995,2.418133,1.6663271,1.8762665,2.0284035,2.205503,2.4090016,2.2241273,1.8926041,1.7279217,1.8698736,2.1953573,2.565536,2.0111895,2.1620839,1.901595,1.5773185,1.8034174,1.7004234,2.0881076,1.8560284],\"xaxis\":\"x\",\"y\":[6.326063,6.9599447,6.8664436,6.945028,6.2499523,6.4762917,6.1579237,7.055593,6.2574477,7.479023,6.272149,7.2713804,5.8799777,6.352491,5.8582554,6.685192,7.1431885,7.3728633,6.838344,6.791372,6.556036,6.458795,6.7473373,6.383667,7.0697064,6.8196683,6.6358175,6.6040907,7.1285863,7.2127585,6.744717,6.838003,5.912319,6.7256813,6.4736905,6.7729535,6.8277216,7.028124,6.5431223,7.0973277,6.6809044,6.967142,6.8304167,6.5903654,6.4931145,6.6040554,6.847252,7.1217,6.951928,6.794158,6.2832108,5.985091,6.1200767,6.6113234,5.9854255,6.683618,6.4851513,6.4572477,6.406686,6.428871,6.331547,6.594099,6.685112,6.2028675,7.022855,6.81492,6.1181498,5.8165913,6.6808424,6.0758014,5.9262595,6.5408187,6.5603914,6.4436407,6.4843416,6.3322062,6.726312,6.3398266,5.576062,6.155062,6.149462,6.2588873,5.9452724,6.1830163,5.865547,5.9110947,6.184998,6.4239445,6.021743,5.99811,6.2235336,6.0570884,5.802744,6.058542,6.462475,6.397371,6.3199425,6.225245,6.437115,5.9102387],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox3\",\"non_spine_synapse_009\"],[\"bbox3\",\"non_spine_synapse_008\"],[\"bbox3\",\"non_spine_synapse_007\"],[\"bbox3\",\"non_spine_synapse_006\"],[\"bbox3\",\"non_spine_synapse_005\"],[\"bbox3\",\"non_spine_synapse_004\"],[\"bbox3\",\"non_spine_synapse_003\"],[\"bbox3\",\"non_spine_synapse_002\"],[\"bbox3\",\"non_spine_synapse_001\"],[\"bbox3\",\"non_spine_synapse_061\"],[\"bbox3\",\"non_spine_synapse_060\"],[\"bbox3\",\"non_spine_synapse_059\"],[\"bbox3\",\"non_spine_synapse_058\"],[\"bbox3\",\"non_spine_synapse_057\"],[\"bbox3\",\"non_spine_synapse_056\"],[\"bbox3\",\"non_spine_synapse_055\"],[\"bbox3\",\"non_spine_synapse_054\"],[\"bbox3\",\"non_spine_synapse_053\"],[\"bbox3\",\"non_spine_synapse_052\"],[\"bbox3\",\"non_spine_synapse_051\"],[\"bbox3\",\"non_spine_synapse_050\"],[\"bbox3\",\"non_spine_synapse_049\"],[\"bbox3\",\"non_spine_synapse_048\"],[\"bbox3\",\"non_spine_synapse_047\"],[\"bbox3\",\"non_spine_synapse_046\"],[\"bbox3\",\"non_spine_synapse_045\"],[\"bbox3\",\"non_spine_synapse_044\"],[\"bbox3\",\"non_spine_synapse_043\"],[\"bbox3\",\"non_spine_synapse_042\"],[\"bbox3\",\"non_spine_synapse_041\"],[\"bbox3\",\"non_spine_synapse_040\"],[\"bbox3\",\"non_spine_synapse_039\"],[\"bbox3\",\"non_spine_synapse_038\"],[\"bbox3\",\"non_spine_synapse_037\"],[\"bbox3\",\"non_spine_synapse_036\"],[\"bbox3\",\"non_spine_synapse_035\"],[\"bbox3\",\"non_spine_synapse_034\"],[\"bbox3\",\"non_spine_synapse_033\"],[\"bbox3\",\"non_spine_synapse_032\"],[\"bbox3\",\"non_spine_synapse_031\"],[\"bbox3\",\"non_spine_synapse_030\"],[\"bbox3\",\"non_spine_synapse_029\"],[\"bbox3\",\"non_spine_synapse_028\"],[\"bbox3\",\"non_spine_synapse_027\"],[\"bbox3\",\"non_spine_synapse_026\"],[\"bbox3\",\"non_spine_synapse_025\"],[\"bbox3\",\"non_spine_synapse_024\"],[\"bbox3\",\"non_spine_synapse_023\"],[\"bbox3\",\"non_spine_synapse_022\"],[\"bbox3\",\"non_spine_synapse_021\"],[\"bbox3\",\"non_spine_synapse_020\"],[\"bbox3\",\"non_spine_synapse_019\"],[\"bbox3\",\"non_spine_synapse_018\"],[\"bbox3\",\"non_spine_synapse_017\"],[\"bbox3\",\"non_spine_synapse_016\"],[\"bbox3\",\"non_spine_synapse_015\"],[\"bbox3\",\"non_spine_synapse_014\"],[\"bbox3\",\"non_spine_synapse_013\"],[\"bbox3\",\"spine_synapse_001\"],[\"bbox3\",\"non_spine_synapse_012\"],[\"bbox3\",\"non_spine_synapse_011\"],[\"bbox3\",\"non_spine_synapse_010\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_y=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox3\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox3\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.8457015,1.686475,1.8737981,2.2299533,1.8289009,0.6388161,1.3299502,1.7313812,1.7509884,1.5141133,1.3258698,1.4978921,0.8064798,1.1638875,1.7254692,1.5907732,0.88944143,0.8567526,1.531599,1.1478052,0.8017769,0.6001686,1.4490206,1.5194066,1.8443745,0.8697635,1.3961837,1.4920034,1.6265417,1.2887223,1.3745407,1.2895213,0.9368249,1.6376885,0.99691415,1.0752714,0.6379779,1.1098622,1.0169189,1.5988492,1.2196866,0.76476985,1.2046639,1.3135325,1.7057936,0.50954324,0.5841611,1.1370591,1.1295274,0.6477655,1.1357856,1.2876352,1.0257604,0.99883974,1.0728536,0.9430559,0.68055946,1.6398295,1.1454122,0.72018844,1.3482691,1.3218886],\"xaxis\":\"x\",\"y\":[5.7837157,6.2756457,6.3682375,5.7420506,5.977252,6.057221,6.36302,6.4396796,6.634494,6.1962075,5.943683,6.375511,5.6305757,6.0236764,6.213812,6.099872,6.304055,6.1396356,5.904754,5.770087,5.0830474,6.100108,6.142657,6.1587424,5.8253403,5.686188,6.1782293,6.180106,5.6642966,5.822932,5.7539964,6.209315,5.7952657,5.7016587,5.909398,6.0613933,5.675005,6.0899954,5.928738,5.7213287,6.0380397,5.387316,5.38361,4.8597703,5.4195833,5.351725,5.2805867,5.6707196,5.76454,5.3003936,5.774347,5.056572,5.486849,5.1733623,5.5742745,5.3863373,5.4927235,5.8335533,5.151239,5.4143324,5.1814756,5.274105],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox4\",\"explorative_2024-09-02_Ali_Karimi_040\"],[\"bbox4\",\"explorative_2024-09-02_Ali_Karimi_039\"],[\"bbox4\",\"explorative_2024-09-02_Ali_Karimi_038\"],[\"bbox4\",\"explorative_2024-09-02_Ali_Karimi_037\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_036\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_035\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_034\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_033\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_032\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_031\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_030\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_029\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_028\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_027\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_026\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_025\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_024\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_023\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_022\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_021\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_020\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_019\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_018\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_017\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_016\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_015\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_014\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_013\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_012\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_011\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_010\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_009\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_008\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_007\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_006\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_005\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_004\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_003\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_002\"],[\"bbox4\",\"explorative_2024-08-03_Ali_Karimi_001\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_y=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox4\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox4\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0.8739907,1.1295762,0.9395399,2.0209148,1.8136376,0.9903561,2.1734476,1.4791936,1.5725201,2.1885688,1.8121874,1.0148607,1.2110313,1.0932102,2.124795,2.0910962,1.6431801,1.4424651,1.2634959,1.6156607,2.1258335,1.8718227,1.6466628,2.256527,1.9614286,2.0752647,1.7271669,1.6209544,1.3928841,1.7652967,1.975083,1.8527673,1.7297674,1.9683955,1.9491774,2.1201525,1.3256266,1.7156023,2.2603695,1.7769926],\"xaxis\":\"x\",\"y\":[5.3843117,5.9491315,5.5892134,5.7009206,5.270047,5.554914,5.809518,5.392431,5.5581436,5.083759,5.4952717,5.0590787,5.2338133,5.0396967,5.763001,5.535059,4.9971585,5.301975,5.437281,5.360623,5.9235463,5.0552354,5.358806,5.396616,5.027365,5.312774,5.858646,5.46526,5.275495,5.741008,5.181814,5.2399077,5.867944,5.191258,5.5590477,5.8755336,5.2332296,5.514091,5.4478464,5.7356915],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox5\",\"non_spine_synapse_005\"],[\"bbox5\",\"spine_synapse_001\"],[\"bbox5\",\"non_spine_synapse_004\"],[\"bbox5\",\"non_spine_synapse_003\"],[\"bbox5\",\"non_spine_synapse_002\"],[\"bbox5\",\"non_spine_synapse_001\"],[\"bbox5\",\"non_spine_synapse_074\"],[\"bbox5\",\"non_spine_synapse_073\"],[\"bbox5\",\"spine_synapse_012\"],[\"bbox5\",\"non_spine_synapse_072\"],[\"bbox5\",\"spine_synapse_011\"],[\"bbox5\",\"spine_synapse_010\"],[\"bbox5\",\"spine_synapse_009\"],[\"bbox5\",\"spine_synapse_008\"],[\"bbox5\",\"non_spine_synapse_071\"],[\"bbox5\",\"non_spine_synapse_070\"],[\"bbox5\",\"non_spine_synapse_069\"],[\"bbox5\",\"non_spine_synapse_068\"],[\"bbox5\",\"non_spine_synapse_067\"],[\"bbox5\",\"non_spine_synapse_066\"],[\"bbox5\",\"non_spine_synapse_065\"],[\"bbox5\",\"spine_synapse_007\"],[\"bbox5\",\"spine_synapse_006\"],[\"bbox5\",\"spine_synapse_005\"],[\"bbox5\",\"non_spine_synapse_064\"],[\"bbox5\",\"non_spine_synapse_063\"],[\"bbox5\",\"non_spine_synapse_062\"],[\"bbox5\",\"non_spine_synapse_061\"],[\"bbox5\",\"non_spine_synapse_060\"],[\"bbox5\",\"non_spine_synapse_059\"],[\"bbox5\",\"non_spine_synapse_058\"],[\"bbox5\",\"non_spine_synapse_057\"],[\"bbox5\",\"non_spine_synapse_056\"],[\"bbox5\",\"non_spine_synapse_055\"],[\"bbox5\",\"non_spine_synapse_054\"],[\"bbox5\",\"non_spine_synapse_053\"],[\"bbox5\",\"non_spine_synapse_052\"],[\"bbox5\",\"non_spine_synapse_051\"],[\"bbox5\",\"spine_synapse_004\"],[\"bbox5\",\"non_spine_synapse_050\"],[\"bbox5\",\"non_spine_synapse_049\"],[\"bbox5\",\"non_spine_synapse_048\"],[\"bbox5\",\"non_spine_synapse_047\"],[\"bbox5\",\"non_spine_synapse_046\"],[\"bbox5\",\"non_spine_synapse_045\"],[\"bbox5\",\"non_spine_synapse_044\"],[\"bbox5\",\"non_spine_synapse_043\"],[\"bbox5\",\"non_spine_synapse_042\"],[\"bbox5\",\"non_spine_synapse_041\"],[\"bbox5\",\"non_spine_synapse_040\"],[\"bbox5\",\"non_spine_synapse_039\"],[\"bbox5\",\"non_spine_synapse_038\"],[\"bbox5\",\"non_spine_synapse_037\"],[\"bbox5\",\"non_spine_synapse_036\"],[\"bbox5\",\"non_spine_synapse_035\"],[\"bbox5\",\"non_spine_synapse_034\"],[\"bbox5\",\"non_spine_synapse_033\"],[\"bbox5\",\"non_spine_synapse_032\"],[\"bbox5\",\"non_spine_synapse_031\"],[\"bbox5\",\"non_spine_synapse_030\"],[\"bbox5\",\"non_spine_synapse_029\"],[\"bbox5\",\"non_spine_synapse_028\"],[\"bbox5\",\"non_spine_synapse_027\"],[\"bbox5\",\"non_spine_synapse_026\"],[\"bbox5\",\"non_spine_synapse_025\"],[\"bbox5\",\"non_spine_synapse_024\"],[\"bbox5\",\"non_spine_synapse_023\"],[\"bbox5\",\"non_spine_synapse_022\"],[\"bbox5\",\"non_spine_synapse_021\"],[\"bbox5\",\"non_spine_synapse_020\"],[\"bbox5\",\"non_spine_synapse_019\"],[\"bbox5\",\"non_spine_synapse_018\"],[\"bbox5\",\"non_spine_synapse_017\"],[\"bbox5\",\"spine_synapse_003\"],[\"bbox5\",\"spine_synapse_002\"],[\"bbox5\",\"non_spine_synapse_016\"],[\"bbox5\",\"non_spine_synapse_015\"],[\"bbox5\",\"non_spine_synapse_014\"],[\"bbox5\",\"non_spine_synapse_013\"],[\"bbox5\",\"non_spine_synapse_012\"],[\"bbox5\",\"non_spine_synapse_011\"],[\"bbox5\",\"non_spine_synapse_010\"],[\"bbox5\",\"non_spine_synapse_009\"],[\"bbox5\",\"non_spine_synapse_008\"],[\"bbox5\",\"non_spine_synapse_007\"],[\"bbox5\",\"non_spine_synapse_006\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_y=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox5\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox5\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.2354586,2.1958086,1.95095,1.8806335,1.6613055,1.7348299,1.8410894,1.785179,2.044951,1.6531831,1.9505576,1.5341187,1.0755742,1.8902828,1.5393391,1.9683665,1.3752367,1.7046607,1.7065334,1.4406333,1.8443403,1.4468153,1.7080082,1.2467192,1.5985398,1.4973326,1.411685,1.1946311,1.30362,2.300223,0.7707788,1.5616124,0.622007,1.3401212,0.06638969,1.6045223,1.3427358,1.6150775,1.7267298,0.47594178,1.9717605,0.62471974,2.761084,2.1802118,2.1691723,2.5683339,0.94310725,0.3068035,0.7163847,0.38312823,0.68404746,2.4408448,1.3087096,0.9991446,-0.021084063,0.5013392,0.7861882,0.56467265,1.2663199,0.275978,0.45628938,0.013077659,-0.15172225,-0.009798002,0.23991103,0.1832353,0.25381464,-0.047289312,0.77468413,0.1761033,0.3330751,1.593604,0.26586053,0.79371166,0.07373436,0.54997814,0.43319124,1.5541425,2.3159614,1.0535835,0.5651438,0.020549921,0.11775613,0.8379129,0.3904481,2.5239367],\"xaxis\":\"x\",\"y\":[4.810584,5.768322,5.221465,5.1326475,5.3920774,5.48416,5.826811,5.667678,5.435934,4.97268,5.780829,4.908673,5.2530484,4.992185,5.0420613,5.3847103,5.295141,5.148041,5.6463146,4.956923,5.182929,4.980341,5.4177723,5.030325,5.3770924,5.4831767,5.389746,5.212768,5.149822,5.254217,5.248027,6.486047,4.7757826,6.7926083,5.783696,6.4009733,5.1335106,6.183709,4.5040636,5.442799,4.548631,5.4095354,5.8683534,6.2958717,8.085125,7.78085,5.336691,6.1890497,7.6991615,4.5737586,4.661208,7.9977074,5.9447904,7.378923,5.1182604,4.7024083,4.8692904,4.9819856,6.9794936,6.719738,5.0835376,5.1953464,5.8431687,6.451851,4.7876377,4.9404683,5.6743,5.8849764,7.526801,6.7478104,5.0777497,7.178527,4.9181314,7.347968,6.2802877,5.2767973,5.1534743,7.3990526,7.0260773,7.4498677,7.2074065,5.5724335,5.2833233,6.927157,5.742641,7.457143],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[\"bbox6\",\"spine_synapse_072\"],[\"bbox6\",\"non_spine_synapse_026\"],[\"bbox6\",\"non_spine_synapse_025\"],[\"bbox6\",\"spine_synapse_073\"],[\"bbox6\",\"spine_synapse_071\"],[\"bbox6\",\"spine_synapse_070\"],[\"bbox6\",\"spine_synapse_069\"],[\"bbox6\",\"spine_synapse_068\"],[\"bbox6\",\"spine_synapse_067\"],[\"bbox6\",\"non_spine_synapse_024\"],[\"bbox6\",\"non_spine_synapse_023\"],[\"bbox6\",\"spine_synapse_066\"],[\"bbox6\",\"spine_synapse_065\"],[\"bbox6\",\"spine_synapse_064\"],[\"bbox6\",\"spine_synapse_063\"],[\"bbox6\",\"spine_synapse_062\"],[\"bbox6\",\"spine_synapse_061\"],[\"bbox6\",\"spine_synapse_060\"],[\"bbox6\",\"spine_synapse_059\"],[\"bbox6\",\"spine_synapse_058\"],[\"bbox6\",\"spine_synapse_057\"],[\"bbox6\",\"spine_synapse_056\"],[\"bbox6\",\"spine_synapse_055\"],[\"bbox6\",\"spine_synapse_054\"],[\"bbox6\",\"spine_synapse_053\"],[\"bbox6\",\"spine_synapse_051\"],[\"bbox6\",\"spine_synapse_063\"],[\"bbox6\",\"spine_synapse_050\"],[\"bbox6\",\"spine_synapse_049\"],[\"bbox6\",\"spine_synapse_048\"],[\"bbox6\",\"spine_synapse_047   \"],[\"bbox6\",\"spine_synapse_046\"],[\"bbox6\",\"spine_synapse_045\"],[\"bbox6\",\"non_spine_synapse_022\"],[\"bbox6\",\"spine_synapse_044\"],[\"bbox6\",\"spine_synapse_043\"],[\"bbox6\",\"spine_synapse_042\"],[\"bbox6\",\"spine_synapse_041\"],[\"bbox6\",\"spine_synapse_040\"],[\"bbox6\",\"non_spine_synapse_021\"],[\"bbox6\",\"spine_synapse_039\"],[\"bbox6\",\"spine_synapse_038\"],[\"bbox6\",\"spine_synapse_037\"],[\"bbox6\",\"spine_synapse_036\"],[\"bbox6\",\"spine_synapse_035\"],[\"bbox6\",\"spine_synapse_034\"],[\"bbox6\",\"spine_synapse_033\"],[\"bbox6\",\"spine_synapse_032\"],[\"bbox6\",\"spine_synapse_031\"],[\"bbox6\",\"spine_synapse_030\"],[\"bbox6\",\"spine_synapse_029\"],[\"bbox6\",\"spine_synapse_028\"],[\"bbox6\",\"spine_synapse_027\"],[\"bbox6\",\"spine_synapse_026\"],[\"bbox6\",\"spine_synapse_025\"],[\"bbox6\",\"spine_synapse_024\"],[\"bbox6\",\"spine_synapse_023\"],[\"bbox6\",\"spine_synapse_022\"],[\"bbox6\",\"spine_synapse_021\"],[\"bbox6\",\"spine_synapse_020\"],[\"bbox6\",\"non_spine_synapse_020\"],[\"bbox6\",\"spine_synapse_019\"],[\"bbox6\",\"non_spine_synapse_019\"],[\"bbox6\",\"spine_synapse_018\"],[\"bbox6\",\"spine_synapse_017\"],[\"bbox6\",\"spine_synapse_016\"],[\"bbox6\",\"spine_synapse_015\"],[\"bbox6\",\"spine_synapse_014\"],[\"bbox6\",\"spine_synapse_013\"],[\"bbox6\",\"spine_synapse_012\"],[\"bbox6\",\"spine_synapse_011\"],[\"bbox6\",\"non_spine_synapse_018   \"],[\"bbox6\",\"spine_synapse_010\"],[\"bbox6\",\"spine_synapse_008\"],[\"bbox6\",\"spine_synapse_007\"],[\"bbox6\",\"spine_synapse_006\"],[\"bbox6\",\"spine_synapse_005\"],[\"bbox6\",\"spine_synapse_004\"],[\"bbox6\",\"spine_synapse_003\"],[\"bbox6\",\"spine_synapse_002\"],[\"bbox6\",\"non_spine_synapse_017\"],[\"bbox6\",\"non_spine_synapse_016\"],[\"bbox6\",\"spine_synapse_001\"],[\"bbox6\",\"non_spine_synapse_015\"],[\"bbox6\",\"non_spine_synapse_014\"],[\"bbox6\",\"non_spine_synapse_013\"],[\"bbox6\",\"non_spine_synapse_012\"],[\"bbox6\",\"non_spine_synapse_011\"],[\"bbox6\",\"non_spine_synapse_010\"],[\"bbox6\",\"non_spine_synapse_009\"],[\"bbox6\",\"non_spine_synapse_008\"],[\"bbox6\",\"non_spine_synapse_007\"],[\"bbox6\",\"non_spine_synapse_006\"],[\"bbox6\",\"non_spine_synapse_005\"],[\"bbox6\",\"non_spine_synapse_004\"],[\"bbox6\",\"non_spine_synapse_003\"],[\"bbox6\",\"non_spine_synapse_002\"],[\"bbox6\",\"non_spine_synapse_001\"]],\"hovertemplate\":\"bbox_name=%{customdata[0]}\\u003cbr\\u003eumap_y=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cbr\\u003eVar1=%{customdata[1]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"bbox6\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"bbox6\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[3.227021,1.6358249,0.66898304,0.72642076,2.3974304,3.3598304,1.3008735,1.7811091,1.6631522,1.9048952,2.9439027,1.402845,3.6309,1.5611838,2.6700857,1.8447746,2.3611863,3.5685654,3.1201096,2.1629357,3.25016,2.328182,2.59974,1.3568105,2.6606338,3.3667607,0.81062895,2.809072,3.3910985,1.6149882,3.246594,1.0046537,2.1780543,0.8258995,2.866013,0.4435001,1.8626055,1.9411737,3.2940218,2.2883863,0.7778869,2.6574953,0.37146613,3.0781379,1.9038953,1.0750041,3.3617382,2.6073484,0.8229247,2.7971292,1.9524784,1.3268062,2.3604314,0.9404065,1.4119037,1.5751576,2.5936174,1.1025264,2.9585974,1.0577112,1.8039218,1.9831907,1.1003261,2.0808558,0.6611317,1.7542853,1.240304,1.3685106,1.4700996,0.7307389,0.41031393,0.84627247,0.7768766,1.7600399,1.5335617,1.0705366,1.1148101,1.5478966,1.2163827,0.82278734,0.6554125,1.120802,0.8775195,1.0931414,0.7791284,0.3260666,0.09897812,1.2741694,2.3373797,0.8476316,0.38650313,0.027244566,0.035219327,0.28895354,0.49221024,0.06965756,0.46060064,0.006996735],\"xaxis\":\"x\",\"y\":[6.3445544,6.9583397,5.681394,6.139238,7.6409755,5.244455,7.07718,7.510227,7.68656,6.8033724,4.843742,6.101495,6.9824667,4.32593,4.873795,6.7886834,6.42131,6.0170546,6.958283,7.128164,5.635093,4.457616,6.9471254,6.7016373,4.7627954,5.556624,6.0622745,7.2830505,6.777718,4.3181605,6.0667667,4.42154,6.653395,4.4562583,6.543924,4.6817265,6.469272,4.521957,5.7280216,4.4136744,6.638803,5.860805,4.8853345,6.177097,5.8538537,5.524816,6.3107963,6.211399,5.4432397,4.9347,4.528188,5.5164137,4.866807,6.7566867,6.287085,4.2625403,4.986957,5.5821724,5.343608,6.379747,4.3832693,6.2734766,4.324419,6.177731,6.130877,6.5714707,6.0513854,6.143765,5.6975393,5.368249,5.552102,5.873147,6.003284,6.3846593,5.86478,5.73287,4.465147,6.024478,6.7477264,5.756844,4.6815677,4.8914404,6.691334,6.4059734,6.9376597,5.9376945,5.3152246,6.5422006,5.576097,6.487182,6.1984415,5.831464,5.9725738,6.5895286,6.195723,6.2208967,5.606689,6.0136857],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"umap_y\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"umap_z\"}},\"legend\":{\"title\":{\"text\":\"bbox_name\"},\"tracegroupgap\":0},\"title\":{\"text\":\"UMAP (y vs z)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d9b02d33-300d-48fd-b7b1-a59c33e4ccd7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    subplot_titles=[\n",
        "        \"UMAP (x vs y)\",\n",
        "        \"UMAP (x vs z)\",\n",
        "        \"UMAP (y vs z)\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "cat_codes = df[\"bbox_name\"].astype(\"category\").cat.codes\n",
        "trace_xy = go.Scatter(\n",
        "    x=df[\"umap_x\"],\n",
        "    y=df[\"umap_y\"],\n",
        "    mode=\"markers\",\n",
        "    name=\"(x vs y)\",\n",
        "    marker=dict(\n",
        "        color=cat_codes,\n",
        "        colorscale=\"Viridis\",\n",
        "        showscale=True,\n",
        "        size=5\n",
        "    ),\n",
        "    text=df[\"bbox_name\"],    # hover text\n",
        "    hovertemplate=\"<b>bbox_name:%{text}</b><br>umap_x=%{x}<br>umap_y=%{y}<extra></extra>\"\n",
        ")\n",
        "fig.add_trace(trace_xy, row=1, col=1)\n",
        "\n",
        "trace_xz = go.Scatter(\n",
        "    x=df[\"umap_x\"],\n",
        "    y=df[\"umap_z\"],\n",
        "    mode=\"markers\",\n",
        "    name=\"(x vs z)\",\n",
        "    marker=dict(\n",
        "        color=cat_codes,\n",
        "        colorscale=\"Viridis\",\n",
        "        showscale=False,  # we already have a colorbar in the first subplot\n",
        "        size=5\n",
        "    ),\n",
        "    text=df[\"bbox_name\"],\n",
        "    hovertemplate=\"<b>bbox_name:%{text}</b><br>umap_x=%{x}<br>umap_z=%{y}<extra></extra>\"\n",
        ")\n",
        "fig.add_trace(trace_xz, row=1, col=2)\n",
        "\n",
        "# 3) UMAP_y vs UMAP_z\n",
        "trace_yz = go.Scatter(\n",
        "    x=df[\"umap_y\"],\n",
        "    y=df[\"umap_z\"],\n",
        "    mode=\"markers\",\n",
        "    name=\"(y vs z)\",\n",
        "    marker=dict(\n",
        "        color=cat_codes,\n",
        "        colorscale=\"Viridis\",\n",
        "        showscale=False,\n",
        "        size=5\n",
        "    ),\n",
        "    text=df[\"bbox_name\"],\n",
        "    hovertemplate=\"<b>bbox_name:%{text}</b><br>umap_y=%{x}<br>umap_z=%{y}<extra></extra>\"\n",
        ")\n",
        "fig.add_trace(trace_yz, row=1, col=3)\n",
        "\n",
        "# Adjust layout\n",
        "fig.update_layout(\n",
        "    title=\"2D UMAP Projections (All Pairwise Components)\",\n",
        "    width=1800,   # wide figure\n",
        "    height=600,\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "outputId": "7541ae30-bd67-482b-f31c-21bbfbcc5f03",
        "id": "JK5axKVyZcGb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"82d02814-6253-42a3-9c16-e1810ff379c0\" class=\"plotly-graph-div\" style=\"height:600px; width:1800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"82d02814-6253-42a3-9c16-e1810ff379c0\")) {                    Plotly.newPlot(                        \"82d02814-6253-42a3-9c16-e1810ff379c0\",                        [{\"hovertemplate\":\"\\u003cb\\u003ebbox_name:%{text}\\u003c\\u002fb\\u003e\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_y=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5],\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"showscale\":true,\"size\":5},\"mode\":\"markers\",\"name\":\"(x vs y)\",\"text\":[\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\"],\"x\":[6.320848,6.868704,6.527309,5.7201967,5.504691,6.368925,5.584473,6.5815415,6.0709996,5.547961,6.8613815,5.539868,7.9374447,5.977072,7.0375266,5.73709,7.845846,5.671688,5.7251077,7.393874,5.9098163,7.603018,7.804602,6.8634253,7.083628,6.8541102,5.6302857,7.454782,6.653799,6.8165684,7.8831396,7.42407,6.6293106,7.413919,5.8100405,5.655298,5.9776444,6.7837343,5.727114,7.1478252,7.5097566,5.7703094,6.0439177,7.275848,7.187259,5.7678304,5.625099,6.057722,7.1064067,7.548423,7.4722323,7.786518,6.8941984,6.349089,5.96265,7.2368374,7.354787,6.1669087,6.471332,7.5181804,6.8196044,7.1974874,6.0142126,6.4876976,7.136677,6.6587753,5.6834297,7.0344157,6.272738,7.0256357,6.322197,6.745193,6.142396,7.042722,6.3555856,6.531251,7.272558,6.827308,5.869167,6.0442777,6.3084316,6.022226,6.66891,6.550416,5.9476347,7.1514087,6.5631356,6.698946,5.925877,6.4896374,7.283149,6.410516,5.743928,7.3338947,6.6301923,6.5739636,7.2799606,6.2305193,6.6339893,6.408582,5.8733983,6.067681,6.0072026,7.0327215,5.886594,6.6982126,6.2971234,6.7953167,7.000367,7.0031757,7.177908,6.6612144,6.705218,6.577227,6.811989,5.8185253,6.991147,6.618037,6.862479,6.3358097,5.721546,6.3934493,6.0909424,5.8245454,6.564363,6.8570747,6.3224053,6.2827277,6.881747,6.092355,6.065134,6.3030496,5.9195414,5.5396104,5.898544,6.642313,6.7227163,6.6150155,6.28568,6.9710894,6.74594,6.098579,6.474288,6.0101795,6.029689,5.860342,6.8189664,6.6419573,5.9279537,6.186878,6.659691,5.9242744,5.96739,6.434012,6.252948,6.0649095,6.628577,6.2640266,6.883459,6.6475296,5.8160086,6.370837,5.8936477,6.402843,6.0717006,5.956161,6.043161,6.5802774,6.90029,6.5803432,6.7610893,6.714982,5.965941,6.169317,6.5512986,6.6295857,6.604784,6.7101054,6.615584,6.0637126,6.599324,6.1229477,6.408752,6.659329,6.453432,5.884198,6.7596426,6.4507174,6.731519,6.311013,6.2546487,6.6682267,6.824485,6.013682,6.178463,6.419103,6.633221,6.112539,6.253562,6.630824,6.7976365,6.2445617,6.8099837,6.364212,6.350959,6.4317384,6.0656176,6.2572565,6.7914867,6.5521812,6.12482,6.3774133,6.3220057,6.449023,6.213191,6.207129,6.29685,5.97172,6.6198497,6.520426,6.249248,6.1131454,6.0224733,6.428279,6.4086185,6.1801815,5.8169684,6.378914,6.325536,5.993609,6.46431,6.096682,5.9950237,6.1605315,5.918073,6.086242,6.3906775,6.4350557,6.190655,6.3768654,5.4771075,6.2515664,6.1293797,6.1048007,6.2216773,6.013161,6.0574145,6.0381093,5.8084116,6.061588,6.267484,5.9901223,5.8095207,5.8964863,5.875873,5.7806754,5.874742,6.0463257,5.655362,5.648787,5.9738746,5.601307,5.6571994,6.103901,5.8813534,5.5591617,5.467118,5.1222806,5.443731,5.656317,5.3800054,5.7825794,5.777619,5.7291045,5.723026,5.3576407,5.656458,5.098737,5.4991775,5.5062456,5.334042,5.42541,5.428143,5.685484,5.4760995,5.297463,5.5537944,5.6257753,5.4340334,7.0386553,7.161118,5.079734,5.849782,5.19003,6.066254,5.0431232,7.295348,4.9668546,5.842884,5.4071603,5.8727407,5.337935,5.7475967,5.1678505,6.7131124,7.6985884,4.491574,4.7941628,6.4862995,5.5549474,5.346467,6.0116234,4.2672005,5.1481633,5.4135695,5.235376,5.0175576,5.0662208,4.604719,5.178563,4.9820986,5.253937,5.5445843,5.7992177,5.487877,5.2713995,4.947555,4.9786363,5.9010663,5.568516,5.1878967,4.9511094,5.612332,5.3764815,5.168915,4.808191,5.1414795,5.209918,4.944958,5.4503384,5.609389,5.354393,5.426832,4.8942533,5.1652775,5.603424,5.5516405,4.7182803,4.660108,4.518363,5.7620873,6.382896,5.227513,5.5483947,5.8431516,4.930075,6.7839684,4.4657154,6.740739,5.6334734,5.5762324,4.9351544,4.833312,6.4615545,5.79223,5.2197466,5.8570433,6.443044,5.2295094,4.887509,5.7994885,7.1796155,4.7778473,5.8613987,6.53244,5.7513614,7.6550727,5.551396,4.9879117,5.3967752,5.310584,5.69732,4.4922686,7.0045495,6.9170647,6.0404882,4.733388,5.046545,5.72999,5.7402034,4.5323014,4.5541058,6.0729065,5.073003,4.8287435,6.232051,6.5645237,4.7437863,7.357915,5.0111313,4.664418,6.280439,6.8772693,4.7838674,6.0399632,4.711765,6.3515296,4.746116,6.0598435,4.7348275,4.8940277,4.9978385,4.8542843,4.83325,4.580312,5.164249,5.31132,5.096924,5.027365,4.854425,4.945194,4.8034296,6.5321317,4.9198084,5.212032,5.1563935,6.6642323,7.346506,5.240856,5.122102,5.455065,5.2771926,5.971975,5.0602703,8.08123,5.075455,5.3909984,5.800288,5.916781,5.729084,5.3790026,6.3238587,7.3313413,6.306697],\"y\":[1.269926,1.4927359,1.1532238,0.86782265,1.3223397,1.5089643,1.3659642,1.9268119,0.69472766,1.1263334,0.99016833,1.4593079,1.703558,1.3703259,1.7580043,0.7027388,2.069424,1.2774208,1.2710545,1.0835494,0.6930185,1.2790532,2.5208964,1.1153023,1.4296037,0.6846901,1.0070198,1.4984587,1.6841856,0.80536443,1.3456845,1.1058097,1.8387506,2.3963609,1.0435407,1.220936,0.84920436,1.3811061,1.417144,1.444629,1.5773708,0.85543925,1.0315205,1.7811003,0.9843891,1.0211095,1.2627047,0.86874527,0.96847725,1.5128411,1.5924027,1.953415,0.7599589,0.62437075,0.72866863,1.4485724,1.5667044,0.79082656,0.7944705,2.1648886,1.1718825,1.2835804,0.8799315,0.93951553,2.7011032,1.0864222,1.0699046,2.1004853,1.0581764,2.0663826,0.862092,1.0576993,1.0402467,2.747193,1.2094586,2.048154,1.7590544,1.5597826,1.0201838,1.0595052,1.093412,1.143118,1.3962892,1.1781464,2.3989115,2.326682,1.6728816,1.9631786,1.3093699,1.2654364,2.1666765,1.2230918,1.2943376,2.1740785,2.0223427,2.539319,2.0341284,1.6906875,1.5493609,1.9134668,1.7847602,1.4358768,1.471908,1.7919289,1.5929948,2.2551508,2.0927644,2.2863226,2.4061253,1.8521435,2.1170557,1.4517274,2.5414474,2.5423586,2.2169292,1.3557911,1.449908,1.9698429,2.6501026,1.6459545,1.964762,2.529689,1.7304962,1.6374708,2.2935712,2.4429016,2.3806498,2.0209966,2.2303169,1.6711258,2.366551,2.2373788,2.1091962,1.7713856,2.183078,1.8980925,2.1625881,2.082995,2.418133,1.6663271,1.8762665,2.0284035,2.205503,2.4090016,2.2241273,1.8926041,1.7279217,1.8698736,2.1953573,2.565536,2.0111895,2.1620839,1.901595,1.5773185,1.8034174,1.7004234,2.0881076,1.8560284,1.8457015,1.686475,1.8737981,2.2299533,1.8289009,0.6388161,1.3299502,1.7313812,1.7509884,1.5141133,1.3258698,1.4978921,0.8064798,1.1638875,1.7254692,1.5907732,0.88944143,0.8567526,1.531599,1.1478052,0.8017769,0.6001686,1.4490206,1.5194066,1.8443745,0.8697635,1.3961837,1.4920034,1.6265417,1.2887223,1.3745407,1.2895213,0.9368249,1.6376885,0.99691415,1.0752714,0.6379779,1.1098622,1.0169189,1.5988492,1.2196866,0.76476985,1.2046639,1.3135325,1.7057936,0.50954324,0.5841611,1.1370591,1.1295274,0.6477655,1.1357856,1.2876352,1.0257604,0.99883974,1.0728536,0.9430559,0.68055946,1.6398295,1.1454122,0.72018844,1.3482691,1.3218886,0.8739907,1.1295762,0.9395399,2.0209148,1.8136376,0.9903561,2.1734476,1.4791936,1.5725201,2.1885688,1.8121874,1.0148607,1.2110313,1.0932102,2.124795,2.0910962,1.6431801,1.4424651,1.2634959,1.6156607,2.1258335,1.8718227,1.6466628,2.256527,1.9614286,2.0752647,1.7271669,1.6209544,1.3928841,1.7652967,1.975083,1.8527673,1.7297674,1.9683955,1.9491774,2.1201525,1.3256266,1.7156023,2.2603695,1.7769926,1.2354586,2.1958086,1.95095,1.8806335,1.6613055,1.7348299,1.8410894,1.785179,2.044951,1.6531831,1.9505576,1.5341187,1.0755742,1.8902828,1.5393391,1.9683665,1.3752367,1.7046607,1.7065334,1.4406333,1.8443403,1.4468153,1.7080082,1.2467192,1.5985398,1.4973326,1.411685,1.1946311,1.30362,2.300223,0.7707788,1.5616124,0.622007,1.3401212,0.06638969,1.6045223,1.3427358,1.6150775,1.7267298,0.47594178,1.9717605,0.62471974,2.761084,2.1802118,2.1691723,2.5683339,0.94310725,0.3068035,0.7163847,0.38312823,0.68404746,2.4408448,1.3087096,0.9991446,-0.021084063,0.5013392,0.7861882,0.56467265,1.2663199,0.275978,0.45628938,0.013077659,-0.15172225,-0.009798002,0.23991103,0.1832353,0.25381464,-0.047289312,0.77468413,0.1761033,0.3330751,1.593604,0.26586053,0.79371166,0.07373436,0.54997814,0.43319124,1.5541425,2.3159614,1.0535835,0.5651438,0.020549921,0.11775613,0.8379129,0.3904481,2.5239367,3.227021,1.6358249,0.66898304,0.72642076,2.3974304,3.3598304,1.3008735,1.7811091,1.6631522,1.9048952,2.9439027,1.402845,3.6309,1.5611838,2.6700857,1.8447746,2.3611863,3.5685654,3.1201096,2.1629357,3.25016,2.328182,2.59974,1.3568105,2.6606338,3.3667607,0.81062895,2.809072,3.3910985,1.6149882,3.246594,1.0046537,2.1780543,0.8258995,2.866013,0.4435001,1.8626055,1.9411737,3.2940218,2.2883863,0.7778869,2.6574953,0.37146613,3.0781379,1.9038953,1.0750041,3.3617382,2.6073484,0.8229247,2.7971292,1.9524784,1.3268062,2.3604314,0.9404065,1.4119037,1.5751576,2.5936174,1.1025264,2.9585974,1.0577112,1.8039218,1.9831907,1.1003261,2.0808558,0.6611317,1.7542853,1.240304,1.3685106,1.4700996,0.7307389,0.41031393,0.84627247,0.7768766,1.7600399,1.5335617,1.0705366,1.1148101,1.5478966,1.2163827,0.82278734,0.6554125,1.120802,0.8775195,1.0931414,0.7791284,0.3260666,0.09897812,1.2741694,2.3373797,0.8476316,0.38650313,0.027244566,0.035219327,0.28895354,0.49221024,0.06965756,0.46060064,0.006996735],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"hovertemplate\":\"\\u003cb\\u003ebbox_name:%{text}\\u003c\\u002fb\\u003e\\u003cbr\\u003eumap_x=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5],\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"showscale\":false,\"size\":5},\"mode\":\"markers\",\"name\":\"(x vs z)\",\"text\":[\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\"],\"x\":[6.320848,6.868704,6.527309,5.7201967,5.504691,6.368925,5.584473,6.5815415,6.0709996,5.547961,6.8613815,5.539868,7.9374447,5.977072,7.0375266,5.73709,7.845846,5.671688,5.7251077,7.393874,5.9098163,7.603018,7.804602,6.8634253,7.083628,6.8541102,5.6302857,7.454782,6.653799,6.8165684,7.8831396,7.42407,6.6293106,7.413919,5.8100405,5.655298,5.9776444,6.7837343,5.727114,7.1478252,7.5097566,5.7703094,6.0439177,7.275848,7.187259,5.7678304,5.625099,6.057722,7.1064067,7.548423,7.4722323,7.786518,6.8941984,6.349089,5.96265,7.2368374,7.354787,6.1669087,6.471332,7.5181804,6.8196044,7.1974874,6.0142126,6.4876976,7.136677,6.6587753,5.6834297,7.0344157,6.272738,7.0256357,6.322197,6.745193,6.142396,7.042722,6.3555856,6.531251,7.272558,6.827308,5.869167,6.0442777,6.3084316,6.022226,6.66891,6.550416,5.9476347,7.1514087,6.5631356,6.698946,5.925877,6.4896374,7.283149,6.410516,5.743928,7.3338947,6.6301923,6.5739636,7.2799606,6.2305193,6.6339893,6.408582,5.8733983,6.067681,6.0072026,7.0327215,5.886594,6.6982126,6.2971234,6.7953167,7.000367,7.0031757,7.177908,6.6612144,6.705218,6.577227,6.811989,5.8185253,6.991147,6.618037,6.862479,6.3358097,5.721546,6.3934493,6.0909424,5.8245454,6.564363,6.8570747,6.3224053,6.2827277,6.881747,6.092355,6.065134,6.3030496,5.9195414,5.5396104,5.898544,6.642313,6.7227163,6.6150155,6.28568,6.9710894,6.74594,6.098579,6.474288,6.0101795,6.029689,5.860342,6.8189664,6.6419573,5.9279537,6.186878,6.659691,5.9242744,5.96739,6.434012,6.252948,6.0649095,6.628577,6.2640266,6.883459,6.6475296,5.8160086,6.370837,5.8936477,6.402843,6.0717006,5.956161,6.043161,6.5802774,6.90029,6.5803432,6.7610893,6.714982,5.965941,6.169317,6.5512986,6.6295857,6.604784,6.7101054,6.615584,6.0637126,6.599324,6.1229477,6.408752,6.659329,6.453432,5.884198,6.7596426,6.4507174,6.731519,6.311013,6.2546487,6.6682267,6.824485,6.013682,6.178463,6.419103,6.633221,6.112539,6.253562,6.630824,6.7976365,6.2445617,6.8099837,6.364212,6.350959,6.4317384,6.0656176,6.2572565,6.7914867,6.5521812,6.12482,6.3774133,6.3220057,6.449023,6.213191,6.207129,6.29685,5.97172,6.6198497,6.520426,6.249248,6.1131454,6.0224733,6.428279,6.4086185,6.1801815,5.8169684,6.378914,6.325536,5.993609,6.46431,6.096682,5.9950237,6.1605315,5.918073,6.086242,6.3906775,6.4350557,6.190655,6.3768654,5.4771075,6.2515664,6.1293797,6.1048007,6.2216773,6.013161,6.0574145,6.0381093,5.8084116,6.061588,6.267484,5.9901223,5.8095207,5.8964863,5.875873,5.7806754,5.874742,6.0463257,5.655362,5.648787,5.9738746,5.601307,5.6571994,6.103901,5.8813534,5.5591617,5.467118,5.1222806,5.443731,5.656317,5.3800054,5.7825794,5.777619,5.7291045,5.723026,5.3576407,5.656458,5.098737,5.4991775,5.5062456,5.334042,5.42541,5.428143,5.685484,5.4760995,5.297463,5.5537944,5.6257753,5.4340334,7.0386553,7.161118,5.079734,5.849782,5.19003,6.066254,5.0431232,7.295348,4.9668546,5.842884,5.4071603,5.8727407,5.337935,5.7475967,5.1678505,6.7131124,7.6985884,4.491574,4.7941628,6.4862995,5.5549474,5.346467,6.0116234,4.2672005,5.1481633,5.4135695,5.235376,5.0175576,5.0662208,4.604719,5.178563,4.9820986,5.253937,5.5445843,5.7992177,5.487877,5.2713995,4.947555,4.9786363,5.9010663,5.568516,5.1878967,4.9511094,5.612332,5.3764815,5.168915,4.808191,5.1414795,5.209918,4.944958,5.4503384,5.609389,5.354393,5.426832,4.8942533,5.1652775,5.603424,5.5516405,4.7182803,4.660108,4.518363,5.7620873,6.382896,5.227513,5.5483947,5.8431516,4.930075,6.7839684,4.4657154,6.740739,5.6334734,5.5762324,4.9351544,4.833312,6.4615545,5.79223,5.2197466,5.8570433,6.443044,5.2295094,4.887509,5.7994885,7.1796155,4.7778473,5.8613987,6.53244,5.7513614,7.6550727,5.551396,4.9879117,5.3967752,5.310584,5.69732,4.4922686,7.0045495,6.9170647,6.0404882,4.733388,5.046545,5.72999,5.7402034,4.5323014,4.5541058,6.0729065,5.073003,4.8287435,6.232051,6.5645237,4.7437863,7.357915,5.0111313,4.664418,6.280439,6.8772693,4.7838674,6.0399632,4.711765,6.3515296,4.746116,6.0598435,4.7348275,4.8940277,4.9978385,4.8542843,4.83325,4.580312,5.164249,5.31132,5.096924,5.027365,4.854425,4.945194,4.8034296,6.5321317,4.9198084,5.212032,5.1563935,6.6642323,7.346506,5.240856,5.122102,5.455065,5.2771926,5.971975,5.0602703,8.08123,5.075455,5.3909984,5.800288,5.916781,5.729084,5.3790026,6.3238587,7.3313413,6.306697],\"y\":[5.035003,5.2424083,5.1918364,5.529476,5.664883,5.062537,5.614609,5.0859056,5.7297487,5.7426186,5.5077996,5.5896807,5.9728303,5.3389797,5.332006,6.032172,5.9234996,5.6477523,5.633202,5.906338,5.875859,6.910996,6.5052147,5.406056,5.4760785,6.129239,6.1408753,5.7478337,5.1371937,5.8785195,6.480586,6.408336,5.3317175,5.677474,5.7257857,5.7948713,6.1833944,5.6608124,5.7642407,5.9678907,6.055814,5.77364,6.066858,5.790117,6.3045444,5.923527,5.973696,5.4473243,5.9144163,6.250254,6.79778,6.771094,6.63371,6.4336576,6.395868,6.3105893,7.336922,6.59581,6.326063,6.9599447,6.8664436,6.945028,6.2499523,6.4762917,6.1579237,7.055593,6.2574477,7.479023,6.272149,7.2713804,5.8799777,6.352491,5.8582554,6.685192,7.1431885,7.3728633,6.838344,6.791372,6.556036,6.458795,6.7473373,6.383667,7.0697064,6.8196683,6.6358175,6.6040907,7.1285863,7.2127585,6.744717,6.838003,5.912319,6.7256813,6.4736905,6.7729535,6.8277216,7.028124,6.5431223,7.0973277,6.6809044,6.967142,6.8304167,6.5903654,6.4931145,6.6040554,6.847252,7.1217,6.951928,6.794158,6.2832108,5.985091,6.1200767,6.6113234,5.9854255,6.683618,6.4851513,6.4572477,6.406686,6.428871,6.331547,6.594099,6.685112,6.2028675,7.022855,6.81492,6.1181498,5.8165913,6.6808424,6.0758014,5.9262595,6.5408187,6.5603914,6.4436407,6.4843416,6.3322062,6.726312,6.3398266,5.576062,6.155062,6.149462,6.2588873,5.9452724,6.1830163,5.865547,5.9110947,6.184998,6.4239445,6.021743,5.99811,6.2235336,6.0570884,5.802744,6.058542,6.462475,6.397371,6.3199425,6.225245,6.437115,5.9102387,5.7837157,6.2756457,6.3682375,5.7420506,5.977252,6.057221,6.36302,6.4396796,6.634494,6.1962075,5.943683,6.375511,5.6305757,6.0236764,6.213812,6.099872,6.304055,6.1396356,5.904754,5.770087,5.0830474,6.100108,6.142657,6.1587424,5.8253403,5.686188,6.1782293,6.180106,5.6642966,5.822932,5.7539964,6.209315,5.7952657,5.7016587,5.909398,6.0613933,5.675005,6.0899954,5.928738,5.7213287,6.0380397,5.387316,5.38361,4.8597703,5.4195833,5.351725,5.2805867,5.6707196,5.76454,5.3003936,5.774347,5.056572,5.486849,5.1733623,5.5742745,5.3863373,5.4927235,5.8335533,5.151239,5.4143324,5.1814756,5.274105,5.3843117,5.9491315,5.5892134,5.7009206,5.270047,5.554914,5.809518,5.392431,5.5581436,5.083759,5.4952717,5.0590787,5.2338133,5.0396967,5.763001,5.535059,4.9971585,5.301975,5.437281,5.360623,5.9235463,5.0552354,5.358806,5.396616,5.027365,5.312774,5.858646,5.46526,5.275495,5.741008,5.181814,5.2399077,5.867944,5.191258,5.5590477,5.8755336,5.2332296,5.514091,5.4478464,5.7356915,4.810584,5.768322,5.221465,5.1326475,5.3920774,5.48416,5.826811,5.667678,5.435934,4.97268,5.780829,4.908673,5.2530484,4.992185,5.0420613,5.3847103,5.295141,5.148041,5.6463146,4.956923,5.182929,4.980341,5.4177723,5.030325,5.3770924,5.4831767,5.389746,5.212768,5.149822,5.254217,5.248027,6.486047,4.7757826,6.7926083,5.783696,6.4009733,5.1335106,6.183709,4.5040636,5.442799,4.548631,5.4095354,5.8683534,6.2958717,8.085125,7.78085,5.336691,6.1890497,7.6991615,4.5737586,4.661208,7.9977074,5.9447904,7.378923,5.1182604,4.7024083,4.8692904,4.9819856,6.9794936,6.719738,5.0835376,5.1953464,5.8431687,6.451851,4.7876377,4.9404683,5.6743,5.8849764,7.526801,6.7478104,5.0777497,7.178527,4.9181314,7.347968,6.2802877,5.2767973,5.1534743,7.3990526,7.0260773,7.4498677,7.2074065,5.5724335,5.2833233,6.927157,5.742641,7.457143,6.3445544,6.9583397,5.681394,6.139238,7.6409755,5.244455,7.07718,7.510227,7.68656,6.8033724,4.843742,6.101495,6.9824667,4.32593,4.873795,6.7886834,6.42131,6.0170546,6.958283,7.128164,5.635093,4.457616,6.9471254,6.7016373,4.7627954,5.556624,6.0622745,7.2830505,6.777718,4.3181605,6.0667667,4.42154,6.653395,4.4562583,6.543924,4.6817265,6.469272,4.521957,5.7280216,4.4136744,6.638803,5.860805,4.8853345,6.177097,5.8538537,5.524816,6.3107963,6.211399,5.4432397,4.9347,4.528188,5.5164137,4.866807,6.7566867,6.287085,4.2625403,4.986957,5.5821724,5.343608,6.379747,4.3832693,6.2734766,4.324419,6.177731,6.130877,6.5714707,6.0513854,6.143765,5.6975393,5.368249,5.552102,5.873147,6.003284,6.3846593,5.86478,5.73287,4.465147,6.024478,6.7477264,5.756844,4.6815677,4.8914404,6.691334,6.4059734,6.9376597,5.9376945,5.3152246,6.5422006,5.576097,6.487182,6.1984415,5.831464,5.9725738,6.5895286,6.195723,6.2208967,5.606689,6.0136857],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"hovertemplate\":\"\\u003cb\\u003ebbox_name:%{text}\\u003c\\u002fb\\u003e\\u003cbr\\u003eumap_y=%{x}\\u003cbr\\u003eumap_z=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5],\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"showscale\":false,\"size\":5},\"mode\":\"markers\",\"name\":\"(y vs z)\",\"text\":[\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox1\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox2\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox3\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox4\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox5\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\",\"bbox6\"],\"x\":[1.269926,1.4927359,1.1532238,0.86782265,1.3223397,1.5089643,1.3659642,1.9268119,0.69472766,1.1263334,0.99016833,1.4593079,1.703558,1.3703259,1.7580043,0.7027388,2.069424,1.2774208,1.2710545,1.0835494,0.6930185,1.2790532,2.5208964,1.1153023,1.4296037,0.6846901,1.0070198,1.4984587,1.6841856,0.80536443,1.3456845,1.1058097,1.8387506,2.3963609,1.0435407,1.220936,0.84920436,1.3811061,1.417144,1.444629,1.5773708,0.85543925,1.0315205,1.7811003,0.9843891,1.0211095,1.2627047,0.86874527,0.96847725,1.5128411,1.5924027,1.953415,0.7599589,0.62437075,0.72866863,1.4485724,1.5667044,0.79082656,0.7944705,2.1648886,1.1718825,1.2835804,0.8799315,0.93951553,2.7011032,1.0864222,1.0699046,2.1004853,1.0581764,2.0663826,0.862092,1.0576993,1.0402467,2.747193,1.2094586,2.048154,1.7590544,1.5597826,1.0201838,1.0595052,1.093412,1.143118,1.3962892,1.1781464,2.3989115,2.326682,1.6728816,1.9631786,1.3093699,1.2654364,2.1666765,1.2230918,1.2943376,2.1740785,2.0223427,2.539319,2.0341284,1.6906875,1.5493609,1.9134668,1.7847602,1.4358768,1.471908,1.7919289,1.5929948,2.2551508,2.0927644,2.2863226,2.4061253,1.8521435,2.1170557,1.4517274,2.5414474,2.5423586,2.2169292,1.3557911,1.449908,1.9698429,2.6501026,1.6459545,1.964762,2.529689,1.7304962,1.6374708,2.2935712,2.4429016,2.3806498,2.0209966,2.2303169,1.6711258,2.366551,2.2373788,2.1091962,1.7713856,2.183078,1.8980925,2.1625881,2.082995,2.418133,1.6663271,1.8762665,2.0284035,2.205503,2.4090016,2.2241273,1.8926041,1.7279217,1.8698736,2.1953573,2.565536,2.0111895,2.1620839,1.901595,1.5773185,1.8034174,1.7004234,2.0881076,1.8560284,1.8457015,1.686475,1.8737981,2.2299533,1.8289009,0.6388161,1.3299502,1.7313812,1.7509884,1.5141133,1.3258698,1.4978921,0.8064798,1.1638875,1.7254692,1.5907732,0.88944143,0.8567526,1.531599,1.1478052,0.8017769,0.6001686,1.4490206,1.5194066,1.8443745,0.8697635,1.3961837,1.4920034,1.6265417,1.2887223,1.3745407,1.2895213,0.9368249,1.6376885,0.99691415,1.0752714,0.6379779,1.1098622,1.0169189,1.5988492,1.2196866,0.76476985,1.2046639,1.3135325,1.7057936,0.50954324,0.5841611,1.1370591,1.1295274,0.6477655,1.1357856,1.2876352,1.0257604,0.99883974,1.0728536,0.9430559,0.68055946,1.6398295,1.1454122,0.72018844,1.3482691,1.3218886,0.8739907,1.1295762,0.9395399,2.0209148,1.8136376,0.9903561,2.1734476,1.4791936,1.5725201,2.1885688,1.8121874,1.0148607,1.2110313,1.0932102,2.124795,2.0910962,1.6431801,1.4424651,1.2634959,1.6156607,2.1258335,1.8718227,1.6466628,2.256527,1.9614286,2.0752647,1.7271669,1.6209544,1.3928841,1.7652967,1.975083,1.8527673,1.7297674,1.9683955,1.9491774,2.1201525,1.3256266,1.7156023,2.2603695,1.7769926,1.2354586,2.1958086,1.95095,1.8806335,1.6613055,1.7348299,1.8410894,1.785179,2.044951,1.6531831,1.9505576,1.5341187,1.0755742,1.8902828,1.5393391,1.9683665,1.3752367,1.7046607,1.7065334,1.4406333,1.8443403,1.4468153,1.7080082,1.2467192,1.5985398,1.4973326,1.411685,1.1946311,1.30362,2.300223,0.7707788,1.5616124,0.622007,1.3401212,0.06638969,1.6045223,1.3427358,1.6150775,1.7267298,0.47594178,1.9717605,0.62471974,2.761084,2.1802118,2.1691723,2.5683339,0.94310725,0.3068035,0.7163847,0.38312823,0.68404746,2.4408448,1.3087096,0.9991446,-0.021084063,0.5013392,0.7861882,0.56467265,1.2663199,0.275978,0.45628938,0.013077659,-0.15172225,-0.009798002,0.23991103,0.1832353,0.25381464,-0.047289312,0.77468413,0.1761033,0.3330751,1.593604,0.26586053,0.79371166,0.07373436,0.54997814,0.43319124,1.5541425,2.3159614,1.0535835,0.5651438,0.020549921,0.11775613,0.8379129,0.3904481,2.5239367,3.227021,1.6358249,0.66898304,0.72642076,2.3974304,3.3598304,1.3008735,1.7811091,1.6631522,1.9048952,2.9439027,1.402845,3.6309,1.5611838,2.6700857,1.8447746,2.3611863,3.5685654,3.1201096,2.1629357,3.25016,2.328182,2.59974,1.3568105,2.6606338,3.3667607,0.81062895,2.809072,3.3910985,1.6149882,3.246594,1.0046537,2.1780543,0.8258995,2.866013,0.4435001,1.8626055,1.9411737,3.2940218,2.2883863,0.7778869,2.6574953,0.37146613,3.0781379,1.9038953,1.0750041,3.3617382,2.6073484,0.8229247,2.7971292,1.9524784,1.3268062,2.3604314,0.9404065,1.4119037,1.5751576,2.5936174,1.1025264,2.9585974,1.0577112,1.8039218,1.9831907,1.1003261,2.0808558,0.6611317,1.7542853,1.240304,1.3685106,1.4700996,0.7307389,0.41031393,0.84627247,0.7768766,1.7600399,1.5335617,1.0705366,1.1148101,1.5478966,1.2163827,0.82278734,0.6554125,1.120802,0.8775195,1.0931414,0.7791284,0.3260666,0.09897812,1.2741694,2.3373797,0.8476316,0.38650313,0.027244566,0.035219327,0.28895354,0.49221024,0.06965756,0.46060064,0.006996735],\"y\":[5.035003,5.2424083,5.1918364,5.529476,5.664883,5.062537,5.614609,5.0859056,5.7297487,5.7426186,5.5077996,5.5896807,5.9728303,5.3389797,5.332006,6.032172,5.9234996,5.6477523,5.633202,5.906338,5.875859,6.910996,6.5052147,5.406056,5.4760785,6.129239,6.1408753,5.7478337,5.1371937,5.8785195,6.480586,6.408336,5.3317175,5.677474,5.7257857,5.7948713,6.1833944,5.6608124,5.7642407,5.9678907,6.055814,5.77364,6.066858,5.790117,6.3045444,5.923527,5.973696,5.4473243,5.9144163,6.250254,6.79778,6.771094,6.63371,6.4336576,6.395868,6.3105893,7.336922,6.59581,6.326063,6.9599447,6.8664436,6.945028,6.2499523,6.4762917,6.1579237,7.055593,6.2574477,7.479023,6.272149,7.2713804,5.8799777,6.352491,5.8582554,6.685192,7.1431885,7.3728633,6.838344,6.791372,6.556036,6.458795,6.7473373,6.383667,7.0697064,6.8196683,6.6358175,6.6040907,7.1285863,7.2127585,6.744717,6.838003,5.912319,6.7256813,6.4736905,6.7729535,6.8277216,7.028124,6.5431223,7.0973277,6.6809044,6.967142,6.8304167,6.5903654,6.4931145,6.6040554,6.847252,7.1217,6.951928,6.794158,6.2832108,5.985091,6.1200767,6.6113234,5.9854255,6.683618,6.4851513,6.4572477,6.406686,6.428871,6.331547,6.594099,6.685112,6.2028675,7.022855,6.81492,6.1181498,5.8165913,6.6808424,6.0758014,5.9262595,6.5408187,6.5603914,6.4436407,6.4843416,6.3322062,6.726312,6.3398266,5.576062,6.155062,6.149462,6.2588873,5.9452724,6.1830163,5.865547,5.9110947,6.184998,6.4239445,6.021743,5.99811,6.2235336,6.0570884,5.802744,6.058542,6.462475,6.397371,6.3199425,6.225245,6.437115,5.9102387,5.7837157,6.2756457,6.3682375,5.7420506,5.977252,6.057221,6.36302,6.4396796,6.634494,6.1962075,5.943683,6.375511,5.6305757,6.0236764,6.213812,6.099872,6.304055,6.1396356,5.904754,5.770087,5.0830474,6.100108,6.142657,6.1587424,5.8253403,5.686188,6.1782293,6.180106,5.6642966,5.822932,5.7539964,6.209315,5.7952657,5.7016587,5.909398,6.0613933,5.675005,6.0899954,5.928738,5.7213287,6.0380397,5.387316,5.38361,4.8597703,5.4195833,5.351725,5.2805867,5.6707196,5.76454,5.3003936,5.774347,5.056572,5.486849,5.1733623,5.5742745,5.3863373,5.4927235,5.8335533,5.151239,5.4143324,5.1814756,5.274105,5.3843117,5.9491315,5.5892134,5.7009206,5.270047,5.554914,5.809518,5.392431,5.5581436,5.083759,5.4952717,5.0590787,5.2338133,5.0396967,5.763001,5.535059,4.9971585,5.301975,5.437281,5.360623,5.9235463,5.0552354,5.358806,5.396616,5.027365,5.312774,5.858646,5.46526,5.275495,5.741008,5.181814,5.2399077,5.867944,5.191258,5.5590477,5.8755336,5.2332296,5.514091,5.4478464,5.7356915,4.810584,5.768322,5.221465,5.1326475,5.3920774,5.48416,5.826811,5.667678,5.435934,4.97268,5.780829,4.908673,5.2530484,4.992185,5.0420613,5.3847103,5.295141,5.148041,5.6463146,4.956923,5.182929,4.980341,5.4177723,5.030325,5.3770924,5.4831767,5.389746,5.212768,5.149822,5.254217,5.248027,6.486047,4.7757826,6.7926083,5.783696,6.4009733,5.1335106,6.183709,4.5040636,5.442799,4.548631,5.4095354,5.8683534,6.2958717,8.085125,7.78085,5.336691,6.1890497,7.6991615,4.5737586,4.661208,7.9977074,5.9447904,7.378923,5.1182604,4.7024083,4.8692904,4.9819856,6.9794936,6.719738,5.0835376,5.1953464,5.8431687,6.451851,4.7876377,4.9404683,5.6743,5.8849764,7.526801,6.7478104,5.0777497,7.178527,4.9181314,7.347968,6.2802877,5.2767973,5.1534743,7.3990526,7.0260773,7.4498677,7.2074065,5.5724335,5.2833233,6.927157,5.742641,7.457143,6.3445544,6.9583397,5.681394,6.139238,7.6409755,5.244455,7.07718,7.510227,7.68656,6.8033724,4.843742,6.101495,6.9824667,4.32593,4.873795,6.7886834,6.42131,6.0170546,6.958283,7.128164,5.635093,4.457616,6.9471254,6.7016373,4.7627954,5.556624,6.0622745,7.2830505,6.777718,4.3181605,6.0667667,4.42154,6.653395,4.4562583,6.543924,4.6817265,6.469272,4.521957,5.7280216,4.4136744,6.638803,5.860805,4.8853345,6.177097,5.8538537,5.524816,6.3107963,6.211399,5.4432397,4.9347,4.528188,5.5164137,4.866807,6.7566867,6.287085,4.2625403,4.986957,5.5821724,5.343608,6.379747,4.3832693,6.2734766,4.324419,6.177731,6.130877,6.5714707,6.0513854,6.143765,5.6975393,5.368249,5.552102,5.873147,6.003284,6.3846593,5.86478,5.73287,4.465147,6.024478,6.7477264,5.756844,4.6815677,4.8914404,6.691334,6.4059734,6.9376597,5.9376945,5.3152246,6.5422006,5.576097,6.487182,6.1984415,5.831464,5.9725738,6.5895286,6.195723,6.2208967,5.606689,6.0136857],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.2888888888888889]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.35555555555555557,0.6444444444444445]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.7111111111111111,1.0]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,1.0]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"UMAP (x vs y)\",\"x\":0.14444444444444446,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"UMAP (x vs z)\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"UMAP (y vs z)\",\"x\":0.8555555555555556,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"2D UMAP Projections (All Pairwise Components)\"},\"width\":1800,\"height\":600,\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('82d02814-6253-42a3-9c16-e1810ff379c0');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}